{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Developer Guide","text":"<p>Kolena is a comprehensive machine learning testing and debugging platform to surface hidden model behaviors and take the mystery out of model development. Kolena helps you:</p> <ul> <li>Perform high-resolution model evaluation</li> <li>Understand and track behavioral improvements and regressions</li> <li>Meaningfully communicate model capabilities</li> <li>Automate model testing and deployment workflows</li> </ul> <p>Kolena organizes your test data, stores and visualizes your model evaluations, and provides tooling to craft better tests. You interface with it through the web at app.kolena.io and programmatically via the <code>kolena</code> Python client.</p>"},{"location":"#why-kolena","title":"Why Kolena?","text":"<p>TL;DR</p> <p>Kolena helps you test your ML models more effectively.</p> <p>Jump right in with the   Quickstart guide.</p> <p>Current ML evaluation techniques are falling short. Engineers run inference on arbitrarily split benchmark datasets, spend weeks staring at error graphs to evaluate their models, and ultimately produce a global metric that fails to capture the true behavior of the model.</p> <p>Models exhibit highly variable performance across different subsets of a domain. A global metric gives you a high-level picture of performance but doesn't tell you what you really want to know: what sort of behavior can I expect from my model in production?</p> <p>To answer this question you need a higher-resolution picture of model performance. Not \"how well does my model perform on class X,\" but \"in what scenarios does my model perform well for class X?\"</p> <p> </p> <p>In the above example, looking only at global metric (e.g. F1 score), we'd almost certainly choose to deploy Model B.</p> <p>But what if the \"High Blur\" scenario isn't important for our product? Most of Model A's failures are from that scenario, and it outperforms Model B in more important scenarios like \"Front View.\" Meanwhile, Model B's underperformance in \"Front View,\" a highly important scenario, is masked by improved performance in the unimportant \"High Blur\" scenario.</p> <p>Test data is more important than training data!</p> <p>Everything you know about a new model's behavior is learned from your tests.</p> <p>Fine-grained tests teach you what you need to learn before a model hits production.</p> <p>Now... why Kolena? Two reasons:</p> <ol> <li>Managing fine-grained tests is a tedious data engineering task, especially under changing data circumstances as    your dataset grows and your understanding of your domain develops</li> <li>Creating fine-grained tests is labor-intensive and typically involves manual annotation of countless images, a    costly and time-consuming process</li> </ol> <p>We built Kolena to solve these two problems.</p>"},{"location":"#read-more","title":"Read More","text":"<ul> <li>Best Practices for ML Model Testing (Kolena Blog)</li> <li>Hidden Stratification Causes Clinically Meaningful Failures in Machine Learning for Medical Imaging (arXiv:1909.12475)</li> <li>No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems (arXiv:2011.12945)</li> </ul>"},{"location":"#developer-guide","title":"Developer Guide","text":"<p>Learn how to use Kolena to test your models effectively:</p> <ul> <li> <p>  Quickstart</p> <p>Run through an example using Kolena to set up rigorous and repeatable model testing in minutes.</p> </li> <li> <p>  Installing <code>kolena</code></p> <p>Install and initialize the <code>kolena</code> Python package, the programmatic interface to Kolena.</p> </li> <li> <p>  Building a Workflow</p> <p>Learn how to use <code>kolena.workflow</code> to test any arbitrary ML problem on Kolena.</p> </li> <li> <p>  Core Concepts</p> <p>Core concepts for testing in Kolena.</p> </li> <li> <p>  Advanced Usage</p> <p>Tutorial documentation for advanced features available in Kolena.</p> </li> <li> <p>  API Reference</p> <p>Developer-focused detailed API reference documentation for <code>kolena</code>.</p> </li> </ul>"},{"location":"building-a-workflow/","title":"Building a Workflow","text":"<p>In this tutorial we'll learn how to use the <code>kolena.workflow</code> workflow builder definitions to test a Keypoint Detection model on the 300-W facial keypoint dataset. This demonstration will show us how we can build a workflow to test any arbitrary ML problem on Kolena.</p>"},{"location":"building-a-workflow/#getting-started","title":"Getting Started","text":"<p>With the <code>kolena</code> Python client installed, first let's initialize a client session:</p> <pre><code>import os\nimport kolena\nkolena.initialize(os.environ[\"KOLENA_TOKEN\"], verbose=True)\n</code></pre> <p>The data used in this tutorial is publicly available in the <code>kolena-public-datasets</code> S3 bucket in a <code>metadata.csv</code> file:</p> <pre><code>import pandas as pd\nDATASET = \"300-W\"\nBUCKET = \"s3://kolena-public-datasets\"\ndf = pd.read_csv(f\"{BUCKET}/{DATASET}/meta/metadata.csv\")\n</code></pre> <p>Note: <code>s3fs</code> dependency</p> <p>To load CSVs directly from S3, make sure to install the <code>s3fs</code> Python module: <code>pip3 install s3fs[boto3]</code> and set up AWS credentials.</p> <p>This <code>metadata.csv</code> file describes a keypoint detection dataset with the following columns:</p> <p>Note: Five-point facial keypoints array</p> <p>For brevity, the 300-W dataset has been pared down to only 5 keypoints: outermost corner of each eye, bottom of nose, and corners of the mouth.</p> <p> Example image and five-point facial keypoints array from 300-W. </p> <ul> <li><code>locator</code>: location of the image in S3</li> <li><code>normalization_factor</code>: normalization factor of the image. This is used to normalize the error by providing a     factor for each image. Common techniques for computation include the Euclidian distance between two points or the     diagonal measurement of the image.</li> <li><code>points</code>: stringified list of coordinates corresponding to the <code>(x, y)</code> coordinates of the keypoint ground truths</li> </ul> <p>Each <code>locator</code> is present exactly one time and contains the keypoint ground truth for that image. In this tutorial, we're implementing our workflow with support for only a single keypoint instance per image, but we could easily adapt our ground truth, inference, and metrics types to accommodate a variable number of keypoint arrays per image.</p>"},{"location":"building-a-workflow/#step-1-defining-data-types","title":"Step 1: Defining Data Types","text":"<p>When building your own workflow you have control over the <code>TestSample</code> (e.g. image), <code>GroundTruth</code> (e.g. 5-element facial keypoint array), and <code>Inference</code> types used in your project.</p>"},{"location":"building-a-workflow/#test-sample-type","title":"Test Sample Type","text":"<p>For the purposes of this tutorial, let's assume our model takes a single image as input along with an optional bounding box around the face in question, produced by an upstream model in our pipeline. We can import and extend the <code>kolena.workflow.Image</code> test sample type for this purpose:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Optional\nfrom kolena.workflow import Image\nfrom kolena.workflow.annotation import BoundingBox\n@dataclass(frozen=True)\nclass TestSample(Image):\nbbox: Optional[BoundingBox] = None\n</code></pre>"},{"location":"building-a-workflow/#ground-truth-type","title":"Ground Truth Type","text":"<p>Next, let's define our <code>GroundTruth</code> type, typically containing the manually-annotated information necessary to evaluate model inferences:</p> <pre><code>from kolena.workflow import GroundTruth as GT\nfrom kolena.workflow.annotation import Keypoints\n@dataclass(frozen=True)\nclass GroundTruth(GT):\nkeypoints: Keypoints\n# In order to compute normalized error, some normalization factor describing\n# the size of the face in the image is required.\nnormalization_factor: float\n</code></pre>"},{"location":"building-a-workflow/#inference-type","title":"Inference Type","text":"<p>Lastly, we'll define our <code>Inference</code> type, containing model outputs that are evaluated against a ground truth. Note that our model produces not only a <code>Keypoints</code> array, but also an associated <code>confidence</code> value that we may use to ignore low-confidence predictions:</p> <pre><code>from kolena.workflow import Inference as Inf\n@dataclass(frozen=True)\nclass Inference(Inf):\nkeypoints: Keypoints\nconfidence: float\n</code></pre> <p>With our test sample, ground truth, and inference defined, we can now use <code>define_workflow</code> to declare our workflow:</p> <pre><code>from kolena.workflow import define_workflow\n# use these TestCase, TestSuite, and Model definitions to create and run tests\n_, TestCase, TestSuite, Model = define_workflow(\n\"Keypoint Detection\", TestSample, GroundTruth, Inference\n)\n</code></pre>"},{"location":"building-a-workflow/#step-2-defining-metrics","title":"Step 2: Defining Metrics","text":"<p>With our core data types defined, the next step is to lay out our evaluation criteria: our metrics.</p>"},{"location":"building-a-workflow/#test-sample-metrics","title":"Test Sample Metrics","text":"<p>Test Sample Metrics (<code>MetricsTestSample</code>) are metrics computed from a single test sample and its associated ground truths and inferences.</p> <p>For the keypoint detection workflow, an example metric may be normalized mean error (NME), the normalized distance between the ground truth and inference keypoints.</p> <pre><code>from kolena.workflow import MetricsTestSample\n@dataclass(frozen=True)\nclass TestSampleMetrics(MetricsTestSample):\nnormalized_mean_error: float\n# If the normalized mean error is above some configured threshold, this test\n# sample is considered an \"alignment failure\".\nalignment_failure: bool\n</code></pre>"},{"location":"building-a-workflow/#test-case-metrics","title":"Test Case Metrics","text":"<p>Test case metrics (<code>MetricsTestCase</code>) are aggregate metrics computed across a population. All of your standard evaluation metrics should go here \u2014 things like accuracy, precision, recall, or any other aggregate metrics that apply to your problem.</p> <p>For keypoint detection, we care about the mean NME and alignment failure rate across the different test samples in a test case:</p> <pre><code>from kolena.workflow import MetricsTestCase\n@dataclass(frozen=True)\nclass TestCaseMetrics(MetricsTestCase):\nmean_nme: float\nalignment_failure_rate: float\n</code></pre> <p>Tip: Plots</p> <p>Evaluators can also compute test-case-level plots using the <code>Plot</code> API. These plots are visualized on the   Results dashboard alongside the metrics reported for each test case.</p> <p>Tip: Test Suite Metrics</p> <p>Metrics can also be computed per test suite by extending <code>MetricsTestSuite</code>.</p> <p>Test suite metrics typically measure variance in performance across different test cases, being used e.g. to measure fairness across demographics for a test suite with test cases stratifying by demographic.</p>"},{"location":"building-a-workflow/#step-3-creating-tests","title":"Step 3: Creating Tests","text":"<p>With our data already in an S3 bucket and metadata loaded into memory, we can start creating test cases!</p> <p>Let's create a simple test case containing the entire dataset:</p> <pre><code>import json\ntest_samples = [TestSample(locator) for locator in df[\"locator\"]]\nground_truths = [\nGroundTruth(\nkeypoints=Keypoints(points=json.loads(record.points)),\nnormalization_factor=record.normalization_factor,\n)\nfor record in df.itertuples()\n]\nts_with_gt = list(zip(test_samples, ground_truths))\ntest_case = TestCase(f\"{DATASET} :: basic\", test_samples=ts_with_gt)\n</code></pre> <p>Note: Creating test cases</p> <p>In this tutorial we created only a single simple test case, but more advanced test cases can be generated in a variety of fast and scalable ways, either programmatically with the <code>kolena</code> Python client or visually in the   Studio.</p> <p>Now that we have a basic test case for our entire dataset let's create a test suite for it:</p> <pre><code>test_suite = TestSuite(f\"{DATASET} :: basic\", test_cases=[test_case])\n</code></pre>"},{"location":"building-a-workflow/#step-4-running-tests","title":"Step 4: Running Tests","text":"<p>With basic tests defined for the 300-W dataset, we're almost ready to start testing our models.</p>"},{"location":"building-a-workflow/#implementing-an-evaluator","title":"Implementing an Evaluator","text":"<p>Core to the testing process is the <code>Evaluator</code> implementation to compute the metrics defined in step 2. Usually, an evaluator simply plugs your existing metrics computation logic into the class-based or function-based evaluator interface.</p> <p>Evaluators can have arbitrary configuration (<code>EvaluatorConfiguration</code>), allowing you to evaluate model performance under a variety of conditions. For this keypoint detection example, perhaps we want to compute performance at a few different NME threshold values, as this threshold drives the <code>alignment_failure</code> metric.</p> <pre><code>from kolena.workflow import EvaluatorConfiguration\n@dataclass(frozen=True)\nclass NmeThreshold(EvaluatorConfiguration):\n# threshold for NME above which an image is considered an \"alignment failure\"\nthreshold: float\ndef display_name(self):\nreturn f\"NME threshold: {self.threshold}\"\n</code></pre> <p>Here, we'll mock out an evaluator implementation using the function-based interface:</p> <pre><code>from random import random\nfrom typing import List\nfrom kolena.workflow import EvaluationResults, TestCases\ndef evaluate_keypoint_detection(\ntest_samples: List[TestSample],\nground_truths: List[GroundTruth],\ninferences: List[Inference],\ntest_cases: TestCases,\nconfiguration: NmeThreshold,  # uncomment when configuration is used\n) -&gt; EvaluationResults:\n# compute per-sample metrics for each test sample\nper_sample_metrics = [\nTestSampleMetrics(normalized_mean_error=random(), alignment_failure=bool(random() &gt; 0.5))\nfor gt, inf in zip(ground_truths, inferences)\n]\n# compute aggregate metrics across all test cases using `test_cases.iter(...)`\naggregate_metrics = []\nfor test_case, *s in test_cases.iter(test_samples, ground_truths, inferences, per_sample_metrics):\ntest_case_metrics = TestCaseMetrics(mean_nme=random(), alignment_failure_rate=random())\naggregate_metrics.append((test_case, test_case_metrics))\nreturn EvaluationResults(\nmetrics_test_sample=list(zip(test_samples, per_sample_metrics)),\nmetrics_test_case=aggregate_metrics,\n)\n</code></pre>"},{"location":"building-a-workflow/#running-tests","title":"Running tests","text":"<p>To test our models, we can define an <code>infer</code> function that maps the <code>TestSample</code> object we defined above into an <code>Inference</code>:</p> <pre><code>from random import randint\ndef infer(test_sample: TestSample) -&gt; Inference:\n\"\"\"\n    1. load the image pointed to at `test_sample.locator`\n    2. pass the image to our model and transform its output into an `Inference` object\n    \"\"\"\n# Generate the dummy inference for the demo purpose.\nreturn Inference(Keypoints([(randint(100, 400), randint(100, 400)) for _ in range(5)]), random())\nmodel = Model(\"example-model-name\", infer=infer, metadata=dict(\ndescription=\"Any freeform metadata can go here\",\nhint=\"It may be helpful to include information about the model's framwwork, training methodology, dataset, etc.\",\n))\n</code></pre> <p>We now have the pieces in place to run tests on our new workflow using <code>test</code>:</p> <pre><code>from kolena.workflow import test\ntest(\nmodel,\ntest_suite,\nevaluate_keypoint_detection,\nconfigurations=[NmeThreshold(0.01), NmeThreshold(0.05), NmeThreshold(0.1)],\n)\n</code></pre> <p>That wraps up the testing process! We can now visit   Results to analyze and debug our model's performance on this test suite.</p>"},{"location":"building-a-workflow/#conclusion","title":"Conclusion","text":"<p>In this tutorial we learned how to build a workflow for an arbitrary ML problem, using a facial keypoint detection model as an example. We created new tests, tested our models on Kolena, and learned how to customize evaluation to fit our exact expectations.</p> <p>This tutorial just scratches the surface of what's possible with Kolena and covered a fraction of the <code>kolena</code> API \u2014 now that we're up and running, we can think about ways to create more detailed tests, improve existing tests, and dive deep into model behaviors.</p>"},{"location":"installing-kolena/","title":"Installing <code>kolena</code>","text":"<p>Testing on Kolena is conducted using the <code>kolena</code> Python package. You use the client to create and run tests from your infrastructure that can be explored in our web platform.</p> <p><code>kolena</code> is released under the open-source Apache-2.0 license. The package is hosted on PyPI and can be installed using your preferred Python package manager.</p>"},{"location":"installing-kolena/#installation","title":"Installation","text":"<p>The first step to start testing with Kolena is to install <code>kolena</code>. Client builds can be installed directly from PyPI using any Python package manager such as pip or Poetry:</p> <code>pip</code><code>poetry</code> <pre><code>pip install kolena\n</code></pre> <pre><code>poetry add kolena\n</code></pre>"},{"location":"installing-kolena/#extra-dependency-groups","title":"Extra Dependency Groups","text":"<p>Certain metrics computation functionality depends on additional packages like scikit-learn. These extra dependencies can be installed via the <code>metrics</code> group:</p> <code>pip</code><code>poetry</code> <pre><code>pip install kolena[metrics]\n</code></pre> <pre><code>poetry add kolena[metrics]\n</code></pre>"},{"location":"installing-kolena/#initialization","title":"Initialization","text":"<p>Once you have <code>kolena</code> installed, initialize a session with <code>kolena.initialize(token)</code>.</p> <p>From the   Developer page, generate an API token and set the <code>KOLENA_TOKEN</code> environment variable:</p> <pre><code>export KOLENA_TOKEN=\"********\"\n</code></pre> <p>With the <code>KOLENA_TOKEN</code> environment variable set, initialize a client session:</p> <pre><code>import os\nimport kolena\nkolena.initialize(os.environ[\"KOLENA_TOKEN\"], verbose=True)\n</code></pre> <p>By default, sessions have static scope and persist until the interpreter is exited. Additional logging can be configured by specifying <code>initialize(..., verbose=True)</code>.</p> <p>Tip: <code>logging</code></p> <p>Integrate <code>kolena</code> into your existing logging system by filtering for events from the <code>\"kolena\"</code> logger. All log messages are emitted as both Python standard library <code>logging</code> events as well as stdout/stderr messages.</p>"},{"location":"installing-kolena/#supported-python-versions","title":"Supported Python Versions","text":"<p><code>kolena</code> is compatible with all active Python versions.</p>  Python Version Compatible <code>kolena</code> Versions 3.11 \u22650.69 3.10 All Versions 3.9 All Versions 3.8 All Versions 3.7 All Versions 3.6 (EOL: December 2021) \u22640.46"},{"location":"quickstart/","title":"Quickstart","text":"<p>Install Kolena to set up rigorous and repeatable model testing in minutes.</p> <p>In this quickstart guide, we'll use the <code>age_estimation</code> example integration to demonstrate the how to curate test data and test models in Kolena.</p>"},{"location":"quickstart/#install-kolena","title":"Install <code>kolena</code>","text":"<p>Install the <code>kolena</code> Python package to programmatically interact with Kolena:</p> <code>pip</code><code>poetry</code> <pre><code>pip install kolena\n</code></pre> <pre><code>poetry add kolena\n</code></pre>"},{"location":"quickstart/#clone-the-examples","title":"Clone the Examples","text":"<p>The kolenaIO/kolena repository contains a number of example integrations to clone and run directly:</p> <ul> <li> <p>Example: Age Estimation</p> <p></p> <p>Age Estimation using the Labeled Faces in the Wild (LFW) dataset</p> </li> <li> <p>Example: Keypoint Detection</p> <p></p> <p>Facial Keypoint Detection using the 300 Faces in the Wild (300-W) dataset</p> </li> <li> <p>Example: Text Summarization</p> <p></p> <p>Text Summarization using OpenAI GPT-family models and the CNN-DailyMail dataset</p> </li> </ul> <p>To get started, clone the <code>kolena</code> repository:</p> <pre><code>git clone https://github.com/kolenaIO/kolena.git\n</code></pre> <p>With the repository cloned, let's set up the <code>age_estimation</code> example:</p> <pre><code>cd kolena/examples/age_estimation\npoetry update &amp;&amp; poetry install\n</code></pre> <p>Now we're up and running and can start creating test suites and testing models.</p>"},{"location":"quickstart/#create-test-suites","title":"Create Test Suites","text":"<p>Each of the example integrations comes with scripts for two flows:</p> <ol> <li><code>seed_test_suite.py</code>: Create test cases and test suite(s) from a source dataset</li> <li><code>seed_test_run.py</code>: Test model(s) on the created test suites</li> </ol> <p>Before running <code>seed_test_suite.py</code>, let's first configure our environment by populating the <code>KOLENA_TOKEN</code> environment variable. Visit the   Developer page to generate an API token and copy and paste the code snippet into your environment:</p> <pre><code>export KOLENA_TOKEN=\"********\"\n</code></pre> <p>We can now create test suites using the provided seeding script:</p> <pre><code>poetry run python3 age_estimation/seed_test_suite.py\n</code></pre> <p>After this script has completed, we can visit the   Test Suites page to view our newly created test suites.</p> <p>In this <code>age_estimation</code> example, we've created test suites stratifying the LFW dataset (which is stored as a CSV in S3) into test cases by age, estimated race, and estimated gender.</p>"},{"location":"quickstart/#test-a-model","title":"Test a Model","text":"<p>After we've created test suites, the final step is to test models on these test suites. The <code>age_estimation</code> example provides the <code>ssrnet</code> model for this step:</p> <pre><code>poetry run python3 age_estimation/seed_test_run.py \\\n\"ssrnet\" \\\n\"age :: labeled-faces-in-the-wild [age estimation]\" \\\n\"race :: labeled-faces-in-the-wild [age estimation]\" \\\n\"gender :: labeled-faces-in-the-wild [age estimation]\"\n</code></pre> <p>Note: Testing additional models</p> <p>In this example, model results have already been extracted and are stored in CSV files in S3. To run a new model, plug it into the <code>infer</code> method in <code>seed_test_run.py</code>.</p> <p>Once this script has completed, click the results link in your console or visit   Results to view the test results for this newly tested model.</p>"},{"location":"quickstart/#conclusion","title":"Conclusion","text":"<p>In this quickstart, we used an example integration from kolenaIO/kolena to create test suites from the Labeled Faces in the Wild (LFW) dataset and test the open-source <code>ssrnet</code> model on these test suites.</p> <p>This example shows us how to define an ML problem as a workflow for testing in Kolena, and can be arbitrarily extended with additional metrics, plots, visualizations, and data.</p>"},{"location":"advanced-usage/","title":"Advanced Usage","text":"<p>This section contains tutorial documentation for advanced features available in Kolena.</p> <ul> <li> <p> Packaging for Automated Evaluation</p> <p>Package metrics evaluation logic in a Docker container image to dynamically compute metrics on relevant subsets of your test data.</p> </li> </ul>"},{"location":"advanced-usage/packaging-for-automated-evaluation/","title":"Packaging for Automated Evaluation","text":""},{"location":"advanced-usage/packaging-for-automated-evaluation/#introduction","title":"Introduction","text":"<p>In addition to analyzing and debugging model performance, we can also use the Kolena platform to create and curate test cases and test suites. Kolena can automatically compute metrics on it for any models that have already uploaded inferences. In this guide, we'll learn how to package our custom metrics engine such that it can be used in this automatic evaluation process.</p> <p>To enable automatic metrics computation when applicable, we need to package the metrics evaluation logic into a Docker image that the Kolena platform can run. The following sections explain how to build this Docker image and link it for metrics computation on the Kolena platform.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#build-evaluator-docker-image","title":"Build Evaluator Docker Image","text":"<p>We will use the keypoint detection workflow we've built in the Building a Workflow guide to illustrate the process. Here is the project structure:</p> <pre><code>.\n\u251c\u2500\u2500 docker/\n\u2502   \u251c\u2500\u2500 build.sh\n\u2502   \u251c\u2500\u2500 publish.sh\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 keypoint_detection/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 evaluator.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 workflow.py\n\u251c\u2500\u2500 poetry.lock\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>The <code>keypoint_detection</code> directory is where our workflow is defined, with evaluator logic in <code>evaluator.py</code> and workflow data objects in <code>workflow.py</code>. The <code>main.py</code> will be the entry point where <code>test</code> is executed.</p> <p>From the workflow building guide, we know that metrics evaluation using <code>test</code> involves a <code>model</code>, a <code>test_suite</code>, an <code>evaluator</code>, and optional <code>configurations</code>:</p> <pre><code>test(model, test_suite, evaluator, configurations=configurations)\n</code></pre> <p>When executing <code>test</code> locally, the model and test suite can be initiated by user inputs. When Kolena executes <code>test</code> under automation, this information would have to be obtained through environment variables. Kolena sets up following environment variables for evaluator execution:</p> <ul> <li><code>KOLENA_MODEL_NAME</code></li> <li><code>KOLENA_TEST_SUITE_NAME</code></li> <li><code>KOLENA_TEST_SUITE_VERSION</code></li> <li><code>KOLENA_TOKEN</code></li> </ul> <p>The main script would therefore be adjusted like code sample below.</p> keypoint_detection/main.py<pre><code>import os\nimport kolena\nfrom kolena.workflow import test\nfrom .evaluator import evaluate_keypoint_detection, NmeThreshold\nfrom .workflow import Model, TestSuite\ndef main() -&gt; None:\nkolena.initialize(os.environ[\"KOLENA_TOKEN\"], verbose=True)\nmodel = Model(os.environ[\"KOLENA_MODEL_NAME\"])\ntest_suite = TestSuite.load(\nos.environ[\"KOLENA_TEST_SUITE_NAME\"],\nos.environ[\"KOLENA_TEST_SUITE_VERSION\"],\n)\ntest(model, test_suite, evaluate_keypoint_detection, configurations=[NmeThreshold(0.05)])\nif __name__ == \"__main__\":\nmain()\n</code></pre> <p>Now that we have the main script ready, the next step is to package this script into a Docker image.</p> docker/Dockerfile<pre><code>FROM python:3.9-slim AS base\nWORKDIR /opt/keypoint_detection/\nFROM base AS builder\nARG KOLENA_TOKEN\nENV POETRY_VIRTUALENVS_IN_PROJECT=true \\\nPOETRY_NO_INTERACTION=1\nRUN python3 -m pip install poetry\n\nCOPY pyproject.toml poetry.lock ./\nCOPY keypoint_detection ./keypoint_detection\nRUN poetry install --only main\n\nFROM base\nCOPY --from=builder /opt/keypoint_detection /opt/keypoint_detection/\nCOPY --from=builder /opt/keypoint_detection/.venv .venv/\n\nENTRYPOINT [ \"/opt/keypoint_detection/.venv/bin/python\", \"keypoint_detection/main.py\" ]\n</code></pre> docker/build.sh<pre><code>#!/usr/bin/env bash\nset -eu\n\nIMAGE_NAME=\"keypoint_detection_evaluator\"\nIMAGE_VERSION=\"v1\"\nIMAGE_TAG=\"$IMAGE_NAME:$IMAGE_VERSION\"\necho \"building $IMAGE_TAG...\"\nexport DOCKER_BUILDKIT=1\nexport COMPOSE_DOCKER_CLI_BUILD=1\ndocker build \\\n--tag \"$IMAGE_TAG\" \\\n--file \"docker/Dockerfile\" \\\n--build-arg KOLENA_TOKEN=${KOLENA_TOKEN} \\\n.\n</code></pre> <p>This build process installs the <code>kolena</code> package, and as such needs the <code>KOLENA_TOKEN</code> environment variable to be populated with your Kolena API key. Follow the <code>kolena</code> Python client guide to obtain an API key if you have not done so.</p> <pre><code>export KOLENA_TOKEN=\"&lt;kolena-api-token&gt;\"\n./docker/build.sh\n</code></pre>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#register-evaluator-for-workflow","title":"Register Evaluator for Workflow","text":"<p>The final step is to publish the Docker image and associate the image with the <code>Keypoint Detection</code> workflow.</p> <p>Kolena supports metrics computation using Docker image hosted on any public Docker registry or Kolena's Docker registry. In this tutorial, we will publish our image to Kolena's Docker registry. However, the steps should be easy to adapt to public Docker registry.</p> <p>The repositories on Kolena Docker registry must be prefixed with the organization name. This is to protect unauthorized access from unintended parties. Replace <code>&lt;organization&gt;</code> in <code>publish.sh</code> script with the actual organization name and run it. This would push our Docker image to the repository and register it for the workflow.</p> docker/publish.sh<pre><code>#!/usr/bin/env bash\nset -eu\n\nIMAGE_NAME=\"keypoint_detection_evaluator\"\nIMAGE_VERSION=\"v1\"\nIMAGE_TAG=\"${IMAGE_NAME}:${IMAGE_VERSION}\"\nDOCKER_REGISTRY=\"docker.kolena.io\"\nWORKFLOW=\"Keypoint Detection\"\nEVALUATOR_NAME=\"evaluate_keypoint_detection\"\nORGANIZATION=&lt;organization&gt;\n\nTARGET_IMAGE_TAG=\"$DOCKER_REGISTRY/$ORGANIZATION/$IMAGE_TAG\"\n# create repository if not exist\npoetry run kolena repository create --name \"$ORGANIZATION/$IMAGE_NAME\"\necho $KOLENA_TOKEN | docker login -u \"$ORGANIZATION\" --password-stdin $DOCKER_REGISTRY\necho \"publishing $TARGET_IMAGE_TAG...\"\ndocker tag $IMAGE_TAG $TARGET_IMAGE_TAG\ndocker push $TARGET_IMAGE_TAG\necho \"registering image $TARGET_IMAGE_TAG for evaluator $EVALUATOR_NAME of workflow $WORKFLOW...\"\npoetry run kolena evaluator register \\\n--workflow \"$WORKFLOW\" \\\n--evaluator-name \"$EVALUATOR_NAME\" \\\n--image $TARGET_IMAGE_TAG\n</code></pre> <pre><code>./docker/publish.sh\n</code></pre> <p>In <code>publish.sh</code>, we used Kolena client SDK command-line <code>kolena</code> to associate the Docker image to evaluator <code>evaluate_keypoint_detection</code> of workflow <code>Keypoint Detection</code>. You can find out more of its usage with the <code>--help</code> option.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#using-automatic-metrics-evaluation","title":"Using Automatic Metrics Evaluation","text":"<p>At this point, we are all set to leverage Kolena's automatic metrics evaluation capability. To see it in action, let's first use Kolena's Studio to curate a new test case.</p> <p>Head over to the   Studio and use the \"Explore\" tab to learn more about the test samples from a given test case. Select multiple test samples of interest and then go to the \"Create\" tab to create a new test case with the \"Create Test Case\" button. You will notice there's an option to compute metrics on this new test case for applicable models. Since we have the evaluator image registered for our workflow <code>Keypoint Detection</code>, Kolena will automatically compute metrics for the new case if this option is checked. After the computation completes, metrics of the new test case are immediately ready for us to analyze on the Results page.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we learned how to configure Kolena to automatically compute metrics when applicable, and why it brings values to model testing and analyzing process. We can use these tools to continue improving our test cases and our models.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#appendix","title":"Appendix","text":""},{"location":"advanced-usage/packaging-for-automated-evaluation/#using-dockerkolenaio","title":"Using docker.kolena.io","text":"<p>In this tutorial, we published an evaluator container image to <code>docker.kolena.io</code>, Kolena's Docker Registry. In this section, we'll explain how to use the Docker CLI to interact with <code>docker.kolena.io</code>.</p> <p>The first step is to use <code>docker login</code> to log into <code>docker.kolena.io</code>. Using your organization's name (e.g. <code>my-organization</code>, the part after <code>app.kolena.io</code> when you visit the app) as a username and your API token as a password, log in with the following command:</p> <pre><code>echo $KOLENA_TOKEN | docker login --username my-organization --password-stdin docker.kolena.io\n</code></pre> <p>Once you've successfully logged in, you can use Docker CLI to perform actions on the Kolena Docker registry. For example, to pull a previously published Docker image, use a command like:</p> <pre><code>docker pull docker.kolena.io/my-organization/&lt;docker-image-tag&gt;\n</code></pre> <p>If you're building Docker images for a new workflow, use the <code>kolena</code> command-line tool to create the repository on <code>docker.kolena.io</code> first. As mentioned in Register Evaluator for Workflow, the repository must be prefixed with your organization's name.</p> <pre><code>poetry run kolena repository create -n my-organization/new-evaluator\n</code></pre> <p>After the repository is created, we can use the Docker CLI to publish a newly built Docker image to <code>docs.kolena.io</code>:</p> <pre><code>docker push docker.kolena.io/my-organization/new-evaluator:v1\n</code></pre>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#using-secrets-in-your-evaluator","title":"Using Secrets in your Evaluator","text":"<p>If secret or sensitive data is used in your evaluation process, Kolena's secret manager can store this securely and pass it as the environment variable <code>KOLENA_EVALUATOR_SECRET</code> at runtime.</p> <p>Update the evaluator register command in <code>docker/publish.sh</code> to pass in sensitive data for the evaluator:</p> <pre><code>poetry run kolena evaluator register --workflow \"$WORKFLOW\" \\\n--evaluator-name \"$EVALUATOR_NAME\" \\\n--image $TARGET_IMAGE_TAG \\\n--secret '&lt;your secret&gt;'\n</code></pre>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#using-aws-apis-in-your-evaluator","title":"Using AWS APIs in your Evaluator","text":"<p>If your evaluator requires access to AWS APIs, specify the full AWS role ARN it should use in the evaluator register command.</p> <pre><code>poetry run kolena evaluator register --workflow \"$WORKFLOW\" \\\n--evaluator-name \"$EVALUATOR_NAME\" \\\n--image $TARGET_IMAGE_TAG \\\n--aws-assume-role &lt;target_role_arn&gt;\n</code></pre> <p>The output of the command would look like:</p> <pre><code>{\n\"workflow\": \"Keypoint Detection\",\n\"name\": \"evaluate_keypoint_detection\",\n\"image\": \"docker.kolena.io/my-organization/keypoint_detection_evaluator:v1\",\n\"created\": \"2023-04-03 16:18:10.703 -0700\",\n\"secret\": null,\n\"aws_role_config\": {\n\"job_role_arn\": \"&lt;Kolena AWS role ARN&gt;\",\n\"external_id\": \"&lt;Generated external_id&gt;\",\n\"assume_role_arn\": \"&lt;target_role_arn&gt;\"\n}\n}\n</code></pre> <p>The response includes the AWS role ARN that Kolena will use to run the evaluator Docker image, <code>aws_role_config.job_role_arn</code>, and the external_id, <code>aws_role_config.externa_id</code>, to verify that requests are made from Kolena.</p> <p>To allow Kolena's AWS role to assume the target role in your AWS account, you need to configure the trust policy of the target role. Here is an example of the trust policy.</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [{\n\"Effect\": \"Allow\",\n\"Action\": [\"sts:AssumeRole\"],\n\"Principal\": {\n\"AWS\": \"&lt;Kolena AWS role ARN&gt;\"\n},\n\"Condition\": {\n\"StringEquals\": {\n\"sts:ExternalId\": \"&lt;External_id generated by Kolena&gt;\"\n}\n}\n}]\n}\n</code></pre> <p>Please refer to AWS documents for details on Delegate access across AWS accounts using IAM roles.</p> <p>At runtime, Kolena would pass in the target role and the <code>external_id</code> in environment variables <code>KOLENA_EVALUATOR_ASSUME_ROLE_ARN</code> and <code>KOLENA_EVALUATOR_EXTERNAL_ID</code>, respectively. The evaluator would then use AWS assume-role to transit into the intended target role, and use AWS APIs under the new role.</p> <pre><code>import os\nimport boto3\nresponse = boto3.client(\"sts\").assume_role(\nRoleArn=os.environ[\"KOLENA_EVALUATOR_ASSUME_ROLE_ARN\"],\nExternalId=os.environ[\"KOLENA_EVALUATOR_EXTERNAL_ID\"],\nRoleSessionName=\"metrics-evaluator\",\n)\ncredentials = response[\"Credentials\"]\n</code></pre> <p>An example of making AWS API requests under the assumed role is shown below.</p> <pre><code># use credentials to initialize AWS sessions/clients\nclient = boto3.client(\n\"s3\",\naws_access_key_id=credentials[\"AccessKeyId\"],\naws_secret_access_key=credentials[\"SecretAccessKey\"],\naws_session_token=credentials[\"SessionToken\"],\n)\n</code></pre>"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>In this section, we'll get acquainted with the core concepts on Kolena, and learn in-depth about the various features offered. For a brief introduction, see the Quickstart Guide or the Building a Workflow tutorial. For code-level API documentation, see the API Reference Documentation for the <code>kolena</code> Python client.</p> <ul> <li> <p>  Workflow</p> <p>Testing in Kolena is broken down by the type of ML problem you're solving, called a workflow. Any ML problem that can be tested can be modeled as a workflow in Kolena.</p> </li> </ul> <ul> <li> <p>  Test Cases &amp; Test Suites</p> <p>Test cases and test suites are used to organize test data in Kolena.</p> </li> </ul> <ul> <li> <p>  Models</p> <p>In Kolena, a model is a deterministic transformation from test samples to inferences.</p> </li> </ul>"},{"location":"core-concepts/model/","title":"Model","text":"<p>In Kolena, a model is a deterministic transformation from test samples to inferences.</p> <p>Kolena only stores metadata associated with your model in its   Models registry. Models themselves \u2014 their code or their weights \u2014 are never uploaded to Kolena, only the inferences from models.</p> <p>Models are considered black boxes, which makes Kolena agnostic to the underlying framework and architecture. It's possible to test any sort of model, from deep learning to rules-based, on Kolena.</p>"},{"location":"core-concepts/model/#creating-models","title":"Creating Models","text":"<p>The <code>Model</code> class is used to programmatically create models for testing. Rather than importing the class from <code>kolena.workflow</code> directly, use the <code>Model</code> definition returned from <code>define_workflow</code> bound to the test sample and inference types for your workflow:</p> <pre><code>from kolena.workflow import define_workflow\nfrom my_workflow import MyTestSample, MyGroundTruth, MyInference\n*_, Model = define_workflow(\"My Workflow\", MyTestSample, MyGroundTruth, MyInference)\n</code></pre> <p>With this class, models can be created, loaded, and updated:</p> <pre><code>my_model = Model(\"example-model\")\n</code></pre>"},{"location":"core-concepts/model/#implementing-infer","title":"Implementing <code>infer</code>","text":"<p>To test a model using the <code>test</code> method, a <code>Model.infer</code> implementation must be provided. <code>infer</code> is where the model itself \u2014 the deterministic transformation from test sample to inference \u2014 lives.</p> <pre><code># in practice, use TestSample and Inference types from your workflow\nfrom kolena.workflow import TestSample, Inference\ndef infer(test_sample: TestSample) -&gt; Inference:\n...\n</code></pre> <p>When running a model live, this function usually involves loading the image/document/etc. from the <code>TestSample</code>, passing it to your model, and constructing an <code>Inference</code> object from the model outputs. When loading results from e.g. a CSV, this function is often just a lookup.</p> Example: Loading inferences from CSV <p>This example considers a classification workflow using the <code>Image</code> test sample type and the following inference type:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Optional\nfrom kolena.workflow import Inference\nfrom kolena.workflow.annotation import ScoredClassificationLabel\n@dataclass(frozen=True)\nclass MyInference(Inference):\n# use Optional to accommodate missing inferences\nprediction: Optional[ScoredClassificationLabel] = None\n</code></pre> <p>With inferences stored in an <code>inferences.csv</code> with the <code>locator</code>, <code>label</code> and <code>score</code> columns, implementing <code>infer</code> as a lookup is straightforward:</p> <pre><code>import pandas as pd\nfrom kolena.workflow import Image\nfrom my_workflow import MyInference\ninference_by_locator = {\nrecord.locator: MyInference(prediction=ScoredClassificationLabel(\nlabel=record.label,\nscore=record.score,\n)) for record in pd.read_csv(\"inferences.csv\").itertuples()\n}\ndef infer(test_sample: Image) -&gt; MyInference:\nreturn inference_by_locator.get(test_sample.locator, MyInference())\n</code></pre> <p>Note: Ensure that models are deterministic</p> <p>To preserve reproducibility, ensure that models tested in Kolena are deterministic.</p> <p>This is particularly important for generative models. If your model has a random seed parameter, consider including the random seed value used for testing as a piece of metadata attached to the model.</p>"},{"location":"core-concepts/model/#metadata","title":"Metadata","text":"<p>When creating a model, you have the option to specify free-form <code>metadata</code> to associate with the model. This metadata can be useful to track relevant information about the model, such as:</p> <ul> <li>Framework (e.g. PyTorch, TensorFlow, custom, etc.) and version used</li> <li>Person who trained the model, e.g. <code>name@company.ai</code></li> <li>GitHub branch, file, or commit hash used to run the model</li> <li>Links to your experimentation tracking system</li> <li>Free-form notes about methodology or observations</li> <li>Location in e.g. S3 where the model's weights are stored</li> <li>Training dataset specifier or URL</li> <li>Hyperparameters applied during training</li> </ul> <p>Metadata can be specified on the command line or edited on the web on the   Models page.</p>"},{"location":"core-concepts/model/#faq-best-practices","title":"FAQ &amp; Best Practices","text":"How should models be named? <p>Two factors influence model naming:</p> <ol> <li>A model's name is unique, and</li> <li>A model is deterministic.</li> </ol> <p>This means that anything that may change your model's outputs, such as environment or packaging, should be tracked as a new model! We recommend storing a variety of information in the model name, for example:</p> <ul> <li>Model architecture, e.g. <code>YOLOR-D6</code></li> <li>Input size, e.g. <code>1280x1280</code></li> <li>Framework, e.g. <code>pytorch-1.7</code></li> <li>Additional tracking information, such as its name in Weights &amp; Biases, e.g. <code>helpful-meadow-5</code></li> </ul> <p>An example model name may therefore be:</p> <pre><code>helpful-meadow-5 (YOLOR-D6, 1280x1280, pytorch-1.7)\n</code></pre> <p>Model names can be edited on the web on the   Models page.</p>"},{"location":"core-concepts/test-suite/","title":"Test Case &amp; Test Suite","text":"<p>Test cases and test suites are used to organize test data in Kolena.</p> <p>A test case is a collection of test samples and their associated ground truths. Test cases can be thought of as benchmark datasets, or slices of a benchmark dataset.</p> <p>A test suite is a collection of test cases. Models are tested on test suites.</p> <p>Test cases and test suites are found on the   Test Suites page on Kolena.</p>"},{"location":"core-concepts/test-suite/#managing-test-cases-test-suites","title":"Managing Test Cases &amp; Test Suites","text":"<p>The <code>TestCase</code> and <code>TestSuite</code> classes are used to programmatically create test cases and test suites. Rather than importing these classes from <code>kolena.workflow</code> directly, Use the definitions returned from <code>define_workflow</code> bound to the test sample and ground truth types for your workflow:</p> <pre><code>from kolena.workflow import define_workflow\nfrom my_workflow import MyTestSample, MyGroundTruth, MyInference\n_, TestCase, TestSuite, _ = define_workflow(\n\"My Workflow\",\nMyTestSample,\nMyGroundTruth,\nMyInference,\n)\n</code></pre> <p>These classes can then be used to create, load, and edit test cases and test suites:</p> Test CaseTest Suite <p>Create using <code>TestCase.create</code>:</p> <pre><code># throws if a test case with name 'example-test-case' already exists\ntest_case = TestCase.create(\n\"example-test-case\",\n# optionally include list of test samples and ground truths to populate the new test case\n# test_samples=[(ts0, gt0), (ts1, gt1), (ts2, gt2)],\n)\n</code></pre> <p>Load using <code>TestCase.load</code>:</p> <pre><code># throws if a test case with name 'example-test-case' does not exist\ntest_case = TestCase.load(\"example-test-case\")\n</code></pre> <p>Use the <code>TestCase</code> constructor for idempotent create/load behavior:</p> <pre><code># loads 'example-test-case' or creates it if it does not already exist\ntest_case = TestCase(\"example-test-case\")\n</code></pre> <p>Test cases be edited using the context-managed <code>Editor</code> interface:</p> <pre><code>with TestCase(\"example-test-case\").edit(reset=True) as editor:\n# perform desired editing actions within context\neditor.add(ts0, gt0)\n</code></pre> <p>Create using <code>TestSuite.create</code>:</p> <pre><code># throws if a test suite with name 'example-test-suite' already exists\ntest_suite = TestSuite.create(\n\"example-test-suite\",\n# optionally include list of test cases to populate the new test suite\n# test_cases=[test_case0, test_case1, test_case2],\n)\n</code></pre> <p>Load using <code>TestSuite.load</code>:</p> <pre><code># throws if a test suite with name 'example-test-suite' does not exist\ntest_suite = TestSuite.load(\"example-test-suite\")\n</code></pre> <p>Use the <code>TestSuite</code> constructor for idempotent create/load behavior:</p> <pre><code># loads 'example-test-suite' or creates it if it does not already exist\ntest_suite = TestSuite(\"example-test-suite\")\n</code></pre> <p>Test suites be edited using the context-managed <code>Editor</code> interface:</p> <pre><code>with TestSuite(\"example-test-suite\").edit() as editor:\neditor.add(test_case_a)\neditor.remove(test_case_b)\n# perform desired editing actions within context\n</code></pre>"},{"location":"core-concepts/test-suite/#versioning","title":"Versioning","text":"<p>All test data on Kolena is versioned and immutable<sup>1</sup>. Previous versions of test cases and test suites are always available and can be visualized on the web and loaded programmatically by specifying a version.</p> <pre><code># load a specific version of a test suite\ntest_suite_v2 = TestSuite.load(\"example-name\", version=2)\n</code></pre>"},{"location":"core-concepts/test-suite/#faq-best-practices","title":"FAQ &amp; Best Practices","text":"How should I map my existing benchmark into test cases and test suites? <p>To start, create a test suite containing a single test case for the complete benchmark. This single-test-case test suite represents standard, aggregate evaluation on a benchmark dataset.</p> <p>Once this test suite has been created, you can start creating test cases! Use the Studio, the Stratifier, or the Python client to create test cases slicing through (stratifying) this benchmark.</p> How many test cases should a test suite include? <p>While test suites can hold anywhere from one to thousands of test cases, the sweet spot for the signal-to-noise ratio is in the dozens or low hundreds of test cases per test suite.</p> <p>Note that the relationship between benchmark dataset and test suite doesn't need to be 1:1. Often it can be useful to create different test suites for different stratification strategies applied to the same benchmark.</p> How many samples should be included in a test case? <p>While there's no one-size-fits-all answer, we usually recommend including at least 100 samples in each test case. Smaller test cases can be used to provide a very rough signal about the presence or absence of a model beahvior, but shouldn't be relied upon for much more than a directional indication of performance.</p> <p>The multi-model Results comparison view in Kolena takes the number of test samples within a test case into account when highlighting improvements and regressions. The larger the test case, the smaller the \u2206 required to consider a change from one model to another as \"significant.\"</p> How many negative samples should a test case include? <p>Many workflows, such as object detection or binary classification, have a concept of \"negative\" samples. In object detection, a \"negative sample\" is a sample (i.e. image) that does not include any objects to be detected.</p> <p>Negative samples can have a large impact on certain metrics. To continue with the object detection example, the precision metric depends on the number of false positive detections:</p> \\[ \\text{Precision} := \\dfrac{\\text{# True Positives}}{\\text{# True Positives} + \\text{# False Positives}} \\] <p>Therefore, since each negative sample has some likelihood of yielding false positive detections but no likelihood of yielding true positive detections, adding negative samples to a test case may decrease aggregate precision values computed across the test case.</p> <p>As a general rule of thumb, we recommend including an even balance of positive and negative samples in each test case. This composition minimizes the likelihood of different metrics being heavily skewed in one direction or another.</p> <ol> <li> <p>Immutability caveat: test suites, along with any test cases and test samples they hold, can be deleted on the   Test Suites page.\u00a0\u21a9</p> </li> </ol>"},{"location":"core-concepts/workflow/","title":"Workflow","text":"<p>Testing in Kolena is broken down by the type of ML problem you're solving, called a workflow. Any ML problem that can be tested can be modeled as a workflow in Kolena.</p> <p>Examples of workflows include:</p> <ul> <li>  Keypoint Detection using images</li> <li> array-string  Text Summarization using articles/documents</li> <li>  Age Estimation (regression) using images</li> <li>  Video Retrieval using text queries on a corpus of videos</li> </ul> <p>With the <code>kolena.workflow</code> client module, any arbitrary ML problem can be defined as a workflow and tested on Kolena.</p> <p>There are three main components of a workflow:</p> <p>Info</p> <p>These three types can be thought of as the data model, or the schema, of a workflow.</p> <ol> <li>Test Sample: the inputs to a model, e.g. image, video, document</li> <li>Ground Truth: the expected model outputs</li> <li>Inference: the actual model outputs</li> </ol>"},{"location":"core-concepts/workflow/#test-sample","title":"Test Sample","text":"<p>In Kolena, \"test sample\" is the general term for the input to a model.</p> <p>For standard computer vision (CV) models, the test sample is often a single image. Video-based computer vision models would have a video test sample type, and stereo vision models would use image pairs. For natural language processing models, the test sample may be a document or text snippet.</p> <p>When building a workflow, you can extend and compose these base test sample types as necessary, or use the base types directly if no customization is required.</p>"},{"location":"core-concepts/workflow/#metadata","title":"Metadata","text":"<p>Any additional information associated with a test sample, e.g. details about how it was collected, can be included as metadata. We recommend uploading any and all metadata that you have available, as metadata can be useful for searching through data in the Studio, interpreting model results, and creating new test cases.</p> <pre><code>from dataclasses import dataclass, field\nfrom kolena.workflow import Document, Metadata\n@dataclass(frozen=True)\nclass MyDocument(Document):\n# locator: str  # inherited from parent Document\ndoc_id: int  # example of a field that is explicitly required\nmetadata: Metadata = field(default_factory=dict)  # free-form, optional metadata\n</code></pre> <p>Use <code>pydantic</code> dataclasses</p> <p>When building a workflow, object definitions can us standard library <code>dataclasses</code> or Pydantic <code>dataclasses</code>. Pydantic brings helpful runtime type validation and coercion and can be used as a drop-in replacement for standard library <code>dataclasses</code>.</p>"},{"location":"core-concepts/workflow/#composite-test-samples","title":"Composite Test Samples","text":"<p>Kolena is not prescriptive about the shape of your ML problem. Test samples can be composed, using the <code>Composite</code> test sample type, to mirror the shape of your problem directly.</p> <p>Consider the example of an autonomous vehicle application that uses four cameras, one for each of the <code>front</code>, <code>right</code>, <code>rear</code>, and <code>left</code> views:</p> <pre><code>from dataclasses import dataclass\nfrom kolena.workflow import Composite, Image\n@dataclass(frozen=True)\nclass QuadImage(Composite):\nfront: Image\nright: Image\nrear: Image\nleft: Image\n</code></pre> How can I specify annotations on <code>Composite</code> test samples? <p>Image-level (or video-level, document-level, etc.) annotations can be specified when using composite test samples. To specify image-level objets in each of the four images, ground truth or inference definitions may look like this:</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\nfrom kolena.workflow import DataObject, GroundTruth\nfrom kolena.workflow.annotation import BoundingBox\n@dataclass(frozen=True)\nclass SingleImageGroundTruth(DataObject):\nobjects: List[BoundingBox]\n@dataclass(frozen=True)\nclass QuadImageGroundTruth(GroundTruth):\n# attribute names matches attribute names in test sample\nfront: SingleImageGroundTruth\nright: SingleImageGroundTruth\nrear: SingleImageGroundTruth\nleft: SingleImageGroundTruth\n</code></pre>"},{"location":"core-concepts/workflow/#ground-truth","title":"Ground Truth","text":"<p>The ground truth represents the expected output from a model when provided with a test sample. Ground truths are often manually annotated and are used to determine the correctness of model predictions.</p> <p>In the   Studio, ground truths are always displayed alongside their paired test samples. Any annotations, such as bounding boxes or polygons, are visualized on top of the test sample.</p> <p>The contents of a ground truth are driven by the requirements of the workflow. Take this example for a multiclass object detection workflow:</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\nfrom kolena.workflow import GroundTruth\nfrom kolena.workflow.annotation import LabeledBoundingBox\n@dataclass(frozen=True)\nclass MyGroundTruth(GroundTruth):\nobjects: List[LabeledBoundingBox]\n</code></pre> Where should additional information that isn't used for model evaluation live? <p>We recommend scoping the ground truth to only the data required for model evaluation. Any additional metadata, annotations, or assets associated with a test sample can be included as a part of the test sample itself or in its free-form metadata.</p> <p>However, it isn't a strict requirement that ground truths only contain information used for model evaluation. Sometimes it makes sense to include additional information as optional fields inside a ground truth definition.</p>"},{"location":"core-concepts/workflow/#inference","title":"Inference","text":"<p>A workflow's inference type contains the actual output produced by a model when given a test sample. Inferences are also referred to as \"raw inferences,\" as they represent the raw output from a model.</p> <p>The inference type and ground truth type for a workflow will often look very similar to one another.</p>"},{"location":"core-concepts/workflow/#extending-annotation-types","title":"Extending Annotation Types","text":"<p>Annotation types can be extended to include additional fields, when necessary.</p> <p>Consider the example of a <code>Keypoints</code> detection model that detects anywhere from 0 to N keypoints arrays when provided an image. Each keypoints array has an associated class label and confidence value. This model's inference type could be defined as follows:</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\nfrom kolena.workflow import Inference\nfrom kolena.workflow.annotation import Keypoints\n@dataclass(frozen=True)\nclass ScoredLabeledKeypoints(Keypoints):\n# points: List[Tuple[float, float]]  # inherited from Keypoints\nscore: float  # confidence score, between 0 and 1\nlabel: str  # predicted class\n@dataclass(frozen=True)\nclass MyInference(Inference):\npredictions: List[ScoredLabeledKeypoints]\n</code></pre>"},{"location":"core-concepts/workflow/#deduplication","title":"Deduplication","text":"<p>Models are considered deterministic inputs from test samples to inferences. This means that, when testing in Kolena, a given model only needs to process a given test sample once. Kolena uses this to speed up the process of running tests, ensuring that compute cycles are not wasted processing a given test sample multiple times when test samples exist in multiple test cases.</p> <p>When calling <code>test</code>, only samples that do not already have inferences uploaded from the given model will be processed. To change this behavior and re-process all test samples, regardless of any uploaded inferences, use the <code>reset</code> flag:</p> <pre><code># all test samples are processed and inferences [re]uploaded when reset=True\ntest(model, test_suite, evaluator, reset=True)\n</code></pre>"},{"location":"core-concepts/workflow/#defining-a-workflow","title":"Defining a Workflow","text":"<p>With test sample, ground truth, and inference types declared, defining a workflow provides the <code>TestCase</code>, <code>TestSuite</code>, and <code>Model</code> definitions to use when creating tests and testing models with this workflow:</p> <pre><code>from kolena.workflow import define_workflow\nfrom my_workflow import MyTestSample, MyGroundTruth, MyInference\n_, TestCase, TestSuite, Model = define_workflow(\n\"My Example Workflow\",\nMyTestSample,\nMyGroundTruth,\nMyInference,\n)\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>This page answers common questions about Kolena and how to use it to test ML models.</p> <p>If you don't see your question here, please reach out to us on Slack or at contact@kolena.io!</p>"},{"location":"faq/#about-kolena","title":"About Kolena","text":"What data types does Kolena support? <p>Testing in Kolena is fully customizable and supports computer vision, natural language processing, and structured data (tabular, time series) machine learning models. This includes images, documents, videos, 3D models and point clouds, and more.</p> <p>See the available data types in <code>kolena.workflow.TestSample</code>, and the available annotation types in <code>kolena.workflow.annotation</code>.</p> <p>We're constantly adding new data types and annotation types \u2014 if you don't see what you're looking for, reach out to us and we'll happily extend our system to support your use case.</p> Do I have to upload my datasets to Kolena? <p>No. Kolena doesn't store your data (images, videos, documents, 3D assetes, etc.) directly, only URLs pointing to the right location in a cloud bucket or internal infrastructure that you own.</p> <p>While onboarding your team, we'll discuss what access restrictions are necessary for your data and select the right integration solution. As one example, as a part of the integration we might restrict access to files registered with Kolena to only users on your corporate VPN.</p> <p>We support a variety of integration patterns depending on your organization's requirements and security stance. Get in touch with us to discuss details!</p> Do I have to upload my models to Kolena? <p>No. Tests are always run in your environment using the <code>kolena</code> Python client, and you never have to package or upload models to Kolena.</p> Where does Kolena fit into the MLOps development life cycle? <p>Kolena is primarily a testing (or \"offline evaluation\") platform, coming after training and before deployment. We believe that increased emphasis on this offline evaluation segment of the model development life cycle can save effort upstream in the data collection and training process as well as prevent headaches downstream in deployment.</p>"},{"location":"faq/#using-kolena","title":"Using Kolena","text":"How do I generate an API token? <p>Generate an API token by visiting the   Developer page, located at the bottom of the lefthand sidebar, then copy/paste the shell snippet to set this token as <code>KOLENA_TOKEN</code> in your environment.</p> How many API tokens can I generate? <p>API tokens are scoped to your username. Each user is limited to one valid token at a time \u2014 generating a new token on the   Developer page invalidates any previous token generated for your user.</p> <p>To retrieve a service user API token that is not scoped to a specific username, please reach out to us on Slack or at contact@kolena.io.</p> How can I add new users to my organization? <p>Certain members of each organization have administrator privileges. These administrators can add new users, and grant users administrator privileges, by visiting the   Organization Settings page and adding entries to the Authorized Users table.</p> <p>Note that this page is only visible for organization administrators.</p> I'm new to Kolena \u2014 how can I learn more about the platform and how to use it? <p>On each page, there is a button with the   icon next to the page title. Click on this button to bring up a detailed tutorial explaining the contents of the current page and how it's used.</p> How can I report a bug? <p>If you encounter a bug when using the <code>kolena</code> Python client or when using app.kolena.io, message us on Slack, email your support representative or contact@kolena.io, or open an issue on the <code>kolena</code> repository for Python-client-related issues.</p> <p>Please include any relevant stacktrace or platform URL when reporting an issue.</p>"},{"location":"reference/","title":"API Reference","text":"<p>This section contains detailed API reference documentation for <code>kolena</code>.</p> <ul> <li> <p> <code>kolena.workflow</code></p> <p>Building blocks to test any ML problem in Kolena</p> </li> </ul> <ul> <li> <p><code>kolena.initialize</code></p> <p>Initialize client sessions</p> </li> <li> <p><code>kolena.errors</code></p> <p>Custom error definitions</p> </li> </ul>"},{"location":"reference/errors/","title":"<code>kolena.errors</code>","text":"<p>Reference for various exceptions raised from <code>kolena</code>. All custom exceptions extend the base <code>KolenaError</code>.</p>"},{"location":"reference/errors/#kolena.errors.KolenaError","title":"<code>KolenaError</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Base error for all Kolena errors to extend. Allows consumers to catch Kolena specific errors.</p>"},{"location":"reference/errors/#kolena.errors.InputValidationError","title":"<code>InputValidationError</code>","text":"<p>         Bases: <code>ValueError</code>, <code>KolenaError</code></p> <p>Exception indicating that provided input data failed validation.</p>"},{"location":"reference/errors/#kolena.errors.IncorrectUsageError","title":"<code>IncorrectUsageError</code>","text":"<p>         Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that the user performed a disallowed action with the client.</p>"},{"location":"reference/errors/#kolena.errors.InvalidTokenError","title":"<code>InvalidTokenError</code>","text":"<p>         Bases: <code>ValueError</code>, <code>KolenaError</code></p> <p>Exception indicating that provided token value was invalid.</p>"},{"location":"reference/errors/#kolena.errors.InvalidClientStateError","title":"<code>InvalidClientStateError</code>","text":"<p>         Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that client state was invalid.</p>"},{"location":"reference/errors/#kolena.errors.UninitializedError","title":"<code>UninitializedError</code>","text":"<p>         Bases: <code>InvalidClientStateError</code></p> <p>Exception indicating that the client has not been properly initialized before usage.</p>"},{"location":"reference/errors/#kolena.errors.DirectInstantiationError","title":"<code>DirectInstantiationError</code>","text":"<p>         Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that the default constructor was used for a class that does not support direct instantiation. Available static constructors should be used when this exception is encountered.</p>"},{"location":"reference/errors/#kolena.errors.FrozenObjectError","title":"<code>FrozenObjectError</code>","text":"<p>         Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that the user attempted to modify a frozen object.</p>"},{"location":"reference/errors/#kolena.errors.UnauthenticatedError","title":"<code>UnauthenticatedError</code>","text":"<p>         Bases: <code>HTTPError</code>, <code>KolenaError</code></p> <p>Exception indicating unauthenticated usage of the client.</p>"},{"location":"reference/errors/#kolena.errors.RemoteError","title":"<code>RemoteError</code>","text":"<p>         Bases: <code>HTTPError</code>, <code>KolenaError</code></p> <p>Exception indicating that a remote error occurred in communications between the Kolena client and server.</p>"},{"location":"reference/errors/#kolena.errors.CustomMetricsException","title":"<code>CustomMetricsException</code>","text":"<p>         Bases: <code>KolenaError</code></p> <p>Exception indicating that there's an error when computing custom metrics.</p>"},{"location":"reference/errors/#kolena.errors.WorkflowMismatchError","title":"<code>WorkflowMismatchError</code>","text":"<p>         Bases: <code>KolenaError</code></p> <p>Exception indicating a workflow mismatch.</p>"},{"location":"reference/errors/#kolena.errors.NotFoundError","title":"<code>NotFoundError</code>","text":"<p>         Bases: <code>RemoteError</code></p> <p>Exception indicating an entity is not found</p>"},{"location":"reference/errors/#kolena.errors.NameConflictError","title":"<code>NameConflictError</code>","text":"<p>         Bases: <code>RemoteError</code></p> <p>Exception indicating the name of an entity is conflict</p>"},{"location":"reference/initialize/","title":"<code>kolena.initialize</code>","text":""},{"location":"reference/initialize/#kolena.initialize.initialize","title":"<code>initialize(api_token, *args, verbose=False, proxies=None, **kwargs)</code>","text":"<p>Initialize a client session.</p> <p>Retrieve an API token from the   Developer page and populate the <code>KOLENA_TOKEN</code> environment variable before initializing:</p> <pre><code>import os\nimport kolena\nkolena.initialize(os.environ[\"KOLENA_TOKEN\"], verbose=True)\n</code></pre> <p>A session has a global scope and remains active until interpreter shutdown.</p> <p>Note</p> <p>As of version 0.29.0: the <code>entity</code> argument is no longer needed; the signature <code>initialize(entity, api_token)</code> has been deprecated and replaced by <code>initialize(api_token)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>api_token</code> <code>str</code> <p>Provided API token. This token is a secret and should be treated with caution.</p> required <code>verbose</code> <code>bool</code> <p>Optionally configure client to run in verbose mode, providing more information about execution. All logging events are emitted as Python standard library <code>logging</code> events from the <code>\"kolena\"</code> logger as well as to stdout/stderr directly.</p> <code>False</code> <code>proxies</code> <code>Optional[Dict[str, str]]</code> <p>Optionally configure client to run with <code>http</code> or <code>https</code> proxies. The <code>proxies</code> parameter is passed through to the <code>requests</code> package and can be configured accordingly.</p> <code>None</code> <p>Raises:</p> Type Description <code>InvalidTokenError</code> <p>The provided <code>api_token</code> is not valid.</p> <code>InputValidationError</code> <p>The provided combination or number of args is not valid.</p>"},{"location":"reference/workflow/","title":"<code>kolena.workflow</code>","text":"<p><code>kolena.workflow</code> contains the definitions to build a workflow:</p> <ol> <li> <p>Design data types, including any <code>annotations</code> or <code>assets</code>:</p> <p>Defining a workflow</p> <p><code>TestSample</code>, <code>GroundTruth</code>, and <code>Inference</code> can be thought of as the data model, or schema, for a workflow.</p> <p>  Learn more \u2197</p> <ul> <li><code>TestSample</code>: model inputs, e.g. images, videos, documents</li> <li><code>GroundTruth</code>: expected model outputs</li> <li><code>Inference</code>: real model outputs</li> </ul> </li> <li> <p>Define metrics and how they are computed:</p> <ul> <li><code>Evaluator</code>: metrics computation engine</li> </ul> </li> <li> <p>Create tests:</p> <p>Managing tests</p> <p>See the test case and test suite developer guide for an introduction to the test case and test suite concept.</p> <ul> <li><code>TestCase</code>: a test dataset, or a slice thereof</li> <li><code>TestSuite</code>: a collection of test cases</li> </ul> </li> <li> <p>Test models:</p> <ul> <li><code>Model</code>: descriptor for a model</li> <li><code>test</code>: interface to run tests</li> </ul> </li> </ol>"},{"location":"reference/workflow/annotation/","title":"Annotations: <code>kolena.workflow.annotation</code>","text":"<p>Annotations are visualized in Kolena as overlays on top of <code>TestSample</code> objects.</p> <p>The following annotation types are available:</p> <ul> <li><code>BoundingBox</code></li> <li><code>Polygon</code></li> <li><code>Polyline</code></li> <li><code>Keypoints</code></li> <li><code>BoundingBox3D</code></li> <li><code>SegmentationMask</code></li> <li><code>BitmapMask</code></li> <li><code>ClassificationLabel</code></li> </ul> <p>For example, when viewing images in the Studio, any annotations (such as lists of <code>BoundingBox</code> objects) present in the <code>TestSample</code>, <code>GroundTruth</code>, <code>Inference</code>, or <code>MetricsTestSample</code> objects are rendered on top of the image.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Annotation","title":"<code>Annotation</code>","text":"<p>         Bases: <code>TypedDataObject[_AnnotationType]</code></p> <p>The base class for all annotation types.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox","title":"<code>BoundingBox</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox.top_left","title":"<code>top_left: Tuple[float, float]</code>  <code>instance-attribute</code>","text":"<p>The top left vertex (in <code>(x, y)</code> image coordinates) of this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox.bottom_right","title":"<code>bottom_right: Tuple[float, float]</code>  <code>instance-attribute</code>","text":"<p>The bottom right vertex (in <code>(x, y)</code> image coordinates) of this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox","title":"<code>LabeledBoundingBox</code>","text":"<p>         Bases: <code>BoundingBox</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices and a string label.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox","title":"<code>ScoredBoundingBox</code>","text":"<p>         Bases: <code>BoundingBox</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox","title":"<code>ScoredLabeledBoundingBox</code>","text":"<p>         Bases: <code>BoundingBox</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices, a string label, and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polygon","title":"<code>Polygon</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polygon.points","title":"<code>points: List[Tuple[float, float]]</code>  <code>instance-attribute</code>","text":"<p>The sequence of <code>(x, y)</code> points comprising the boundary of this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledPolygon","title":"<code>LabeledPolygon</code>","text":"<p>         Bases: <code>Polygon</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates and a string label.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledPolygon.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredPolygon","title":"<code>ScoredPolygon</code>","text":"<p>         Bases: <code>Polygon</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredPolygon.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledPolygon","title":"<code>ScoredLabeledPolygon</code>","text":"<p>         Bases: <code>Polygon</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates with a string label and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledPolygon.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledPolygon.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Keypoints","title":"<code>Keypoints</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Array of any number of keypoints specified in pixel coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Keypoints.points","title":"<code>points: List[Tuple[float, float]]</code>  <code>instance-attribute</code>","text":"<p>The sequence of discrete <code>(x, y)</code> points comprising this keypoints annotation.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polyline","title":"<code>Polyline</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Polyline with any number of vertices specified in pixel coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polyline.points","title":"<code>points: List[Tuple[float, float]]</code>  <code>instance-attribute</code>","text":"<p>The sequence of connected <code>(x, y)</code> points comprising this polyline.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D","title":"<code>BoundingBox3D</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Three-dimensional cuboid bounding box in a right-handed coordinate system.</p> <p>Specified by <code>(x, y, z)</code> coordinates for the <code>center</code> of the cuboid, <code>(x, y, z)</code> <code>dimensions</code>, and a <code>rotation</code> parameter specifying the degrees of rotation about each axis <code>(x, y, z)</code> ranging <code>[-\u03c0, \u03c0]</code>.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D.center","title":"<code>center: Tuple[float, float, float]</code>  <code>instance-attribute</code>","text":"<p><code>(x, y, z)</code> coordinates specifying the center of the bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D.dimensions","title":"<code>dimensions: Tuple[float, float, float]</code>  <code>instance-attribute</code>","text":"<p><code>(x, y, z)</code> measurements specifying the dimensions of the bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D.rotations","title":"<code>rotations: Tuple[float, float, float]</code>  <code>instance-attribute</code>","text":"<p>Rotations in degrees about each <code>(x, y, z)</code> axis.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox3D","title":"<code>LabeledBoundingBox3D</code>","text":"<p>         Bases: <code>BoundingBox3D</code></p> <p><code>BoundingBox3D</code> with an additional string label.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox3D.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox3D","title":"<code>ScoredBoundingBox3D</code>","text":"<p>         Bases: <code>BoundingBox3D</code></p> <p><code>BoundingBox3D</code> with an additional float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox3D.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox3D","title":"<code>ScoredLabeledBoundingBox3D</code>","text":"<p>         Bases: <code>BoundingBox3D</code></p> <p><code>BoundingBox3D</code> with an additional string label and float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox3D.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox3D.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.SegmentationMask","title":"<code>SegmentationMask</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Raster segmentation mask. The <code>locator</code> is the URL to the image file representing the segmentation mask.</p> <p>The segmentation mask must be rendered as a single-channel, 8-bit-depth (grayscale) image. For the best results, use a lossless file format such as PNG. Each pixel's value is the numerical ID of its class label, as specified in the <code>labels</code> map. Any pixel value not present in the <code>labels</code> map is rendered as part of the background.</p> <p>For example, <code>labels = {255: \"object\"}</code> will highlight all pixels with the value of 255 as <code>\"object\"</code>. Every other pixel value will be transparent.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.SegmentationMask.labels","title":"<code>labels: Dict[int, str]</code>  <code>instance-attribute</code>","text":"<p>Mapping of unique label IDs (pixel values) to unique label values.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.SegmentationMask.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL of the segmentation mask image.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BitmapMask","title":"<code>BitmapMask</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Arbitrary bitmap mask. The <code>locator</code> is the URL to the image file representing the mask.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BitmapMask.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL of the bitmap data.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ClassificationLabel","title":"<code>ClassificationLabel</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Label of classification.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ClassificationLabel.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>String label for this classification.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredClassificationLabel","title":"<code>ScoredClassificationLabel</code>","text":"<p>         Bases: <code>ClassificationLabel</code></p> <p>Classification label with accompanying score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredClassificationLabel.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>Score associated with this label.</p>"},{"location":"reference/workflow/asset/","title":"Assets: <code>kolena.workflow.asset</code>","text":"<p>Assets are additional files linked to the <code>TestSample</code>, <code>GroundTruth</code>, or <code>Inference</code> objects for your workflow. Assets can be visualized in the Kolena Studio when exploring your test cases or model results.</p> <p>The following asset types are available:</p> <ul> <li><code>ImageAsset</code></li> <li><code>PlainTextAsset</code></li> <li><code>BinaryAsset</code></li> <li><code>PointCloudAsset</code></li> <li><code>VideoAsset</code></li> </ul>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.Asset","title":"<code>Asset</code>","text":"<p>         Bases: <code>TypedDataObject[_AssetType]</code></p> <p>Base class for all asset types.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.ImageAsset","title":"<code>ImageAsset</code>","text":"<p>         Bases: <code>Asset</code></p> <p>An image in a cloud bucket.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.ImageAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this image in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-image-asset.png</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PlainTextAsset","title":"<code>PlainTextAsset</code>","text":"<p>         Bases: <code>Asset</code></p> <p>A plain text file in a cloud bucket.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PlainTextAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this text file in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-text-asset.txt</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BinaryAsset","title":"<code>BinaryAsset</code>","text":"<p>         Bases: <code>Asset</code></p> <p>A binary file in a cloud bucket.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BinaryAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this text file in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-binary-asset.bin</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PointCloudAsset","title":"<code>PointCloudAsset</code>","text":"<p>         Bases: <code>Asset</code></p> <p>A three-dimensional point cloud located in a cloud bucket. Points are assumed to be specified in a right-handed, Z-up coordinate system with the origin around the sensor that captured the point cloud.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PointCloudAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this point cloud in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-point-cloud.pcd</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BaseVideoAsset","title":"<code>BaseVideoAsset</code>","text":"<p>         Bases: <code>Asset</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BaseVideoAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset","title":"<code>VideoAsset</code>","text":"<p>         Bases: <code>BaseVideoAsset</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.thumbnail","title":"<code>thumbnail: Optional[ImageAsset] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally provide asset locator for custom video thumbnail image.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.start","title":"<code>start: Optional[float] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally specify start time of video snippet, in seconds.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.end","title":"<code>end: Optional[float] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally specify end time of video snippet, in seconds.</p>"},{"location":"reference/workflow/define-workflow/","title":"<code>kolena.workflow.define_workflow</code>","text":""},{"location":"reference/workflow/define-workflow/#kolena.workflow.define_workflow.define_workflow","title":"<code>define_workflow(name, test_sample_type, ground_truth_type, inference_type)</code>","text":"<p>Define a new workflow, specifying its test sample, ground truth, and inference types.</p> <pre><code>from kolena.workflow import define_workflow\nfrom my_code import MyTestSample, MyGroundTruth, MyInference\n_, TestCase, TestSuite, Model = define_workflow(\n\"My Workflow\",\nMyTestSample,   # extends e.g. kolena.workflow.Image (or uses directly)\nMyGroundTruth,  # extends kolena.workflow.GroundTruth\nMyInference,    # extends kolena.workflow.Inference\n)\n</code></pre> <p><code>define_workflow</code> is provided as a convenience method to create the <code>TestCase</code>, <code>TestSuite</code>, and <code>Model</code> objects for a new workflow. These objects can also be defined manually by subclassing them and binding the <code>workflow</code> class variable:</p> <pre><code>from kolena.workflow import TestCase\nfrom my_code import my_workflow\nclass MyTestCase(TestCase):\nworkflow = my_workflow\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the workflow.</p> required <code>test_sample_type</code> <code>Type[TestSample]</code> <p>The type of the <code>TestSample</code> for this workflow.</p> required <code>ground_truth_type</code> <code>Type[GroundTruth]</code> <p>The type of the <code>GroundTruth</code> for this workflow.</p> required <code>inference_type</code> <code>Type[Inference]</code> <p>The type of the <code>Inference</code> for this workflow.</p> required <p>Returns:</p> Type Description <code>Tuple[Workflow, Type[TestCase], Type[TestSuite], Type[Model]]</code> <p>The <code>Workflow</code> object for this workflow along with the <code>TestCase</code>, <code>TestSuite</code>, and <code>Model</code> objects to use when creating and running tests for this workflow.</p>"},{"location":"reference/workflow/evaluator/","title":"<code>kolena.workflow.Evaluator</code>","text":"<p>Simplified interface for <code>Evaluator</code> implementations.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestSample","title":"<code>MetricsTestSample</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>Test-sample-level metrics produced by an <code>Evaluator</code>.</p> <p>This class should be subclassed with the relevant fields for a given workflow.</p> <p>Examples here may include the number of true positive detections on an image, the mean IOU of inferred polygon(s) with ground truth polygon(s), etc.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestCase","title":"<code>MetricsTestCase</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>Test-case-level metrics produced by an <code>Evaluator</code>.</p> <p>This class should be subclassed with the relevant fields for a given workflow.</p> <p>Test-case-level metrics are aggregate metrics like Precision, Recall, and F1 score. Any and all aggregate metrics that fit a workflow should be defined here.</p> <p><code>MetricsTestCase</code> supports nesting metrics objects, for e.g. reporting class-level metrics within a test case that contains multiple classes. Example usage:</p> <pre><code>@dataclass(frozen=True)\nclass PerClassMetrics(MetricsTestCase):\nClass: str\nPrecision: float\nRecall: float\nF1: float\nAP: float\n@dataclass(frozen=True)\nclass TestCaseMetrics(MetricsTestCase):\nmacro_Precision: float\nmacro_Recall: float\nmacro_F1: float\nmAP: float\nPerClass: List[PerClassMetrics]\n</code></pre>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestSuite","title":"<code>MetricsTestSuite</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>Test-suite-level metrics produced by an <code>Evaluator</code>.</p> <p>This class should be subclassed with the relevant fields for a given workflow.</p> <p>Test-suite-level metrics typically measure performance across test cases, e.g. penalizing variance across different subsets of a benchmark.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.EvaluatorConfiguration","title":"<code>EvaluatorConfiguration</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>Configuration for an <code>Evaluator</code>.</p> <p>Example evaluator configurations may specify:</p> <ul> <li>Fixed confidence thresholds at which detections are discarded.</li> <li>Different algorithms/strategies used to compute confidence thresholds     (e.g. \"accuracy optimal\" for a classification-type workflow).</li> </ul>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.EvaluatorConfiguration.display_name","title":"<code>display_name()</code>  <code>abstractmethod</code>","text":"<p>The name to display for this configuration in Kolena. Must be implemented when extending <code>EvaluatorConfiguration</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator","title":"<code>Evaluator(configurations=None)</code>","text":"<p>An <code>Evaluator</code> transforms inferences into metrics.</p> <p>Metrics are computed at the individual test sample level (<code>MetricsTestSample</code>), in aggregate at the test case level (<code>MetricsTestCase</code>), and across populations at the test suite level (<code>MetricsTestSuite</code>).</p> <p>Test-case-level plots (<code>Plot</code>) may also be computed.</p> <p>Parameters:</p> Name Type Description Default <code>configurations</code> <code>Optional[List[EvaluatorConfiguration]]</code> <p>The configurations at which to perform evaluation. Instance methods such as <code>compute_test_sample_metrics</code> are called once per test case per configuration.</p> <code>None</code>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.configurations","title":"<code>configurations: List[EvaluatorConfiguration] = configurations or []</code>  <code>instance-attribute</code>","text":"<p>The configurations with which to perform evaluation, provided on instantiation.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.display_name","title":"<code>display_name()</code>","text":"<p>The name to display for this evaluator in Kolena. Defaults to the name of this class.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_sample_metrics","title":"<code>compute_test_sample_metrics(test_case, inferences, configuration=None)</code>  <code>abstractmethod</code>","text":"<p>Compute metrics for every test sample in a test case, i.e. one <code>MetricsTestSample</code> object for each of the provided test samples.</p> <p>Must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The <code>TestCase</code> to which the provided test samples and ground truths belong.</p> required <code>inferences</code> <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>The test samples, ground truths, and inferences for all entries in a test case.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>The evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[TestSample, MetricsTestSample]]</code> <p><code>TestSample</code>-level metrics for each provided test sample.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_case_metrics","title":"<code>compute_test_case_metrics(test_case, inferences, metrics, configuration=None)</code>  <code>abstractmethod</code>","text":"<p>Compute aggregate metrics (<code>MetricsTestCase</code>) across a test case.</p> <p>Must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case in question.</p> required <code>inferences</code> <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>The test samples, ground truths, and inferences for all entries in a test case.</p> required <code>metrics</code> <code>List[MetricsTestSample]</code> <p>The <code>TestSample</code>-level metrics computed by <code>compute_test_sample_metrics</code>, provided in the same order as <code>inferences</code>.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>The evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>MetricsTestCase</code> <p><code>TestCase</code>-level metrics for the provided test case.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_case_plots","title":"<code>compute_test_case_plots(test_case, inferences, metrics, configuration=None)</code>","text":"<p>Optionally compute any number of plots to visualize the results for a test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case in question</p> required <code>inferences</code> <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>The test samples, ground truths, and inferences for all entries in a test case.</p> required <code>metrics</code> <code>List[MetricsTestSample]</code> <p>The <code>TestSample</code>-level metrics computed by <code>compute_test_sample_metrics</code>, provided in the same order as <code>inferences</code>.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>the evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[List[Plot]]</code> <p>Zero or more plots for this test case at this configuration.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_suite_metrics","title":"<code>compute_test_suite_metrics(test_suite, metrics, configuration=None)</code>","text":"<p>Optionally compute <code>TestSuite</code>-level metrics (<code>MetricsTestSuite</code>) across the provided <code>test_suite</code>.</p> <p>Parameters:</p> Name Type Description Default <code>test_suite</code> <code>TestSuite</code> <p>The test suite in question.</p> required <code>metrics</code> <code>List[Tuple[TestCase, MetricsTestCase]]</code> <p>The <code>TestCase</code>-level metrics computed by <code>compute_test_case_metrics</code>.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>The evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[MetricsTestSuite]</code> <p>The <code>TestSuite</code>-level metrics for this test suite.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.BasicEvaluatorFunction","title":"<code>BasicEvaluatorFunction = Union[ConfiguredEvaluatorFunction, UnconfiguredEvaluatorFunction]</code>  <code>module-attribute</code>","text":"<p><code>BasicEvaluatorFunction</code> provides a function-based evaluator interface that takes the inferences for all test samples in a test suite and a <code>TestCases</code> as input and computes the corresponding test-sample-level, test-case-level, and test-suite-level metrics (and optionally plots) as output.</p> <p>Example implementation, relying on <code>compute_per_sample</code> and <code>compute_aggregate</code> functions implemented elsewhere:</p> <pre><code>def evaluate(\ntest_samples: List[TestSample],\nground_truths: List[GroundTruth],\ninferences: List[Inference],\ntest_cases: TestCases,\n# configuration: EvaluatorConfiguration,  # uncomment when configuration is used\n) -&gt; EvaluationResults:\n# compute per-sample metrics for each test sample\nper_sample_metrics = [compute_per_sample(gt, inf) for gt, inf in zip(ground_truths, inferences)]\n# compute aggregate metrics across all test cases using `test_cases.iter(...)`\naggregate_metrics: List[Tuple[TestCase, MetricsTestCase]] = []\nfor test_case, *s in test_cases.iter(test_samples, ground_truths, inferences, per_sample_metrics):\n# subset of `test_samples`/`ground_truths`/`inferences`/`test_sample_metrics` in given test case\ntc_test_samples, tc_ground_truths, tc_inferences, tc_per_sample_metrics = s\naggregate_metrics.append((test_case, compute_aggregate(tc_per_sample_metrics)))\n# if desired, compute and add `plots_test_case` and `metrics_test_suite`\nreturn EvaluationResults(\nmetrics_test_sample=list(zip(test_samples, per_sample_metrics)),\nmetrics_test_case=aggregate_metrics,\n)\n</code></pre> <p>The control flow is in general more streamlined than with <code>Evaluator</code>, but requires a couple of assumptions to hold:</p> <ul> <li>Test-sample-level metrics do not vary by test case</li> <li>Ground truths corresponding to a given test sample do not vary by test case</li> </ul> <p>This <code>BasicEvaluatorFunction</code> is provided to the test run at runtime, and is expected to have the following signature:</p> <p>Parameters:</p> Name Type Description Default <code>test_samples</code> <code>List[TestSample]</code> <p>A list of distinct <code>TestSample</code> values that correspond to all test samples in the test run.</p> required <code>ground_truths</code> <code>List[GroundTruth]</code> <p>A list of <code>GroundTruth</code> values corresponding to and sequenced in the same order as <code>test_samples</code>.</p> required <code>inferences</code> <code>List[Inference]</code> <p>A list of <code>Inference</code> values corresponding to and sequenced in the same order as <code>test_samples</code>.</p> required <code>test_cases</code> <code>TestCases</code> <p>An instance of <code>TestCases</code>, used to provide iteration groupings for evaluating test-case-level metrics.</p> required <code>evaluator_configuration</code> <code>EvaluatorConfiguration</code> <p>The <code>EvaluatorConfiguration</code> to use when performing the evaluation. This parameter may be omitted in the function definition for implementations that do not use any configuration object.</p> required <p>Returns:</p> Type Description <code>EvaluationResults</code> <p>An <code>EvaluationResults</code> object tracking the test-sample-level, test-case-level and test-suite-level metrics and plots for the input collection of test samples.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.TestCases","title":"<code>TestCases</code>","text":"<p>Provides an iterator method for grouping test-sample-level metric results with the test cases that they belong to.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.TestCases.iter","title":"<code>iter(test_samples, ground_truths, inferences, metrics_test_sample)</code>  <code>abstractmethod</code>","text":"<p>Matches test sample metrics to the corresponding test cases that they belong to.</p> <p>Parameters:</p> Name Type Description Default <code>test_samples</code> <code>List[TestSample]</code> <p>All unique test samples within the test run, sequenced in the same order as the other parameters.</p> required <code>ground_truths</code> <code>List[GroundTruth]</code> <p>Ground truths corresponding to <code>test_samples</code>, sequenced in the same order.</p> required <code>inferences</code> <code>List[Inference]</code> <p>Inferences corresponding to <code>test_samples</code>, sequenced in the same order.</p> required <code>metrics_test_sample</code> <code>List[MetricsTestSample]</code> <p>Test-sample-level metrics corresponding to <code>test_samples</code>, sequenced in the same order.</p> required <p>Returns:</p> Type Description <code>Iterator[Tuple[TestCase, List[TestSample], List[GroundTruth], List[Inference], List[MetricsTestSample]]]</code> <p>Iterator that groups each test case in the test run to the lists of member test samples, inferences, and test-sample-level metrics.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults","title":"<code>EvaluationResults</code>","text":"<p>A bundle of metrics computed for a test run grouped at the test-sample-level, test-case-level, and test-suite-level. Optionally includes <code>Plot</code>s at the test-case-level.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.metrics_test_sample","title":"<code>metrics_test_sample: List[Tuple[BaseTestSample, BaseMetricsTestSample]]</code>  <code>instance-attribute</code>","text":"<p>Sample-level metrics, extending <code>MetricsTestSample</code>, for every provided test sample.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.metrics_test_case","title":"<code>metrics_test_case: List[Tuple[TestCase, MetricsTestCase]]</code>  <code>instance-attribute</code>","text":"<p>Aggregate metrics, extending <code>MetricsTestCase</code>, computed across each test case yielded from <code>TestCases.iter</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.plots_test_case","title":"<code>plots_test_case: List[Tuple[TestCase, List[Plot]]] = field(default_factory=list)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optional test-case-level plots.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.metrics_test_suite","title":"<code>metrics_test_suite: Optional[MetricsTestSuite] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optional test-suite-level metrics, extending <code>MetricsTestSuite</code>.</p>"},{"location":"reference/workflow/ground-truth/","title":"<code>kolena.workflow.GroundTruth</code>","text":"<p>The ground truth associated with a <code>TestSample</code>. Typically, a ground truth will represent the expected output of a model when given a test sample and will be manually annotated by a human.</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\nfrom kolena.workflow import GroundTruth\nfrom kolena.workflow.annotation import Polyline, SegmentationMask\n@dataclass(frozen=True)\nclass AvGroundTruth(GroundTruth):\nroad_area: SegmentationMask\nlane_boundaries: List[Polyline]\nvisibility_score: int\n</code></pre> <p>A <code>TestCase</code> holds a list of test samples (model inputs) paired with ground truths (expected outputs).</p>"},{"location":"reference/workflow/ground-truth/#kolena.workflow.ground_truth.GroundTruth","title":"<code>GroundTruth</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>The ground truth against which a model is evaluated.</p> <p>A test case contains one or more <code>TestSample</code> objects each paired with a ground truth object. During evaluation, these test samples, ground truths, and your model's inferences are provided to the <code>Evaluator</code> implementation.</p> <p>This object may contain any combination of scalars (e.g. <code>str</code>, <code>float</code>), <code>Annotation</code> objects, or lists of these objects.</p> <p>For <code>Composite</code>, each object can contain multiple basic test sample elements. To associate a set of attributes and/or annotations as the ground truth to a target test sample element, declare annotations by extending <code>DataObject</code> and use the same attribute name as used in the <code>Composite</code> test sample.</p> <p>Continue with the example given in <code>Composite</code>, where the <code>FacePairSample</code> test sample type is defined using a pair of images under the <code>source</code> and <code>target</code> members, we can design a corresponding ground truth type with image-level annotations defined in the <code>FaceRegion</code> object:</p> <pre><code>from dataclasses import dataclass\nfrom kolena.workflow import DataObject, GroundTruth\nfrom kolena.workflow.annotation import BoundingBox, Keypoints\n@dataclass(frozen=True)\nclass FaceRegion(DataObject):\nbounding_box: BoundingBox\nkeypoints: Keypoints\n@dataclass(frozen=True)\nclass FacePair(GroundTruth):\nsource: FaceRegion\ntarget: FaceRegion\nis_same_person: bool\n</code></pre> <p>This way, it is clear which bounding boxes and keypoints are associated to which image in the test sample.</p>"},{"location":"reference/workflow/inference/","title":"<code>kolena.workflow.Inference</code>","text":"<p>The output from a <code>Model</code>. In other words, a model is a deterministic transformation from a <code>TestSample</code> to an <code>Inference</code>.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Optional\nfrom kolena.workflow import Inference\nfrom kolena.workflow.annotation import Keypoints\n@dataclass(frozen=True)\nclass PoseEstimate(Inference):\nskeleton: Optional[Keypoints] = None  # leave empty if nothing is detected\nconfidence: Optional[float] = None\n</code></pre>"},{"location":"reference/workflow/inference/#kolena.workflow.inference.Inference","title":"<code>Inference</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>The inference produced by a model.</p> <p>Typically the structure of this object closely mirrors the structure of the <code>GroundTruth</code> for a workflow, but this is not a requirement.</p> <p>During evaluation, the <code>TestSample</code> objects, ground truth objects, and these inference objects are provided to the <code>Evaluator</code> implementation to compute metrics.</p> <p>This object may contain any combination of scalars (e.g. <code>str</code>, <code>float</code>), <code>Annotation</code> objects, or lists of these objects.</p> <p>A model processing a <code>Composite</code> test sample can produce an inference result for each of its elements. To associate an inference result to each test sample element, put the attributes and/or annotations inside a <code>DataObject</code> and use the same attribute name as that used in the <code>Composite</code> test sample.</p> <p>Continue with the example given in <code>Composite</code>, where the <code>FacePairSample</code> test sample type is defined using a pair of images under the <code>source</code> and <code>target</code> members, we can design a corresponding inference type with image-level annotations defined in the <code>FaceRegion</code> object:</p> <pre><code>from dataclasses import dataclass\nfrom kolena.workflow import DataObject, Inference\nfrom kolena.workflow.annotation import BoundingBox, Keypoints\n@dataclass(frozen=True)\nclass FaceRegion(DataObject):\nbounding_box: BoundingBox\nkeypoints: Keypoints\n@dataclass(frozen=True)\nclass FacePair(Inference):\nsource: FaceRegion\ntarget: FaceRegion\nsimilarity: float\n</code></pre> <p>This way, it is clear which bounding boxes and keypoints are associated to which image in the test sample.</p>"},{"location":"reference/workflow/metrics/","title":"<code>kolena.workflow.metrics</code>","text":""},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.InferenceMatches","title":"<code>InferenceMatches</code>","text":"<p>         Bases: <code>Generic[GT, Inf]</code></p> <p>The result of <code>match_inferences</code>, providing lists of matches between ground truth and inference objects, unmatched ground truths, and unmatched inferences. After applying some confidence threshold on returned inference objects, <code>InferenceMatches</code> can be used to calculate metrics such as precision and recall.</p> <p>Objects are of type <code>BoundingBox</code> or <code>Polygon</code>, depending on the type of inputs provided to <code>match_inferences</code>.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.InferenceMatches.matched","title":"<code>matched: List[Tuple[GT, Inf]]</code>  <code>instance-attribute</code>","text":"<p>Pairs of matched ground truth and inference objects above the IOU threshold. Considered as true positive detections after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.InferenceMatches.unmatched_gt","title":"<code>unmatched_gt: List[GT]</code>  <code>instance-attribute</code>","text":"<p>Unmatched ground truth objects. Considered as false negatives.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.InferenceMatches.unmatched_inf","title":"<code>unmatched_inf: List[Inf]</code>  <code>instance-attribute</code>","text":"<p>Unmatched inference objects. Considered as false positives after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.MulticlassInferenceMatches","title":"<code>MulticlassInferenceMatches</code>","text":"<p>         Bases: <code>Generic[GT_Multiclass, Inf_Multiclass]</code></p> <p>The result of <code>match_inferences_multiclass</code>, providing lists of matches between ground truth and inference objects, unmatched ground truths, and unmatched inferences.</p> <p>Unmatched ground truths may be matched with an inference of a different class when no inference of its own class is suitable, i.e. a \"confused\" match. <code>MultiClassInferenceMatches</code> can be used to calculate metrics such as precision and recall per class, after applying some confidence threshold on the returned inference objects.</p> <p>Objects are of type <code>LabeledBoundingBox</code> or <code>LabeledPolygon</code>, depending on the type of inputs provided to <code>match_inferences_multiclass</code>.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.MulticlassInferenceMatches.matched","title":"<code>matched: List[Tuple[GT_Multiclass, Inf_Multiclass]]</code>  <code>instance-attribute</code>","text":"<p>Pairs of matched ground truth and inference objects above the IOU threshold. Considered as true positive detections after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.MulticlassInferenceMatches.unmatched_gt","title":"<code>unmatched_gt: List[Tuple[GT_Multiclass, Optional[Inf_Multiclass]]]</code>  <code>instance-attribute</code>","text":"<p>Pairs of unmatched ground truth objects with its confused inference object (i.e. IOU above threshold with mismatching <code>label</code>), if such an inference exists. Considered as false negatives and \"confused\" detections.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.MulticlassInferenceMatches.unmatched_inf","title":"<code>unmatched_inf: List[Inf_Multiclass]</code>  <code>instance-attribute</code>","text":"<p>Unmatched inference objects. Considered as false positives after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.precision","title":"<code>precision(true_positives, false_positives)</code>","text":"<p>Precision represents the proportion of predictions that are correct.</p> \\[ \\text{Precision} = \\frac{\\text{# True Positives}}{\\text{# True Positives} + \\text{# False Positives}} \\] <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>int</code> <p>Number of true positive predictions.</p> required <code>false_positives</code> <code>int</code> <p>Number of false positive predictions.</p> required"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.recall","title":"<code>recall(true_positives, false_negatives)</code>","text":"<p>Recall represents the proportion of ground truths that were successfully predicted.</p> \\[ \\text{Recall} = \\frac{\\text{# True Positives}}{\\text{# True Positives} + \\text{# False Negatives}} \\] <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>int</code> <p>Number of true positive predictions.</p> required <code>false_negatives</code> <code>int</code> <p>Number of false negatives.</p> required"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.f1_score","title":"<code>f1_score(true_positives, false_positives, false_negatives)</code>","text":"<p>F1 score is the harmonic mean between <code>precision</code> and <code>recall</code>.</p> \\[ \\begin{align} \\text{F1} &amp;= \\frac{2}{\\frac{1}{\\text{Precision}} + \\frac{1}{\\text{Recall}}} \\\\[1em] &amp;= 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\end{align} \\] <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>int</code> <p>Number of true positive predictions.</p> required <code>false_positives</code> <code>int</code> <p>Number of false positive predictions.</p> required <code>false_negatives</code> <code>int</code> <p>Number of false negatives.</p> required"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.iou","title":"<code>iou(a, b)</code>","text":"<p>Compute the Intersection Over Union (IOU) of two geometries.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[BoundingBox, Polygon]</code> <p>The first geometry in computation.</p> required <code>b</code> <code>Union[BoundingBox, Polygon]</code> <p>The second geometry in computation.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The value of the IOU between geometries <code>a</code> and <code>b</code>.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.match_inferences","title":"<code>match_inferences(ground_truths, inferences, *, ignored_ground_truths=None, mode='pascal', iou_threshold=0.5)</code>","text":"<p>Matches model inferences with annotated ground truths using the provided configuration.</p> <p>This matcher does not consider labels, which is appropriate for single class object matching. To match with multiple classes (i.e. heeding <code>label</code> classifications), use the multiclass matcher <code>match_inferences_multiclass</code>.</p> <p>Available modes:</p> <ul> <li><code>pascal</code> (PASCAL VOC): For every inference by order of highest confidence, the ground truth of highest IOU is   its match. Multiple inferences are able to match with the same ignored ground truth. See the   PASCAL VOC paper for more information.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[Geometry]</code> <p>A list of <code>BoundingBox</code> or <code>Polygon</code> ground truths.</p> required <code>inferences</code> <code>List[ScoredGeometry]</code> <p>A list of <code>ScoredBoundingBox</code> or <code>ScoredPolygon</code> inferences.</p> required <code>ignored_ground_truths</code> <code>Optional[List[Geometry]]</code> <p>Optionally specify a list of <code>BoundingBox</code> or <code>Polygon</code> ground truths to ignore. These ignored ground truths and any inferences matched with them are omitted from the returned <code>InferenceMatches</code>.</p> <code>None</code> <code>mode</code> <code>Literal['pascal']</code> <p>The matching methodology to use. See available modes above.</p> <code>'pascal'</code> <code>iou_threshold</code> <code>float</code> <p>The IOU (intersection over union, see <code>iou</code>) threshold for valid matches.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>InferenceMatches[GT, Inf]</code> <p><code>InferenceMatches</code> containing the matches (true positives), unmatched ground truths (false negatives) and unmatched inferences (false positives).</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.match_inferences_multiclass","title":"<code>match_inferences_multiclass(ground_truths, inferences, *, ignored_ground_truths=None, mode='pascal', iou_threshold=0.5)</code>","text":"<p>Matches model inferences with annotated ground truths using the provided configuration.</p> <p>This matcher considers <code>label</code> values matching per class. After matching inferences and ground truths with equivalent <code>label</code> values, unmatched inferences and unmatched ground truths are matched once more to identify confused matches, where localization succeeded (i.e. IOU above <code>iou_threshold</code>) but classification failed (i.e. mismatching <code>label</code> values).</p> <p>Available modes:</p> <ul> <li><code>pascal</code> (PASCAL VOC): For every inference by order of highest confidence, the ground truth of highest IOU is   its match. Multiple inferences are able to match with the same ignored ground truth. See the   PASCAL VOC paper for more information.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[LabeledGeometry]</code> <p>A list of <code>LabeledBoundingBox</code> or <code>LabeledPolygon</code> ground truths.</p> required <code>inferences</code> <code>List[ScoredLabeledGeometry]</code> <p>A list of <code>ScoredLabeledBoundingBox</code> or <code>ScoredLabeledPolygon</code> inferences.</p> required <code>ignored_ground_truths</code> <code>Optional[List[LabeledGeometry]]</code> <p>Optionally specify a list of <code>LabeledBoundingBox</code> or <code>LabeledPolygon</code> ground truths to ignore. These ignored ground truths and any inferences matched with them are omitted from the returned <code>MulticlassInferenceMatches</code>.</p> <code>None</code> <code>mode</code> <code>Literal['pascal']</code> <p>The matching methodology to use. See available modes above.</p> <code>'pascal'</code> <code>iou_threshold</code> <code>float</code> <p>The IOU threshold cutoff for valid matches.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>MulticlassInferenceMatches[GT_Multiclass, Inf_Multiclass]</code> <p><code>MulticlassInferenceMatches</code> containing the matches (true positives), unmatched ground truths (false negatives), and unmatched inferences (false positives).</p>"},{"location":"reference/workflow/model/","title":"<code>kolena.worfklow.Model</code>","text":""},{"location":"reference/workflow/model/#kolena.workflow.model.Model","title":"<code>Model(name, infer=None, metadata=None)</code>","text":"<p>         Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>The descriptor of a model tested on Kolena. A model is a deterministic transformation from <code>TestSample</code> inputs to <code>Inference</code> outputs.</p> <p>Rather than importing this class directly, use the <code>Model</code> type definition returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.workflow","title":"<code>workflow: Workflow</code>  <code>instance-attribute</code>","text":"<p>The workflow of this model. Automatically populated when constructing via the model type returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Unique name of the model.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.metadata","title":"<code>metadata: Dict[str, Any]</code>  <code>instance-attribute</code>","text":"<p>Unstructured metadata associated with the model.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.infer","title":"<code>infer: Optional[Callable[[TestSample], Inference]]</code>  <code>instance-attribute</code>","text":"<p>Function transforming a <code>TestSample</code> for a workflow into an <code>Inference</code> object. Required when using <code>test</code> or <code>TestRun.run</code>.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.create","title":"<code>create(name, infer=None, metadata=None)</code>  <code>classmethod</code>","text":"<p>Create a new model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the new model to create.</p> required <code>infer</code> <code>Optional[Callable[[TestSample], Inference]]</code> <p>Optional inference function for this model.</p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Optional unstructured metadata to store with this model.</p> <code>None</code> <p>Returns:</p> Type Description <code>Model</code> <p>The newly created model.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.load","title":"<code>load(name, infer=None)</code>  <code>classmethod</code>","text":"<p>Load an existing model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the model to load.</p> required <code>infer</code> <code>Optional[Callable[[TestSample], Inference]]</code> <p>Optional inference function for this model.</p> <code>None</code>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.load_inferences","title":"<code>load_inferences(test_case)</code>","text":"<p>Load all inferences stored for this model on the provided test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case for which to load inferences.</p> required <p>Returns:</p> Type Description <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>The ground truths and inferences for all test samples in the test case.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.iter_inferences","title":"<code>iter_inferences(test_case)</code>","text":"<p>Iterate over all inferences stored for this model on the provided test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case over which to iterate inferences.</p> required <p>Returns:</p> Type Description <code>Iterator[Tuple[TestSample, GroundTruth, Inference]]</code> <p>Iterator exposing the ground truths and inferences for all test samples in the test case.</p>"},{"location":"reference/workflow/plot/","title":"Plots: <code>kolena.workflow.plot</code>","text":"<p>This module surfaces plot definitions to visualize test-case-level data. Evaluator implementations can optionally compute plots using these definitions for visualization on the   Results page.</p> <p>The following plot types are available:</p> <ul> <li><code>CurvePlot</code></li> <li><code>Histogram</code></li> <li><code>BarPlot</code></li> <li><code>ConfusionMatrix</code></li> </ul>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.NumberSeries","title":"<code>NumberSeries = Sequence[Union[float, int]]</code>  <code>module-attribute</code>","text":"<p>A sequence of numeric values.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.NullableNumberSeries","title":"<code>NullableNumberSeries = Sequence[Union[float, int, None]]</code>  <code>module-attribute</code>","text":"<p>A sequence of numeric values or <code>None</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.AxisConfig","title":"<code>AxisConfig</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>Configuration for the format of a given axis on a plot.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.AxisConfig.type","title":"<code>type: Literal[linear, log]</code>  <code>instance-attribute</code>","text":"<p>Type of axis to display. Supported options are <code>linear</code> and <code>log</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Plot","title":"<code>Plot</code>","text":"<p>         Bases: <code>TypedDataObject[_PlotType]</code></p> <p>A data visualization shown when exploring model results in the web platform.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Curve","title":"<code>Curve</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>A single series on a <code>CurvePlot</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Curve.x","title":"<code>x: NumberSeries</code>  <code>instance-attribute</code>","text":"<p>The <code>x</code> coordinates of this curve. Length must match the provided <code>y</code> coordinates.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Curve.y","title":"<code>y: NumberSeries</code>  <code>instance-attribute</code>","text":"<p>The <code>y</code> coordinates of this curve. Length must match the provided <code>x</code> coordinates.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Curve.label","title":"<code>label: Optional[str] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally specify an additional label (in addition to the associated test case) to apply to this curve, for use when e.g. there are multiple curves generated per test case.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot","title":"<code>CurvePlot</code>","text":"<p>         Bases: <code>Plot</code></p> <p>A plot visualizing one or more curves per test case.</p> <p>Examples include Receiver Operating Characteristic (ROC) curves, Precision versus Recall (PR) curves, Detection-Error Tradeoff (DET) curves, etc.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The title for the plot.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.x_label","title":"<code>x_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>x</code> axis.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.y_label","title":"<code>y_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>y</code> axis.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.curves","title":"<code>curves: List[Curve]</code>  <code>instance-attribute</code>","text":"<p>A test case may generate zero or more curves on a given plot. However, under most circumstances, a single curve per test case is desirable.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.x_config","title":"<code>x_config: Optional[AxisConfig] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>x</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.y_config","title":"<code>y_config: Optional[AxisConfig] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>y</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram","title":"<code>Histogram</code>","text":"<p>         Bases: <code>Plot</code></p> <p>A plot visualizing distribution of one or more continuous values, e.g. distribution of an error metric across all samples within a test case.</p> <p>For visualization of discrete values, see <code>BarPlot</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The title for the plot.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.x_label","title":"<code>x_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>x</code> axis.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.y_label","title":"<code>y_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>y</code> axis.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.buckets","title":"<code>buckets: NumberSeries</code>  <code>instance-attribute</code>","text":"<p>A Histogram requires intervals to bucket the data. For <code>n</code> buckets, <code>n+1</code> consecutive bounds must be specified in increasing order.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.frequency","title":"<code>frequency: Union[NumberSeries, Sequence[NumberSeries]]</code>  <code>instance-attribute</code>","text":"<p>For <code>n</code> buckets, there are <code>n</code> frequencies corresponding to the height of each bucket. The frequency at index <code>i</code> corresponds to the bucket with bounds (<code>i</code>, <code>i+1</code>) in <code>buckets</code>.</p> <p>To specify multiple distributions for a given test case, multiple frequency series can be provided, corresponding e.g. to the distribution for a given class within a test case, with name specified in <code>labels</code>.</p> <p>Specify a list of labels corresponding to the different <code>frequency</code> series when multiple series are provided. Can be omitted when a single <code>frequency</code> series is provided.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.labels","title":"<code>labels: Optional[List[str]] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Specify the label corresponding to a given distribution when multiple are specified in <code>frequency</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.x_config","title":"<code>x_config: Optional[AxisConfig] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>x</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.y_config","title":"<code>y_config: Optional[AxisConfig] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>y</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot","title":"<code>BarPlot</code>","text":"<p>         Bases: <code>Plot</code></p> <p>A plot visualizing a set of bars per test case.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The plot title.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.x_label","title":"<code>x_label: str</code>  <code>instance-attribute</code>","text":"<p>Axis label for the axis along which the bars are laid out (<code>labels</code>).</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.y_label","title":"<code>y_label: str</code>  <code>instance-attribute</code>","text":"<p>Axis label for the axis corresponding to bar height (<code>values</code>).</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.labels","title":"<code>labels: Sequence[Union[str, int, float]]</code>  <code>instance-attribute</code>","text":"<p>Labels for each bar with corresponding height specified in <code>values</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.values","title":"<code>values: NullableNumberSeries</code>  <code>instance-attribute</code>","text":"<p>Values for each bar with corresponding label specified in <code>labels</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.config","title":"<code>config: Optional[AxisConfig] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Custom format options to allow for control over the display of the numerical plot axis (<code>values</code>).</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix","title":"<code>ConfusionMatrix</code>","text":"<p>         Bases: <code>Plot</code></p> <p>A confusion matrix. Example:</p> <pre><code>ConfusionMatrix(\ntitle=\"Cat and Dog Confusion\",\nlabels=[\"Cat\", \"Dog\"],\nmatrix=[[90, 10], [5, 95]],\n)\n</code></pre> <p>Yields a confusion matrix of the form:</p> <pre><code>            Predicted\n\n            Cat   Dog\n           +----+----+\n       Cat | 90 | 10 |\nActual     +----+----+\n       Dog |  5 | 95 |\n           +----+----+\n</code></pre>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The plot title.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.labels","title":"<code>labels: List[str]</code>  <code>instance-attribute</code>","text":"<p>The labels corresponding to each entry in the square <code>matrix</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.matrix","title":"<code>matrix: Sequence[NullableNumberSeries]</code>  <code>instance-attribute</code>","text":"<p>A square matrix, typically representing the number of matches between class <code>i</code> and class <code>j</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.x_label","title":"<code>x_label: str = 'Predicted'</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>The label for the <code>x</code> axis of the confusion matrix.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.y_label","title":"<code>y_label: str = 'Actual'</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>The label for the <code>y</code> axis of the confusion matrix.</p>"},{"location":"reference/workflow/test-case/","title":"<code>kolena.workflow.TestCase</code>","text":""},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase","title":"<code>TestCase(name, version=None, description=None, test_samples=None, reset=False)</code>","text":"<p>         Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A test case holds a list of test samples paired with ground truths representing a testing dataset or a slice of a testing dataset.</p> <p>Rather than importing this class directly, use the <code>TestCase</code> type definition returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.workflow","title":"<code>workflow: Workflow</code>  <code>instance-attribute</code>","text":"<p>The workflow of this test case. Automatically populated when constructing via test case type returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>The unique name of this test case. Cannot be changed after creation.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.version","title":"<code>version: int</code>  <code>instance-attribute</code>","text":"<p>The version of this test case. A test case's version is automatically incremented whenever it is edited via <code>TestCase.edit</code>.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Free-form, human-readable description of this test case. Can be edited at any time via <code>TestCase.edit</code>.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.Editor","title":"<code>Editor(description, reset)</code>","text":""},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.Editor.description","title":"<code>description(description)</code>","text":"<p>Update the description of the test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.Editor.add","title":"<code>add(test_sample, ground_truth)</code>","text":"<p>Add a test sample to the test case. When the test sample already exists in the test case, its ground truth is overwritten with the ground truth provided here.</p> <p>Parameters:</p> Name Type Description Default <code>test_sample</code> <code>TestSample</code> <p>The test sample to add.</p> required <code>ground_truth</code> <code>GroundTruth</code> <p>The ground truth for the test sample.</p> required"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.Editor.remove","title":"<code>remove(test_sample)</code>","text":"<p>Remove a test sample from the test case. Does nothing if the test sample is not in the test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_sample</code> <code>TestSample</code> <p>The test sample to remove.</p> required"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.create","title":"<code>create(name, description=None, test_samples=None)</code>  <code>classmethod</code>","text":"<p>Create a new test case with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new test case to create.</p> required <code>description</code> <code>Optional[str]</code> <p>Optional free-form description of the test case to create.</p> <code>None</code> <code>test_samples</code> <code>Optional[List[Tuple[TestSample, GroundTruth]]]</code> <p>Optionally specify a set of test samples and ground truths to populate the test case.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestCase</code> <p>The newly created test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.load","title":"<code>load(name, version=None)</code>  <code>classmethod</code>","text":"<p>Load an existing test case with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test case to load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify a particular version of the test case to load. Defaults to the latest version when unset.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestCase</code> <p>The loaded test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.load_test_samples","title":"<code>load_test_samples()</code>","text":"<p>Load all <code>TestSample</code>s and <code>GroundTruth</code>s contained in this test case.</p> <p>Returns:</p> Type Description <code>List[Tuple[TestSample, GroundTruth]]</code> <p>A list of each test sample, paired with its ground truth, in this test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.iter_test_samples","title":"<code>iter_test_samples()</code>","text":"<p>Iterate through all <code>TestSample</code>s and <code>GroundTruth</code>s contained in this test case.</p> <p>Returns:</p> Type Description <code>Iterator[Tuple[TestSample, GroundTruth]]</code> <p>An iterator yielding each test sample, paired with its ground truth, in this test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.edit","title":"<code>edit(reset=False)</code>","text":"<p>Edit this test case in a context:</p> <pre><code>with test_case.edit() as editor:\n# perform as many editing actions as desired\neditor.add(...)\neditor.remove(...)\n</code></pre> <p>Changes are committed to the Kolena platform when the context is exited.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>Clear any and all test samples currently in the test case.</p> <code>False</code>"},{"location":"reference/workflow/test-run/","title":"<code>kolena.workflow.test</code>","text":""},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun","title":"<code>TestRun(model, test_suite, evaluator=None, configurations=None, reset=False)</code>","text":"<p>         Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A <code>Model</code> tested on a <code>TestSuite</code> using a specific <code>Evaluator</code> implementation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model being tested.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>evaluator</code> <code>Union[Evaluator, BasicEvaluatorFunction, None]</code> <p>An optional evaluator implementation. Requires a previously configured server-side evaluator to default to if omitted. (Please see <code>BasicEvaluatorFunction</code> for type definition.)</p> <code>None</code> <code>configurations</code> <code>Optional[List[EvaluatorConfiguration]]</code> <p>a list of configurations to use when running the evaluator.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>overwrites existing inferences if set.</p> <code>False</code>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.run","title":"<code>run()</code>","text":"<p>Run the testing process, first extracting inferences for all test samples in the test suite then performing evaluation.</p>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.load_test_samples","title":"<code>load_test_samples()</code>","text":"<p>Load the test samples in the test suite that do not yet have inferences uploaded.</p> <p>Returns:</p> Type Description <code>List[TestSample]</code> <p>a list of all test samples in the test suite still requiring inferences.</p>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.iter_test_samples","title":"<code>iter_test_samples()</code>","text":"<p>Iterate through the test samples in the test suite that do not yet have inferences uploaded.</p> <p>Returns:</p> Type Description <code>Iterator[TestSample]</code> <p>an iterator over each test sample still requiring inferences.</p>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.upload_inferences","title":"<code>upload_inferences(inferences)</code>","text":"<p>Upload inferences from a model.</p> <p>Parameters:</p> Name Type Description Default <code>inferences</code> <code>List[Tuple[TestSample, Inference]]</code> <p>the inferences, paired with their corresponding test samples, to upload.</p> required"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.evaluate","title":"<code>evaluate()</code>","text":"<p>Perform evaluation by computing metrics for individual test samples, in aggregate across test cases, and across the complete test suite at each <code>EvaluatorConfiguration</code>.</p>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.test","title":"<code>test(model, test_suite, evaluator=None, configurations=None, reset=False)</code>","text":"<p>Test a <code>Model</code> on a <code>TestSuite</code> using a specific <code>Evaluator</code> implementation.</p> <pre><code>from kolena.workflow import test\ntest(model, test_suite, evaluator, reset=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model being tested, implementing the <code>infer</code> method.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>evaluator</code> <code>Union[Evaluator, BasicEvaluatorFunction, None]</code> <p>An optional evaluator implementation. Requires a previously configured server-side evaluator to default to if omitted. (Please see <code>BasicEvaluatorFunction</code> for type definition.)</p> <code>None</code> <code>configurations</code> <code>Optional[List[EvaluatorConfiguration]]</code> <p>A list of configurations to use when running the evaluator.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>Overwrites existing inferences if set.</p> <code>False</code>"},{"location":"reference/workflow/test-sample/","title":"<code>kolena.workflow.TestSample</code>","text":"<p>Test samples are the inputs to your models when testing.</p> <p>For example, for a model that processes specific regions within a larger image, its test sample may be defined:</p> <pre><code>from dataclasses import dataclass\nfrom kolena.workflow import Image\nfrom kolena.workflow.annotation import BoundingBox\n@dataclass(frozen=True)\nclass ImageWithRegion(Image):\nregion: BoundingBox\nexample = ImageWithRegion(\nlocator=\"s3://my-bucket/example-image.png\",  # field from Image base class\nregion=BoundingBox(top_left=(0, 0), bottom_right=(100, 100)),\n)\n</code></pre>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Metadata","title":"<code>Metadata = Dict[str, Union[None, StrictStr, StrictFloat, StrictInt, StrictBool, str, float, int, bool, List[Union[None, StrictStr, StrictFloat, StrictInt, StrictBool, str, float, int, bool]]]]</code>  <code>module-attribute</code>","text":"<p>Type of the <code>metadata</code> field that can be included on <code>TestSample</code> definitions. String (<code>str</code>) keys and scalar values (<code>int</code>, <code>float</code>, <code>str</code>, <code>bool</code>, <code>None</code>) as well as scalar list values are permitted.</p> <pre><code>from dataclasses import dataclass, field\nfrom kolena.workflow import Image, Metadata\n@dataclass(frozen=True)\nclass ImageWithMetadata(Image):\nmetadata: Metadata = field(default_factory=dict)\n</code></pre>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.TestSample","title":"<code>TestSample</code>","text":"<p>         Bases: <code>TypedDataObject[_TestSampleType]</code></p> <p>The inputs to a model.</p> <p>Test samples can be customized as necessary for a workflow by extending this class or one of the built-in test sample types.</p> <p>Extensions to the <code>TestSample</code> class may define a <code>metadata</code> field of type <code>Metadata</code> containing a dictionary of scalar properties associated with the test sample, intended for use when sorting or filtering test samples.</p> <p>Kolena handles the <code>metadata</code> field differently from other test sample fields. Updates to the <code>metadata</code> object for a given test sample are merged with previously uploaded metadata. As such, <code>metadata</code> for a given test sample within a test case is not immutable, and should not be relied on when an implementation of <code>Model</code> computes inferences, or when an implementation of <code>Evaluator</code> evaluates metrics.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Composite","title":"<code>Composite</code>","text":"<p>         Bases: <code>TestSample</code></p> <p>A test sample composed of multiple basic <code>TestSample</code> elements.</p> <p>An example application would be each test sample is a pair of face images, and the goal is to predict whether the two images are of the same person. For this use-case the test sample can be defined as:</p> <pre><code>class FacePairSample(Composite):\nsource: Image\ntarget: Image\n</code></pre> <p>To facilitate visualization for this kind of use cases, see usage of <code>GroundTruth</code> and <code>Inference</code>.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Image","title":"<code>Image</code>","text":"<p>         Bases: <code>TestSample</code></p> <p>An image located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Image.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The URL of this image, using e.g. <code>s3</code>, <code>gs</code>, or <code>https</code> scheme (<code>s3://my-bucket/path/to/image.png</code>).</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImagePair","title":"<code>ImagePair</code>","text":"<p>         Bases: <code>Composite</code></p> <p>Two <code>Image</code>s paired together.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImagePair.a","title":"<code>a: Image</code>  <code>instance-attribute</code>","text":"<p>The left <code>Image</code> in the image pair.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImagePair.b","title":"<code>b: Image</code>  <code>instance-attribute</code>","text":"<p>The right <code>Image</code> in the image pair.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Text","title":"<code>Text</code>","text":"<p>         Bases: <code>TestSample</code></p> <p>An inline text snippet.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Text.text","title":"<code>text: str</code>  <code>instance-attribute</code>","text":"<p>The text snippet.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImageText","title":"<code>ImageText</code>","text":"<p>         Bases: <code>Composite</code></p> <p>An image paired with a text snippet.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImageText.image","title":"<code>image: Image</code>  <code>instance-attribute</code>","text":"<p>The <code>Image</code> in this image-text pair.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImageText.text","title":"<code>text: Text</code>  <code>instance-attribute</code>","text":"<p>The text snippet in this image-text pair.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.BaseVideo","title":"<code>BaseVideo</code>","text":"<p>         Bases: <code>TestSample</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.BaseVideo.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video","title":"<code>Video</code>","text":"<p>         Bases: <code>BaseVideo</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video.thumbnail","title":"<code>thumbnail: Optional[ImageAsset] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally provide asset locator for custom video thumbnail.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video.start","title":"<code>start: Optional[float] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally specify start time of video snippet, in seconds.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video.end","title":"<code>end: Optional[float] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally specify end time of video snippet, in seconds.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Document","title":"<code>Document</code>","text":"<p>         Bases: <code>TestSample</code></p> <p>A remotely linked document, e.g. PDF or TXT file.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Document.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the document.</p>"},{"location":"reference/workflow/test-suite/","title":"<code>kolena.workflow.TestSuite</code>","text":""},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite","title":"<code>TestSuite(name, version=None, description=None, test_cases=None, reset=False, tags=None)</code>","text":"<p>         Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A test suite groups together one or more test cases. Typically a test suite represents a benchmark test dataset, with test cases representing different meaningful subsets, or slices, or this benchmark.</p> <p>Rather than importing this class directly, use the <code>TestSuite</code> type definition returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.workflow","title":"<code>workflow: Workflow</code>  <code>instance-attribute</code>","text":"<p>The workflow of this test suite. Automatically populated when constructing via test suite type returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>The unique name of this test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.version","title":"<code>version: int</code>  <code>instance-attribute</code>","text":"<p>The version of this test suite. A test suite's version is automatically incremented whenever it is edited via <code>TestSuite.edit</code>.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Free-form, human-readable description of this test suite. Can be edited at any time via <code>TestSuite.edit</code>.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.test_cases","title":"<code>test_cases: List[TestCase]</code>  <code>instance-attribute</code>","text":"<p>The <code>TestCase</code> objects belonging to this test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.tags","title":"<code>tags: Set[str]</code>  <code>instance-attribute</code>","text":"<p>The tags associated with this test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.Editor","title":"<code>Editor(test_cases, description, tags, reset)</code>","text":""},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.Editor.description","title":"<code>description(description)</code>","text":"<p>Update the description of the test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.Editor.add","title":"<code>add(test_case)</code>","text":"<p>Add a test case to this test suite. If a different version of the test case already exists in this test suite, it is replaced.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case to add to the test suite.</p> required"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.Editor.remove","title":"<code>remove(test_case)</code>","text":"<p>Remove a test case from this test suite. Does nothing if the test case is not in the test suite.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case to remove.</p> required"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.create","title":"<code>create(name, description=None, test_cases=None, tags=None)</code>  <code>classmethod</code>","text":"<p>Create a new test suite with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new test suite to create.</p> required <code>description</code> <code>Optional[str]</code> <p>Optional free-form description of the test suite to create.</p> <code>None</code> <code>test_cases</code> <code>Optional[List[TestCase]]</code> <p>Optionally specify a list of test cases to populate the test suite.</p> <code>None</code> <code>tags</code> <code>Optional[Set[str]]</code> <p>Optionally specify a set of tags to attach to the test suite.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestSuite</code> <p>The newly created test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.load","title":"<code>load(name, version=None)</code>  <code>classmethod</code>","text":"<p>Load an existing test suite with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test suite to load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify a particular version of the test suite to load. Defaults to the latest version when unset.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestSuite</code> <p>The loaded test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.load_all","title":"<code>load_all(*, tags=None)</code>  <code>classmethod</code>","text":"<p>Load the latest version of all non-archived test suites with this workflow.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Optional[Set[str]]</code> <p>Optionally specify a set of tags to apply as a filter. The loaded test suites will include only test suites with tags matching each of these specified tags, i.e. <code>test_suite.tags.intersection(tags) == tags</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[TestSuite]</code> <p>The latest version of all non-archived test suites, with matching tags when specified.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.edit","title":"<code>edit(reset=False)</code>","text":"<p>Edit this test suite in a context:</p> <pre><code>with test_suite.edit() as editor:\n# perform as many editing actions as desired\neditor.add(...)\neditor.remove(...)\n</code></pre> <p>Changes are committed to the Kolena platform when the context is exited.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>Clear any and all test cases currently in the test suite.</p> <code>False</code>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.load_test_samples","title":"<code>load_test_samples()</code>","text":"<p>Load test samples for all test cases within this test suite.</p> <p>Returns:</p> Type Description <code>List[Tuple[TestCase, List[TestSample]]]</code> <p>A list of <code>TestCase</code>s, each paired with the list of <code>TestSample</code>s it contains.</p>"}]}