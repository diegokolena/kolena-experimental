{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Developer Guide","text":"<p>Kolena is a comprehensive machine learning testing and debugging platform to surface hidden model behaviors and take the mystery out of model development. Kolena helps you:</p> <ul> <li>Perform high-resolution model evaluation</li> <li>Understand and track behavioral improvements and regressions</li> <li>Meaningfully communicate model capabilities</li> <li>Automate model testing and deployment workflows</li> </ul> <p>Kolena organizes your test data, stores and visualizes your model evaluations, and provides tooling to craft better tests. You interface with it through the web at app.kolena.io and programmatically via the <code>kolena</code> Python client.</p>"},{"location":"#why-kolena","title":"Why Kolena?","text":"<p>TL;DR</p> <p>Kolena helps you test your ML models more effectively.</p> <p>Jump right in with the   Quickstart guide.</p> <p>Current ML evaluation techniques are falling short. Engineers run inference on arbitrarily split benchmark datasets, spend weeks staring at error graphs to evaluate their models, and ultimately produce a global metric that fails to capture the true behavior of the model.</p> <p>Models exhibit highly variable performance across different subsets of a domain. A global metric gives you a high-level picture of performance but doesn't tell you what you really want to know: what sort of behavior can I expect from my model in production?</p> <p>To answer this question you need a higher-resolution picture of model performance. Not \"how well does my model perform on class X,\" but \"in what scenarios does my model perform well for class X?\"</p> <p> </p> <p>In the above example, looking only at global metric (e.g. F1 score), we'd almost certainly choose to deploy Model B.</p> <p>But what if the \"High Blur\" scenario isn't important for our product? Most of Model A's failures are from that scenario, and it outperforms Model B in more important scenarios like \"Front View.\" Meanwhile, Model B's underperformance in \"Front View,\" a highly important scenario, is masked by improved performance in the unimportant \"High Blur\" scenario.</p> <p>Test data is more important than training data!</p> <p>Everything you know about a new model's behavior is learned from your tests.</p> <p>Fine-grained tests teach you what you need to learn before a model hits production.</p> <p>Now... why Kolena? Two reasons:</p> <ol> <li>Managing fine-grained tests is a tedious data engineering task, especially under changing data circumstances as    your dataset grows and your understanding of your domain develops</li> <li>Creating fine-grained tests is labor-intensive and typically involves manual annotation of countless images, a    costly and time-consuming process</li> </ol> <p>We built Kolena to solve these two problems.</p>"},{"location":"#read-more","title":"Read More","text":"<ul> <li>Best Practices for ML Model Testing (Kolena Blog)</li> <li>Hidden Stratification Causes Clinically Meaningful Failures in Machine Learning for Medical Imaging (arXiv:1909.12475)</li> <li>No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems (arXiv:2011.12945)</li> </ul>"},{"location":"#developer-guide","title":"Developer Guide","text":"<p>Learn how to use Kolena to test your models effectively:</p> <ul> <li> <p>  Quickstart</p> <p>Run through an example using Kolena to set up rigorous and repeatable model testing in minutes.</p> </li> <li> <p>  Installing <code>kolena</code></p> <p>Install and initialize the <code>kolena</code> Python package, the programmatic interface to Kolena.</p> </li> <li> <p>  Building a Workflow</p> <p>Learn how to use <code>kolena.workflow</code> to test any arbitrary ML problem on Kolena.</p> </li> <li> <p>  Core Concepts</p> <p>Core concepts for testing in Kolena.</p> </li> <li> <p>  Advanced Usage</p> <p>Tutorial documentation for advanced features available in Kolena.</p> </li> <li> <p>  API Reference</p> <p>Developer-focused detailed API reference documentation for <code>kolena</code>.</p> </li> </ul>"},{"location":"building-a-workflow/","title":"Building a Workflow","text":"<p>In this tutorial we'll learn how to use the <code>kolena.workflow</code> workflow builder definitions to test a Keypoint Detection model on the 300-W facial keypoint dataset. This demonstration will show us how we can build a workflow to test any arbitrary ML problem on Kolena.</p>","boost":2},{"location":"building-a-workflow/#getting-started","title":"Getting Started","text":"<p>With the <code>kolena</code> Python client installed, first let's initialize a client session:</p> <pre><code>import os\nimport kolena\nkolena.initialize(os.environ[\"KOLENA_TOKEN\"], verbose=True)\n</code></pre> <p>The data used in this tutorial is publicly available in the <code>kolena-public-datasets</code> S3 bucket in a <code>metadata.csv</code> file:</p> <pre><code>import pandas as pd\nDATASET = \"300-W\"\nBUCKET = \"s3://kolena-public-datasets\"\ndf = pd.read_csv(f\"{BUCKET}/{DATASET}/meta/metadata.csv\")\n</code></pre> <p>Note: <code>s3fs</code> dependency</p> <p>To load CSVs directly from S3, make sure to install the <code>s3fs</code> Python module: <code>pip3 install s3fs[boto3]</code> and set up AWS credentials.</p> <p>This <code>metadata.csv</code> file describes a keypoint detection dataset with the following columns:</p> <p>Note: Five-point facial keypoints array</p> <p>For brevity, the 300-W dataset has been pared down to only 5 keypoints: outermost corner of each eye, bottom of nose, and corners of the mouth.</p> <p> Example image and five-point facial keypoints array from 300-W. </p> <ul> <li><code>locator</code>: location of the image in S3</li> <li><code>normalization_factor</code>: normalization factor of the image. This is used to normalize the error by providing a     factor for each image. Common techniques for computation include the Euclidean distance between two points or the     diagonal measurement of the image.</li> <li><code>points</code>: stringified list of coordinates corresponding to the <code>(x, y)</code> coordinates of the keypoint ground truths</li> </ul> <p>Each <code>locator</code> is present exactly one time and contains the keypoint ground truth for that image. In this tutorial, we're implementing our workflow with support for only a single keypoint instance per image, but we could easily adapt our ground truth, inference, and metrics types to accommodate a variable number of keypoint arrays per image.</p>","boost":2},{"location":"building-a-workflow/#step-1-defining-data-types","title":"Step 1: Defining Data Types","text":"<p>When building your own workflow you have control over the <code>TestSample</code> (e.g. image), <code>GroundTruth</code> (e.g. 5-element facial keypoint array), and <code>Inference</code> types used in your project.</p>","boost":2},{"location":"building-a-workflow/#test-sample-type","title":"Test Sample Type","text":"<p>For the purposes of this tutorial, let's assume our model takes a single image as input along with an optional bounding box around the face in question, produced by an upstream model in our pipeline. We can import and extend the <code>kolena.workflow.Image</code> test sample type for this purpose:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Optional\nfrom kolena.workflow import Image\nfrom kolena.workflow.annotation import BoundingBox\n@dataclass(frozen=True)\nclass TestSample(Image):\nbbox: Optional[BoundingBox] = None\n</code></pre>","boost":2},{"location":"building-a-workflow/#ground-truth-type","title":"Ground Truth Type","text":"<p>Next, let's define our <code>GroundTruth</code> type, typically containing the manually-annotated information necessary to evaluate model inferences:</p> <pre><code>from kolena.workflow import GroundTruth as GT\nfrom kolena.workflow.annotation import Keypoints\n@dataclass(frozen=True)\nclass GroundTruth(GT):\nkeypoints: Keypoints\n# In order to compute normalized error, some normalization factor describing\n# the size of the face in the image is required.\nnormalization_factor: float\n</code></pre>","boost":2},{"location":"building-a-workflow/#inference-type","title":"Inference Type","text":"<p>Lastly, we'll define our <code>Inference</code> type, containing model outputs that are evaluated against a ground truth. Note that our model produces not only a <code>Keypoints</code> array, but also an associated <code>confidence</code> value that we may use to ignore low-confidence predictions:</p> <pre><code>from kolena.workflow import Inference as Inf\n@dataclass(frozen=True)\nclass Inference(Inf):\nkeypoints: Keypoints\nconfidence: float\n</code></pre> <p>With our test sample, ground truth, and inference defined, we can now use <code>define_workflow</code> to declare our workflow:</p> <pre><code>from kolena.workflow import define_workflow\n# use these TestCase, TestSuite, and Model definitions to create and run tests\n_, TestCase, TestSuite, Model = define_workflow(\n\"Keypoint Detection\", TestSample, GroundTruth, Inference\n)\n</code></pre>","boost":2},{"location":"building-a-workflow/#step-2-defining-metrics","title":"Step 2: Defining Metrics","text":"<p>With our core data types defined, the next step is to lay out our evaluation criteria: our metrics.</p>","boost":2},{"location":"building-a-workflow/#test-sample-metrics","title":"Test Sample Metrics","text":"<p>Test Sample Metrics (<code>MetricsTestSample</code>) are metrics computed from a single test sample and its associated ground truths and inferences.</p> <p>For the keypoint detection workflow, an example metric may be normalized mean error (NME), the normalized distance between the ground truth and inference keypoints.</p> <pre><code>from kolena.workflow import MetricsTestSample\n@dataclass(frozen=True)\nclass TestSampleMetrics(MetricsTestSample):\nnormalized_mean_error: float\n# If the normalized mean error is above some configured threshold, this test\n# sample is considered an \"alignment failure\".\nalignment_failure: bool\n</code></pre>","boost":2},{"location":"building-a-workflow/#test-case-metrics","title":"Test Case Metrics","text":"<p>Test case metrics (<code>MetricsTestCase</code>) are aggregate metrics computed across a population. All of your standard evaluation metrics should go here \u2014 things like accuracy, precision, recall, or any other aggregate metrics that apply to your problem.</p> <p>For keypoint detection, we care about the mean NME and alignment failure rate across the different test samples in a test case:</p> <pre><code>from kolena.workflow import MetricsTestCase\n@dataclass(frozen=True)\nclass TestCaseMetrics(MetricsTestCase):\nmean_nme: float\nalignment_failure_rate: float\n</code></pre> <p>Tip: Plots</p> <p>Evaluators can also compute test-case-level plots using the <code>Plot</code> API. These plots are visualized on the   Results dashboard alongside the metrics reported for each test case.</p> <p>Tip: Test Suite Metrics</p> <p>Metrics can also be computed per test suite by extending <code>MetricsTestSuite</code>.</p> <p>Test suite metrics typically measure variance in performance across different test cases, being used e.g. to measure fairness across demographics for a test suite with test cases stratifying by demographic.</p>","boost":2},{"location":"building-a-workflow/#step-3-creating-tests","title":"Step 3: Creating Tests","text":"<p>With our data already in an S3 bucket and metadata loaded into memory, we can start creating test cases!</p> <p>Let's create a simple test case containing the entire dataset:</p> <pre><code>import json\ntest_samples = [TestSample(locator) for locator in df[\"locator\"]]\nground_truths = [\nGroundTruth(\nkeypoints=Keypoints(points=json.loads(record.points)),\nnormalization_factor=record.normalization_factor,\n)\nfor record in df.itertuples()\n]\nts_with_gt = list(zip(test_samples, ground_truths))\ntest_case = TestCase(f\"{DATASET} :: basic\", test_samples=ts_with_gt)\n</code></pre> <p>Note: Creating test cases</p> <p>In this tutorial we created only a single simple test case, but more advanced test cases can be generated in a variety of fast and scalable ways, either programmatically with the <code>kolena</code> Python client or visually in the   Studio.</p> <p>Now that we have a basic test case for our entire dataset let's create a test suite for it:</p> <pre><code>test_suite = TestSuite(f\"{DATASET} :: basic\", test_cases=[test_case])\n</code></pre>","boost":2},{"location":"building-a-workflow/#step-4-running-tests","title":"Step 4: Running Tests","text":"<p>With basic tests defined for the 300-W dataset, we're almost ready to start testing our models.</p>","boost":2},{"location":"building-a-workflow/#implementing-an-evaluator","title":"Implementing an Evaluator","text":"<p>Core to the testing process is the <code>Evaluator</code> implementation to compute the metrics defined in step 2. Usually, an evaluator simply plugs your existing metrics computation logic into the class-based or function-based evaluator interface.</p> <p>Evaluators can have arbitrary configuration (<code>EvaluatorConfiguration</code>), allowing you to evaluate model performance under a variety of conditions. For this keypoint detection example, perhaps we want to compute performance at a few different NME threshold values, as this threshold drives the <code>alignment_failure</code> metric.</p> <pre><code>from kolena.workflow import EvaluatorConfiguration\n@dataclass(frozen=True)\nclass NmeThreshold(EvaluatorConfiguration):\n# threshold for NME above which an image is considered an \"alignment failure\"\nthreshold: float\ndef display_name(self):\nreturn f\"NME threshold: {self.threshold}\"\n</code></pre> <p>Here, we'll mock out an evaluator implementation using the function-based interface:</p> <pre><code>from random import random\nfrom typing import List\nfrom kolena.workflow import EvaluationResults, TestCases\ndef evaluate_keypoint_detection(\ntest_samples: List[TestSample],\nground_truths: List[GroundTruth],\ninferences: List[Inference],\ntest_cases: TestCases,\nconfiguration: NmeThreshold,  # uncomment when configuration is used\n) -&gt; EvaluationResults:\n# compute per-sample metrics for each test sample\nper_sample_metrics = [\nTestSampleMetrics(normalized_mean_error=random(), alignment_failure=bool(random() &gt; 0.5))\nfor gt, inf in zip(ground_truths, inferences)\n]\n# compute aggregate metrics across all test cases using `test_cases.iter(...)`\naggregate_metrics = []\nfor test_case, *s in test_cases.iter(test_samples, ground_truths, inferences, per_sample_metrics):\ntest_case_metrics = TestCaseMetrics(mean_nme=random(), alignment_failure_rate=random())\naggregate_metrics.append((test_case, test_case_metrics))\nreturn EvaluationResults(\nmetrics_test_sample=list(zip(test_samples, per_sample_metrics)),\nmetrics_test_case=aggregate_metrics,\n)\n</code></pre>","boost":2},{"location":"building-a-workflow/#running-tests","title":"Running tests","text":"<p>To test our models, we can define an <code>infer</code> function that maps the <code>TestSample</code> object we defined above into an <code>Inference</code>:</p> <pre><code>from random import randint\ndef infer(test_sample: TestSample) -&gt; Inference:\n\"\"\"\n    1. load the image pointed to at `test_sample.locator`\n    2. pass the image to our model and transform its output into an `Inference` object\n    \"\"\"\n# Generate the dummy inference for the demo purpose.\nreturn Inference(Keypoints([(randint(100, 400), randint(100, 400)) for _ in range(5)]), random())\nmodel = Model(\"example-model-name\", infer=infer, metadata=dict(\ndescription=\"Any freeform metadata can go here\",\nhint=\"It may be helpful to include information about the model's framework, training methodology, dataset, etc.\",\n))\n</code></pre> <p>We now have the pieces in place to run tests on our new workflow using <code>test</code>:</p> <pre><code>from kolena.workflow import test\ntest(\nmodel,\ntest_suite,\nevaluate_keypoint_detection,\nconfigurations=[NmeThreshold(0.01), NmeThreshold(0.05), NmeThreshold(0.1)],\n)\n</code></pre> <p>That wraps up the testing process! We can now visit   Results to analyze and debug our model's performance on this test suite.</p>","boost":2},{"location":"building-a-workflow/#conclusion","title":"Conclusion","text":"<p>In this tutorial we learned how to build a workflow for an arbitrary ML problem, using a facial keypoint detection model as an example. We created new tests, tested our models on Kolena, and learned how to customize evaluation to fit our exact expectations.</p> <p>This tutorial just scratches the surface of what's possible with Kolena and covered a fraction of the <code>kolena</code> API \u2014 now that we're up and running, we can think about ways to create more detailed tests, improve existing tests, and dive deep into model behaviors.</p>","boost":2},{"location":"installing-kolena/","title":"Installing <code>kolena</code>","text":"<p>Testing on Kolena is conducted using the <code>kolena</code> Python package. You use the client to create and run tests from your infrastructure that can be explored in our web platform.</p> <p><code>kolena</code> is released under the open-source Apache-2.0 license. The package is hosted on PyPI and can be installed using your preferred Python package manager.</p>"},{"location":"installing-kolena/#installation","title":"Installation","text":"<p>The first step to start testing with Kolena is to install <code>kolena</code>. Client builds can be installed directly from PyPI using any Python package manager such as pip or Poetry:</p> <code>pip</code><code>poetry</code> <pre><code>pip install kolena\n</code></pre> <pre><code>poetry add kolena\n</code></pre> <p>Note</p> <p>Kolena uses <code>pydantic</code> for data validation, but is compatible only with <code>pydantic</code> V1. Prior to <code>0.76.0</code>, the <code>kolena</code> client did not set an upper bound on compatible <code>pydantic</code> version, and as a result, fresh installations of older versions of <code>kolena</code> may install incompatible version of <code>pydantic</code>, resulting in errors such as:</p> <pre><code>AttributeError: type object 'SingleProcessResponse' has no attribute '__pydantic_model__'\n</code></pre> <p>If you see the error above, please install the latest version of <code>kolena</code>.</p>"},{"location":"installing-kolena/#extra-dependency-groups","title":"Extra Dependency Groups","text":"<p>Certain metrics computation functionality depends on additional packages like scikit-learn. These extra dependencies can be installed via the <code>metrics</code> group:</p> <code>pip</code><code>poetry</code> <pre><code>pip install 'kolena[metrics]'\n</code></pre> <pre><code>poetry add 'kolena[metrics]'\n</code></pre>"},{"location":"installing-kolena/#initialization","title":"Initialization","text":"<p>Once you have <code>kolena</code> installed, initialize a session with <code>kolena.initialize(token)</code>.</p> <p>From the   Developer page, generate an API token and set the <code>KOLENA_TOKEN</code> environment variable:</p> <pre><code>export KOLENA_TOKEN=\"********\"\n</code></pre> <p>With the <code>KOLENA_TOKEN</code> environment variable set, initialize a client session:</p> <pre><code>import os\nimport kolena\nkolena.initialize(os.environ[\"KOLENA_TOKEN\"], verbose=True)\n</code></pre> <p>By default, sessions have static scope and persist until the interpreter is exited. Additional logging can be configured by specifying <code>initialize(..., verbose=True)</code>.</p> <p>Tip: <code>logging</code></p> <p>Integrate <code>kolena</code> into your existing logging system by filtering for events from the <code>\"kolena\"</code> logger. All log messages are emitted as both Python standard library <code>logging</code> events as well as stdout/stderr messages.</p>"},{"location":"installing-kolena/#supported-python-versions","title":"Supported Python Versions","text":"<p><code>kolena</code> is compatible with all active Python versions.</p>  Python Version Compatible <code>kolena</code> Versions 3.11 \u22650.69 3.10 All Versions 3.9 All Versions 3.8 All Versions 3.7 All Versions 3.6 (EOL: December 2021) \u22640.46"},{"location":"quickstart/","title":"Quickstart","text":"<p>Install Kolena to set up rigorous and repeatable model testing in minutes.</p> <p>In this quickstart guide, we'll use the <code>age_estimation</code> example integration to demonstrate the how to curate test data and test models in Kolena.</p>"},{"location":"quickstart/#install-kolena","title":"Install <code>kolena</code>","text":"<p>Install the <code>kolena</code> Python package to programmatically interact with Kolena:</p> <code>pip</code><code>poetry</code> <pre><code>pip install kolena\n</code></pre> <pre><code>poetry add kolena\n</code></pre>"},{"location":"quickstart/#clone-the-examples","title":"Clone the Examples","text":"<p>The kolenaIO/kolena repository contains a number of example integrations to clone and run directly:</p> <ul> <li> <p>  Example: Age Estimation \u2197</p> <p></p> <p>Age Estimation using the Labeled Faces in the Wild (LFW) dataset</p> </li> <li> <p>  Example: Keypoint Detection \u2197</p> <p></p> <p>Facial Keypoint Detection using the 300 Faces in the Wild (300-W) dataset</p> </li> <li> <p> array-string  Example: Text Summarization \u2197</p> <p></p> <p>Text Summarization using OpenAI GPT-family models and the CNN-DailyMail dataset</p> </li> <li> <p>  Example: Object Detection (2D) \u2197</p> <p></p> <p>2D Object Detection using the COCO dataset</p> </li> <li> <p>  Example: Object Detection (3D) \u2197</p> <p></p> <p>3D Object Detection using the KITTI dataset</p> </li> <li> <p>  Example: Binary Classification</p> <p></p> <p>Binary Classification of class \"Dog\" using the Dogs vs. Cats dataset</p> </li> <li> <p>  Example: Multiclass Classification</p> <p></p> <p>Multiclass Classification using the CIFAR-10 dataset</p> </li> </ul> <p>To get started, clone the <code>kolena</code> repository:</p> <pre><code>git clone https://github.com/kolenaIO/kolena.git\n</code></pre> <p>With the repository cloned, let's set up the <code>age_estimation</code> example:</p> <pre><code>cd kolena/examples/age_estimation\npoetry update &amp;&amp; poetry install\n</code></pre> <p>Now we're up and running and can start creating test suites and testing models.</p>"},{"location":"quickstart/#create-test-suites","title":"Create Test Suites","text":"<p>Each of the example integrations comes with scripts for two flows:</p> <ol> <li><code>seed_test_suite.py</code>: Create test cases and test suite(s) from a source dataset</li> <li><code>seed_test_run.py</code>: Test model(s) on the created test suites</li> </ol> <p>Before running <code>seed_test_suite.py</code>, let's first configure our environment by populating the <code>KOLENA_TOKEN</code> environment variable. Visit the   Developer page to generate an API token and copy and paste the code snippet into your environment:</p> <pre><code>export KOLENA_TOKEN=\"********\"\n</code></pre> <p>We can now create test suites using the provided seeding script:</p> <pre><code>poetry run python3 age_estimation/seed_test_suite.py\n</code></pre> <p>After this script has completed, we can visit the   Test Suites page to view our newly created test suites.</p> <p>In this <code>age_estimation</code> example, we've created test suites stratifying the LFW dataset (which is stored as a CSV in S3) into test cases by age, estimated race, and estimated gender.</p>"},{"location":"quickstart/#test-a-model","title":"Test a Model","text":"<p>After we've created test suites, the final step is to test models on these test suites. The <code>age_estimation</code> example provides the <code>ssrnet</code> model for this step:</p> <pre><code>poetry run python3 age_estimation/seed_test_run.py \\\n\"ssrnet\" \\\n\"age :: labeled-faces-in-the-wild [age estimation]\" \\\n\"race :: labeled-faces-in-the-wild [age estimation]\" \\\n\"gender :: labeled-faces-in-the-wild [age estimation]\"\n</code></pre> <p>Note: Testing additional models</p> <p>In this example, model results have already been extracted and are stored in CSV files in S3. To run a new model, plug it into the <code>infer</code> method in <code>seed_test_run.py</code>.</p> <p>Once this script has completed, click the results link in your console or visit   Results to view the test results for this newly tested model.</p>"},{"location":"quickstart/#conclusion","title":"Conclusion","text":"<p>In this quickstart, we used an example integration from kolenaIO/kolena to create test suites from the Labeled Faces in the Wild (LFW) dataset and test the open-source <code>ssrnet</code> model on these test suites.</p> <p>This example shows us how to define an ML problem as a workflow for testing in Kolena, and can be arbitrarily extended with additional metrics, plots, visualizations, and data.</p>"},{"location":"advanced-usage/","title":"Advanced Usage","text":"<p>This section contains tutorial documentation for advanced features available in Kolena.</p> <ul> <li> <p> Connecting Cloud Storage</p> <p>Establish integrations to cloud storage providers.</p> </li> </ul> <ul> <li> <p> Packaging for Automated Evaluation</p> <p>Package metrics evaluation logic in a Docker container image to dynamically compute metrics on relevant subsets of your test data.</p> </li> </ul> <ul> <li> <p>  Nesting Test Case Metrics</p> <p>Report class-level metrics within a test case and test ensembles and pipelines of models by nesting aggregate metrics within your <code>MetricsTestCase</code>.</p> </li> </ul> <ul> <li> <p>  Uploading Activation Maps</p> <p>Upload and visualize your activation map for each <code>TestSample</code> along with your model results on the   Studio.</p> </li> </ul>"},{"location":"advanced-usage/nesting-test-case-metrics/","title":"Nesting Test Case Metrics","text":"<p>When computing test case metrics in an evaluator, in some cases it is desirable to compute multiple sets of aggregate metrics within a given test case.</p> <p> </p> <p>Class-level metrics for the <code>airplane</code>, <code>bear</code>, <code>bench</code>, etc. classes reported for the test case <code>complete :: coco-2014-val [Object Detection]</code></p> <p>Here are a few examples of scenarios where this pattern might be warranted:</p> Use Case Description Multiclass workflows For ML tasks with multiple classes, a given test case may contain samples from more than one class. While it's useful to report metrics aggregated across all classes using an averaging method, it's also useful to see aggregate metrics computed for each of the classes. Ensembles of models When testing an ensemble containing multiple models, it can be useful to see metrics from the output of the complete ensemble as well as metrics computed for each of the constituent models. Model pipelines When testing a pipeline of models, in which one model's output is used as an input for the next model, it can be difficult to understand where along the pipeline performance broke down. Reporting overall metrics as well as per-model metrics for each model in the pipeline (the metrics used can differ from one model to the next!) can help pinpoint the cause of failures within a pipeline. <p>In these cases, Kolena provides the API to nest additional aggregate metrics records within a <code>MetricsTestCase</code> object returned from an evaluator. In this tutorial, we'll learn how to use this API to report class-level or other nested test case metrics for our models.</p>"},{"location":"advanced-usage/nesting-test-case-metrics/#example-multiclass-object-detection","title":"Example: Multiclass Object Detection","text":"<p>Let's consider the case of a multiclass object detection task with objects of type <code>Airplane</code>, <code>Boat</code>, and <code>Car</code>. When a test case contains images with each of these three classes, test-case-level metrics are the average (e.g. micro, macro, or weighted) of class-level metrics across each of these three classes.</p> <p>For this workflow, we may consider using macro-averaged precision, recall, and F1 score, and mean average precision score (mAP) across all images as our metrics:</p> Test Case # Images <code>macro_Precision</code> <code>macro_Recall</code> <code>macro_F1</code> <code>mAP</code> Scenario A 2,500 0.91 0.99 0.95 0.97 Scenario B 1,500 0.83 0.96 0.89 0.91 <p>At the API level, these metrics would be defined:</p> <pre><code>from dataclasses import dataclass\nfrom kolena.workflow import MetricsTestCase\n@dataclass(frozen=True)\nclass AggregateMetrics(MetricsTestCase):\n# Test Case, # Images are automatically populated\nmacro_Precision: float\nmacro_Recall: float\nmacro_F1: float\nmAP: float\n</code></pre> <p>These metrics tell us how well the model performs in \"Scenario A\" and \"Scenario B\" across all classes, but they don't tell us anything about per-class model performance. Within each test case, we'd also like to see precision, recall, F1, and AP scores:</p> <code>Class</code> <code>N</code> <code>Precision</code> <code>Recall</code> <code>F1</code> <code>AP</code> <code>Airplane</code> 1,000 0.5 0.5 0.5 0.5 <code>Boat</code> 500 0.5 0.5 0.5 0.5 <code>Car</code> 2,000 0.5 0.5 0.5 0.5 <p>We can report these class-level metrics alongside the macro-averaged overall metrics by nesting <code>MetricsTestCase</code> definitions:</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\nfrom kolena.workflow import MetricsTestCase\n@dataclass(frozen=True)\nclass PerClassMetrics(MetricsTestCase):\nClass: str  # name of the class corresponding to this record\nN: int  # number of samples containing this class\nPrecision: float\nRecall: float\nF1: float\nAP: float\n@dataclass(frozen=True)\nclass AggregateMetrics(MetricsTestCase):\n# Test Case, # Images are automatically populated\nmacro_Precision: float\nmacro_Recall: float\nmacro_F1: float\nmAP: float\nPerClass: List[PerClassMetrics]\n</code></pre> <p>Now we have the definitions to tell us everything we need to know about model performance within a test case: <code>AggregateMetrics</code> describes overall performance across all classes within the test case, and <code>PerClassMetrics</code> describes performance for each of the given classes within the test case.</p>"},{"location":"advanced-usage/nesting-test-case-metrics/#naming-nested-metric-records","title":"Naming Nested Metric Records","text":"<p>When defining nested metrics, e.g. <code>PerClassMetrics</code> in the example above, it's important to identify each row by including at least one <code>str</code>-type column. This column, e.g. <code>Class</code> above, is pinned to the left when displaying nested metrics on the   Results page.</p>"},{"location":"advanced-usage/nesting-test-case-metrics/#statistical-significance","title":"Statistical Significance","text":"<p>When comparing models, Kolena highlights performance improvements and regressions that are likely to be statistically significant. The number of samples being evaluated factors into these calculations.</p> <p>For nested metrics, certain fields like <code>N</code> in the above <code>PerClassMetrics</code> example are used as the population size for statistical significance calculations. To ensure that highlighted improvements and regressions in these nested metrics are statistically significant, populate this field for each class reported. In the above example, <code>N</code> can be populated with the number of images containing a certain class (good) or with the number of instances of that class across all images in the test case (better).</p> <p>For a full list of reserved field names for statistical significance calculations, see the API reference documentation for <code>MetricsTestCase</code>.</p>"},{"location":"advanced-usage/nesting-test-case-metrics/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we learned how to use the <code>MetricsTestCase</code> API to define class-level metrics within a test case. Nesting test case metrics is desirable for workflows with multiple classes, as well as when testing ensembles of models or testing model pipelines.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/","title":"Packaging for Automated Evaluation","text":""},{"location":"advanced-usage/packaging-for-automated-evaluation/#introduction","title":"Introduction","text":"<p>In addition to analyzing and debugging model performance, we can also use the Kolena platform to create and curate test cases and test suites. Kolena can automatically compute metrics on it for any models that have already uploaded inferences. In this guide, we'll learn how to package our custom metrics engine such that it can be used in this automatic evaluation process.</p> <p>To enable automatic metrics computation when applicable, we need to package the metrics evaluation logic into a Docker image that the Kolena platform can run. The following sections explain how to build this Docker image and link it for metrics computation on the Kolena platform.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#build-evaluator-docker-image","title":"Build Evaluator Docker Image","text":"<p>We will use the keypoint detection workflow we've built in the Building a Workflow guide to illustrate the process. Here is the project structure:</p> <pre><code>.\n\u251c\u2500\u2500 docker/\n\u2502   \u251c\u2500\u2500 build.sh\n\u2502   \u251c\u2500\u2500 publish.sh\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 keypoint_detection/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 evaluator.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 workflow.py\n\u251c\u2500\u2500 poetry.lock\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>The <code>keypoint_detection</code> directory is where our workflow is defined, with evaluator logic in <code>evaluator.py</code> and workflow data objects in <code>workflow.py</code>. The <code>main.py</code> will be the entry point where <code>test</code> is executed.</p> <p>From the workflow building guide, we know that metrics evaluation using <code>test</code> involves a <code>model</code>, a <code>test_suite</code>, an <code>evaluator</code>, and optional <code>configurations</code>:</p> <pre><code>test(model, test_suite, evaluator, configurations=configurations)\n</code></pre> <p>Note: test invocation</p> <p>Ensure that <code>reset=True</code> is NOT used in the <code>test</code> method when you only want to re-evaluate metrics and do not have the model <code>infer</code> logic built in the image. The flag would overwrite existing inference and metrics results of the test suite, therefore requires re-running model <code>infer</code> on the test samples.</p> <p>When executing <code>test</code> locally, the model and test suite can be initiated by user inputs. When Kolena executes <code>test</code> under automation, this information would have to be obtained through environment variables. Kolena sets up following environment variables for evaluator execution:</p> <ul> <li><code>KOLENA_MODEL_NAME</code></li> <li><code>KOLENA_TEST_SUITE_NAME</code></li> <li><code>KOLENA_TEST_SUITE_VERSION</code></li> <li><code>KOLENA_TOKEN</code></li> </ul> <p>The main script would therefore be adjusted like code sample below.</p> keypoint_detection/main.py<pre><code>import os\nimport kolena\nfrom kolena.workflow import test\nfrom .evaluator import evaluate_keypoint_detection, NmeThreshold\nfrom .workflow import Model, TestSuite\ndef main() -&gt; None:\nkolena.initialize(os.environ[\"KOLENA_TOKEN\"], verbose=True)\nmodel = Model(os.environ[\"KOLENA_MODEL_NAME\"])\ntest_suite = TestSuite.load(\nos.environ[\"KOLENA_TEST_SUITE_NAME\"],\nos.environ[\"KOLENA_TEST_SUITE_VERSION\"],\n)\ntest(model, test_suite, evaluate_keypoint_detection, configurations=[NmeThreshold(0.05)])\nif __name__ == \"__main__\":\nmain()\n</code></pre> <p>Now that we have the main script ready, the next step is to package this script into a Docker image.</p> docker/Dockerfile<pre><code>FROM python:3.9-slim AS base\nWORKDIR /opt/keypoint_detection/\nFROM base AS builder\nARG KOLENA_TOKEN\nENV POETRY_VIRTUALENVS_IN_PROJECT=true \\\nPOETRY_NO_INTERACTION=1\nRUN python3 -m pip install poetry\n\nCOPY pyproject.toml poetry.lock ./\nCOPY keypoint_detection ./keypoint_detection\nRUN poetry install --only main\n\nFROM base\nCOPY --from=builder /opt/keypoint_detection /opt/keypoint_detection/\nCOPY --from=builder /opt/keypoint_detection/.venv .venv/\n\nENTRYPOINT [ \"/opt/keypoint_detection/.venv/bin/python\", \"keypoint_detection/main.py\" ]\n</code></pre> docker/build.sh<pre><code>#!/usr/bin/env bash\nset -eu\n\nIMAGE_NAME=\"keypoint_detection_evaluator\"\nIMAGE_VERSION=\"v1\"\nIMAGE_TAG=\"$IMAGE_NAME:$IMAGE_VERSION\"\necho \"building $IMAGE_TAG...\"\nexport DOCKER_BUILDKIT=1\nexport COMPOSE_DOCKER_CLI_BUILD=1\ndocker build \\\n--tag \"$IMAGE_TAG\" \\\n--file \"docker/Dockerfile\" \\\n--build-arg KOLENA_TOKEN=${KOLENA_TOKEN} \\\n.\n</code></pre> <p>This build process installs the <code>kolena</code> package, and as such needs the <code>KOLENA_TOKEN</code> environment variable to be populated with your Kolena API key. Follow the <code>kolena</code> Python client guide to obtain an API key if you have not done so.</p> <pre><code>export KOLENA_TOKEN=\"&lt;kolena-api-token&gt;\"\n./docker/build.sh\n</code></pre>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#register-evaluator-for-workflow","title":"Register Evaluator for Workflow","text":"<p>The final step is to publish the Docker image and associate the image with the <code>Keypoint Detection</code> workflow.</p> <p>Kolena supports metrics computation using Docker image hosted on any public Docker registry or Kolena's Docker registry. In this tutorial, we will publish our image to Kolena's Docker registry. However, the steps should be easy to adapt to public Docker registry.</p> <p>The repositories on Kolena Docker registry must be prefixed with the organization name. This is to protect unauthorized access from unintended parties. Replace <code>&lt;organization&gt;</code> in <code>publish.sh</code> script with the actual organization name and run it. This would push our Docker image to the repository and register it for the workflow.</p> docker/publish.sh<pre><code>#!/usr/bin/env bash\nset -eu\n\nIMAGE_NAME=\"keypoint_detection_evaluator\"\nIMAGE_VERSION=\"v1\"\nIMAGE_TAG=\"${IMAGE_NAME}:${IMAGE_VERSION}\"\nDOCKER_REGISTRY=\"docker.kolena.io\"\nWORKFLOW=\"Keypoint Detection\"\nEVALUATOR_NAME=\"evaluate_keypoint_detection\"\nORGANIZATION=&lt;organization&gt;\n\nTARGET_IMAGE_TAG=\"$DOCKER_REGISTRY/$ORGANIZATION/$IMAGE_TAG\"\n# create repository if not exist\npoetry run kolena repository create --name \"$ORGANIZATION/$IMAGE_NAME\"\necho $KOLENA_TOKEN | docker login -u \"$ORGANIZATION\" --password-stdin $DOCKER_REGISTRY\necho \"publishing $TARGET_IMAGE_TAG...\"\ndocker tag $IMAGE_TAG $TARGET_IMAGE_TAG\ndocker push $TARGET_IMAGE_TAG\necho \"registering image $TARGET_IMAGE_TAG for evaluator $EVALUATOR_NAME of workflow $WORKFLOW...\"\npoetry run kolena evaluator register \\\n--workflow \"$WORKFLOW\" \\\n--evaluator-name \"$EVALUATOR_NAME\" \\\n--image $TARGET_IMAGE_TAG\n</code></pre> <pre><code>./docker/publish.sh\n</code></pre> <p>In <code>publish.sh</code>, we used Kolena client SDK command-line <code>kolena</code> to associate the Docker image to evaluator <code>evaluate_keypoint_detection</code> of workflow <code>Keypoint Detection</code>. You can find out more of its usage with the <code>--help</code> option.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#using-automatic-metrics-evaluation","title":"Using Automatic Metrics Evaluation","text":"<p>At this point, we are all set to leverage Kolena's automatic metrics evaluation capability. To see it in action, let's first use Kolena's Studio to curate a new test case.</p> <p>Head over to the   Studio and use the \"Explore\" tab to learn more about the test samples from a given test case. Select multiple test samples of interest and then go to the \"Create\" tab to create a new test case with the \"Create Test Case\" button. You will notice there's an option to compute metrics on this new test case for applicable models. Since we have the evaluator image registered for our workflow <code>Keypoint Detection</code>, Kolena will automatically compute metrics for the new case if this option is checked. After the computation completes, metrics of the new test case are immediately ready for us to analyze on the Results page.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we learned how to configure Kolena to automatically compute metrics when applicable, and why it brings values to model testing and analyzing process. We can use these tools to continue improving our test cases and our models.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#appendix","title":"Appendix","text":""},{"location":"advanced-usage/packaging-for-automated-evaluation/#evaluator-runtime-limits","title":"Evaluator runtime limits","text":"<p>Currently, the environment evaluator runs in does not support GPU. There is a maximum of 6 hours processing time. The evaluation job would be terminated when the run time reaches the limit.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#testing-evaluator-locally","title":"Testing evaluator locally","text":"<p>You can verify the evaluator Docker image by running it locally:</p> <pre><code>docker run --rm \\\n-e KOLENA_TEST_SUITE_NAME=\"${EXISTING_TEST_SUITE_NAME}\" \\\n-e KOLENA_TEST_SUITE_VERSION=3 \\\n-e KOLENA_MODEL_NAME=\"example keypoint detection model\" \\\n-e KOLENA_WORKFLOW=\"Keypoint Detection\" \\\n-e KOLENA_TOKEN=$KOLENA_TOKEN \\\n&lt;evaluator-docker-image&gt;\n</code></pre> <p>You can find a test suite's version on the   Test Suites page. By default, the latest version is displayed.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#using-dockerkolenaio","title":"Using docker.kolena.io","text":"<p>In this tutorial, we published an evaluator container image to <code>docker.kolena.io</code>, Kolena's Docker Registry. In this section, we'll explain how to use the Docker CLI to interact with <code>docker.kolena.io</code>.</p> <p>The first step is to use <code>docker login</code> to log into <code>docker.kolena.io</code>. Using your organization's name (e.g. <code>my-organization</code>, the part after <code>app.kolena.io</code> when you visit the app) as a username and your API token as a password, log in with the following command:</p> <pre><code>echo $KOLENA_TOKEN | docker login --username my-organization --password-stdin docker.kolena.io\n</code></pre> <p>Once you've successfully logged in, you can use Docker CLI to perform actions on the Kolena Docker registry. For example, to pull a previously published Docker image, use a command like:</p> <pre><code>docker pull docker.kolena.io/my-organization/&lt;docker-image-tag&gt;\n</code></pre> <p>If you're building Docker images for a new workflow, use the <code>kolena</code> command-line tool to create the repository on <code>docker.kolena.io</code> first. As mentioned in Register Evaluator for Workflow, the repository must be prefixed with your organization's name.</p> <pre><code>poetry run kolena repository create -n my-organization/new-evaluator\n</code></pre> <p>After the repository is created, we can use the Docker CLI to publish a newly built Docker image to <code>docs.kolena.io</code>:</p> <pre><code>docker push docker.kolena.io/my-organization/new-evaluator:v1\n</code></pre>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#using-secrets-in-your-evaluator","title":"Using Secrets in your Evaluator","text":"<p>If secret or sensitive data is used in your evaluation process, Kolena's secret manager can store this securely and pass it as the environment variable <code>KOLENA_EVALUATOR_SECRET</code> at runtime.</p> <p>Update the evaluator register command in <code>docker/publish.sh</code> to pass in sensitive data for the evaluator:</p> <pre><code>poetry run kolena evaluator register --workflow \"$WORKFLOW\" \\\n--evaluator-name \"$EVALUATOR_NAME\" \\\n--image $TARGET_IMAGE_TAG \\\n--secret '&lt;your secret&gt;'\n</code></pre>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#using-aws-apis-in-your-evaluator","title":"Using AWS APIs in your Evaluator","text":"<p>If your evaluator requires access to AWS APIs, specify the full AWS role ARN it should use in the evaluator register command.</p> <pre><code>poetry run kolena evaluator register --workflow \"$WORKFLOW\" \\\n--evaluator-name \"$EVALUATOR_NAME\" \\\n--image $TARGET_IMAGE_TAG \\\n--aws-assume-role &lt;target_role_arn&gt;\n</code></pre> <p>The output of the command would look like:</p> <pre><code>{\n\"workflow\": \"Keypoint Detection\",\n\"name\": \"evaluate_keypoint_detection\",\n\"image\": \"docker.kolena.io/my-organization/keypoint_detection_evaluator:v1\",\n\"created\": \"2023-04-03 16:18:10.703 -0700\",\n\"secret\": null,\n\"aws_role_config\": {\n\"job_role_arn\": \"&lt;Kolena AWS role ARN&gt;\",\n\"external_id\": \"&lt;Generated external_id&gt;\",\n\"assume_role_arn\": \"&lt;target_role_arn&gt;\"\n}\n}\n</code></pre> <p>The response includes the AWS role ARN that Kolena will use to run the evaluator Docker image, <code>aws_role_config.job_role_arn</code>, and the external_id, <code>aws_role_config.externa_id</code>, to verify that requests are made from Kolena.</p> <p>To allow Kolena's AWS role to assume the target role in your AWS account, you need to configure the trust policy of the target role. Here is an example of the trust policy.</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [{\n\"Effect\": \"Allow\",\n\"Action\": [\"sts:AssumeRole\"],\n\"Principal\": {\n\"AWS\": \"&lt;Kolena AWS role ARN&gt;\"\n},\n\"Condition\": {\n\"StringEquals\": {\n\"sts:ExternalId\": \"&lt;External_id generated by Kolena&gt;\"\n}\n}\n}]\n}\n</code></pre> <p>Please refer to AWS documents for details on Delegate access across AWS accounts using IAM roles.</p> <p>At runtime, Kolena would pass in the target role and the <code>external_id</code> in environment variables <code>KOLENA_EVALUATOR_ASSUME_ROLE_ARN</code> and <code>KOLENA_EVALUATOR_EXTERNAL_ID</code>, respectively. The evaluator would then use AWS assume-role to transit into the intended target role, and use AWS APIs under the new role.</p> <pre><code>import os\nimport boto3\nresponse = boto3.client(\"sts\").assume_role(\nRoleArn=os.environ[\"KOLENA_EVALUATOR_ASSUME_ROLE_ARN\"],\nExternalId=os.environ[\"KOLENA_EVALUATOR_EXTERNAL_ID\"],\nRoleSessionName=\"metrics-evaluator\",\n)\ncredentials = response[\"Credentials\"]\n</code></pre> <p>An example of making AWS API requests under the assumed role is shown below.</p> <pre><code># use credentials to initialize AWS sessions/clients\nclient = boto3.client(\n\"s3\",\naws_access_key_id=credentials[\"AccessKeyId\"],\naws_secret_access_key=credentials[\"SecretAccessKey\"],\naws_session_token=credentials[\"SessionToken\"],\n)\n</code></pre>"},{"location":"advanced-usage/uploading-activation-maps/","title":"Uploading Activation Maps","text":"<p>As models continue to grow larger and more complex, it is increasingly difficult to understand the reasoning behind their decisions or predictions. Understanding why a model produced a specific output is a process called Explainable AI (XAI) and can help data scientists and engineers comprehend and trust model results.</p> <p>There are many explanation methods for different model architectures. Most of the popular techniques used in computer vision workflows output a map that highlights regions in an image that are relevant to the model output. This map is called an activation map.</p> <p> </p> <p>Visualization of an activation map overlaid on an image</p>"},{"location":"advanced-usage/uploading-activation-maps/#popular-interpretation-methods-for-computer-vision","title":"Popular Interpretation Methods for Computer Vision","text":"<p>There are various methodologies that facilitate and aid the interpretation of several computer vision models, and if you are interested in learning more about them, here is a list of some of the popular methods:</p> <ul> <li>Vanilla Gradient (Saliency Maps)</li> <li>Class Activation Mapping (CAM)</li> <li>Gradient-weighted Class Activation Mapping (Grad-CAM) \u2014 PyTorch tutorial</li> </ul>"},{"location":"advanced-usage/uploading-activation-maps/#can-i-visualize-activation-maps-on-kolena","title":"Can I Visualize Activation Maps on Kolena?","text":"<p>Yes! Activation maps can be visualized as an overlay on the corresponding image in   Studio using the <code>BitmapMask</code> annotation type which can help us understand the model\u2019s decision \u2014 what the model \u201csees\u201d when it makes its prediction.</p> <p>In this tutorial, we\u2019ll learn how to upload and visualize activation maps as a part of testing models on Kolena.</p>"},{"location":"advanced-usage/uploading-activation-maps/#how-to-upload-activation-maps-on-kolena","title":"How to Upload Activation Maps on Kolena?","text":"<p>Uploading activation maps to Kolena can be done in three simple steps:</p> <ul> <li>Step 1: creating PNG bitmaps from 2D array activation maps</li> <li>Step 2: uploading PNG bitmaps to cloud storage</li> <li>Step 3: updating inferences and running tests</li> </ul> <p>Let's take a look at each step with example code snippets.</p>"},{"location":"advanced-usage/uploading-activation-maps/#step-1-creating-png-bitmaps","title":"Step 1: Creating PNG Bitmaps","text":"<p>The activation map is a 2D data array ranging from 0 to 1 with <code>(h, w)</code> shape. This array is converted to a PNG bitmap using the following two utility methods:</p> <ul> <li><code>colorize_activation_map</code>: applies color and opacity to the input activation map</li> <li><code>encode_png</code>: encodes the colorized map into an in-memory PNG image represented as binary data</li> </ul> <pre><code>import io\nimport numpy as np\nfrom kolena.workflow.visualization import colorize_activation_map\nfrom kolena.workflow.visualization import encode_png\ndef create_bitmap(activation_map: np.ndarray) -&gt; io.BytesIO:\nbitmap = colorize_activation_map(activation_map)\nimage_buffer = encode_png(bitmap, mode=\"RGBA\")\nreturn image_buffer\n</code></pre> <p>Activation Map Scaling</p> <p>The activation map often has the equal dimensions (i.e., width and height) as the input image or sometimes has the scaled-down dimensions with the fixed ratio. Kolena automatically scales the overlay annotations to the images so there is no need to up-scale the map to match the image dimensions.</p>"},{"location":"advanced-usage/uploading-activation-maps/#step-2-uploading-png-bitmaps-to-cloud-storage","title":"Step 2: Uploading PNG Bitmaps to Cloud Storage","text":"<p>In order to visualize the bitmaps on Kolena, these bitmaps must be uploaded to a cloud storage first, and their locators are used to create <code>BitmapMask</code> annotations. In this section, we will learn how to upload the in-memory bitmaps to an S3 bucket. For other cloud storage services, please refer to your cloud storage's API docs.</p> <pre><code>import io\nimport boto3\nfrom urllib.parse import urlparse\nBUCKET = \"&lt;YOUR_S3_BUCKET&gt;\"\ns3 = boto3.client(\"s3\")\ndef bitmap_locator(filename: str) -&gt; str:\nreturn f\"{BUCKET}/tutorial/activation_maps/{filename}.png\"\ndef upload_bitmap(image_buffer: io.BytesIO, filename: str) -&gt; str:\nlocator = bitmap_locator(filename)\nparsed_url = urlparse(locator)\ns3_bucket = parsed_url.netloc\ns3_key = parsed_url.path.lstrip(\"/\")\ns3.upload_fileobj(image_buffer, s3_bucket, s3_key)\nreturn locator\n</code></pre> <p>With all the building blocks we learned from Step 1 and Step 2, we can now create a <code>BitmapMask</code> with a given activation map.</p> <pre><code>from kolena.workflow.annotation import BitmapMask\ndef create_and_upload_bitmap(\nfilename: str,\nactivation_map: np.ndarray,\n) -&gt; BitmapMask:\nimage_buffer = create_bitmap(activation_map)\nlocator = upload_bitmap(image_buffer, filename)\nreturn BitmapMask(locator)\n</code></pre>"},{"location":"advanced-usage/uploading-activation-maps/#step-3-updating-inference-and-running-tests","title":"Step 3: Updating <code>Inference</code> and Running Tests","text":"<p>Info</p> <p>If you are not familiar with the workflow concept, please read the   Building a Workflow guide.</p> <p>For the purposes of this tutorial, let's assume we already have a workflow built, and we are going to upload the activation maps as one of the fields in <code>Inference</code>. All we need to do is to update the <code>Inference</code> definition to include a new field for the activation map:</p> <pre><code>from kolena.workflow import Inference as Inf\nfrom kolena.workflow.annotation import BitmapMask\n@dataclass(frozen=True)\nclass Inference(Inf):\n...\nactivation_map: BitmapMask\n</code></pre> <p>Info</p> <p>If you are not familiar with how to run tests, please read the Step 4: Running Tests from   Building a Workflow guide.</p> <p>Before you run tests, make sure to update your <code>infer</code> function to return an <code>Inference</code> with the corresponding <code>BitmapMask</code> as its <code>activation_map</code> field. You are now ready to run tests! Once the tests complete, we can now visit   Studio to visualize activation maps overlaid on your <code>Image</code> data.</p>"},{"location":"advanced-usage/uploading-activation-maps/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we learned how to upload activation maps to Kolena in order to visualize activation maps overlaid on your <code>Image</code> data along with your ground truths and inferences.</p>"},{"location":"advanced-usage/connecting-cloud-storage/","title":"Connecting Cloud Storage","text":"<p>Data from cloud storage providers can be loaded to the Kolena platform by creating integrations. Integrations can be managed by users with admin roles by navigating to your Organization Settings's Integrations Tab</p> <ul> <li> <p>S3-Compatible APIs</p> <p>Establish an integration with a third-party system implementing an S3-Compatible API (MinIO, Oracle, Hitachi, etc.).</p> </li> </ul> <ul> <li> <p>Google Cloud Storage</p> <p>Establish an integration with Google Cloud Storage.</p> </li> </ul> <ul> <li> <p>HTTP Basic</p> <p>Establish an integration with an internal system using HTTP basic authentication.</p> </li> </ul>"},{"location":"advanced-usage/connecting-cloud-storage/google-cloud-storage/","title":"Google Cloud Storage","text":"<p>Integrations can be established to Google Cloud Storage.</p> <p>To get started, ensure you have admin access within Kolena. Navigate to your Organization Settings's Integrations Tab and click \"Add Integration\", then \"Google Cloud Storage\".</p>"},{"location":"advanced-usage/connecting-cloud-storage/google-cloud-storage/#1-save-integration-to-create-a-service-account","title":"1. Save Integration to Create a Service Account","text":"<p>From the Integrations Tab, saving the Google Cloud Storage integration will create a service account. Upon creation, the integration's <code>client_email</code> will be used to provide Kolena permission to load data from your Google Cloud Storage buckets.</p>"},{"location":"advanced-usage/connecting-cloud-storage/google-cloud-storage/#2-grant-service-account-read-access","title":"2. Grant Service Account Read Access","text":"<p>Within your Google Cloud Platform console, navigate to the bucket that contains your images. Click on the permissions tab. Click the \"Grant Access\" button and grant the service account created in step 1 the <code>Storage Object Viewer</code> role. The <code>Storage Object Viewer</code> role offers the following permissions:</p> <ul> <li>Grants access to view objects and their metadata, excluding ACLs.</li> <li>Grants access to list the objects in a bucket.</li> </ul> <p>\u200b \u200b</p>"},{"location":"advanced-usage/connecting-cloud-storage/google-cloud-storage/#3-provide-cors-access","title":"3. Provide CORS Access","text":"<p>Create a json file <code>cors.json</code> with the following content:</p> <pre><code>[\n{\n\"origin\": [\"https://app.kolena.io\"],\n\"method\": [\"GET\"],\n\"responseHeader\": [\"Content-Type\"],\n\"maxAgeSeconds\": 3600\n}\n]\n</code></pre> <p>Ensure you have <code>gsutil</code> installed. Then provide CORS access to Kolena for your bucket by running the following command:</p> <p><code>gsutil cors set example_cors_file.json gs://&lt;my-bucket&gt;</code></p>"},{"location":"advanced-usage/connecting-cloud-storage/http-basic/","title":"HTTP Basic","text":"<p>Integrations can be established using HTTP Basic Auth.</p> <p>To get started, ensure you have admin access within Kolena. Navigate to your Organization Settings's Integrations Tab and click \"Add Integration\", then \"HTTP Basic\".</p>"},{"location":"advanced-usage/connecting-cloud-storage/http-basic/#1-save-integration-on-kolena","title":"1. Save Integration on Kolena","text":"<p>On the Integrations Tab, fill in the fields for the integration and then click \"Save\".</p> Field Description URL Origin The origin of the domain you wish to load data from. Ensure you omit the protocol (e.g. <code>https://</code>) Username The username for your http basic auth system Password The password (optional) for your http basic auth system <p>Any locators beginning with <code>https://&lt;URL Origin&gt;</code> will be loaded using this integration.</p>"},{"location":"advanced-usage/connecting-cloud-storage/s3-compatible/","title":"S3-Compatible APIs","text":"<p>Integrations can be established to S3-compatible systems. Supported systems include:</p> <ul> <li> MinIO</li> <li> Oracle Object Storage</li> <li> Hitachi Content Platform (HCP) for cloud scale</li> </ul> <p>To get started, ensure you have admin access within Kolena. Navigate to your Organization Settings's Integrations Tab and click \"Add Integration\", then \"MinIO\".</p> <p>Steps performed outside of Kolena are shown for a subset of possible S3-compatible systems. You may need to consult documentation for your provider to perform equivalent steps.</p>"},{"location":"advanced-usage/connecting-cloud-storage/s3-compatible/#1-create-a-service-user-for-kolena","title":"1. Create a Service User for Kolena","text":"<code>MinIO</code> <pre><code>mc admin user add &lt;deployment_alias&gt; &lt;kolena_user&gt; &lt;secret_access_key&gt;\n</code></pre>"},{"location":"advanced-usage/connecting-cloud-storage/s3-compatible/#2-create-an-access-policy","title":"2. Create an Access Policy","text":"<p>Create a policy to allow read access for a bucket or set of buckets.</p> <p>Save the following JSON policy to a file called <code>/tmp/kolena-policy.json</code>, replacing <code>s3://share-with-kolena</code> with the appropriate bucket(s):</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [{\n\"Sid\": \"S3ListBucket\",\n\"Effect\": \"Allow\",\n\"Action\": [\n\"s3:GetObject\",\n\"s3:ListBucket\"\n],\n\"Resource\": [\n\"arn:aws:s3:::share-with-kolena\",\n\"arn:aws:s3:::share-with-kolena/*\"\n]\n}]\n}\n</code></pre> <p>Note: bucket names</p> <p>Please note that bucket names must follow S3 naming rules</p> <p>Next, create the policy and attach the policy to the service user created in step 1:</p> <code>MinIO</code> <pre><code>mc admin policy create &lt;deployment_alias&gt; kolenaread /tmp/kolena-policy.json\nmc admin policy attach &lt;deployment_alias&gt; kolenaread --user &lt;kolena_user&gt;\n</code></pre>"},{"location":"advanced-usage/connecting-cloud-storage/s3-compatible/#3-save-integration-on-kolena","title":"3. Save Integration on Kolena","text":"<p>Return to the Kolena platform Integrations Tab</p> <p>By default, any locators beginning with <code>s3://</code> will be loaded using this integration.</p> <p>Note: scoping integrations</p> <p>Optionally, each integration can be scoped to a specific bucket such that only locators of the pattern <code>s3://&lt;specific-bucket&gt;/*</code> will be loaded using the integration. This can be necessary if multiple integrations are required. Unchecking \"Apply to all buckets by default?\" and specifying a bucket will enable this behavior.</p> <p>Fill in the fields for the integration and then click \"Save\".</p> Field Description Access Key Id The username (<code>&lt;kolena_user&gt;</code>) of the user created in step 1 Secret Access Key The secret key (<code>&lt;secret_access_key&gt;</code>) of the user created in step 1 Endpoint The hostname or IP address of your S3-compatabile service Port The optional port to access your S3-compatabile service Region The region your buckets will be accessed from"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>In this section, we'll get acquainted with the core concepts on Kolena, and learn in-depth about the various features offered. For a brief introduction, see the Quickstart Guide or the Building a Workflow tutorial. For code-level API documentation, see the API Reference Documentation for the <code>kolena</code> Python client.</p> <ul> <li> <p>  Workflow</p> <p>Testing in Kolena is broken down by the type of ML problem you're solving, called a workflow. Any ML problem that can be tested can be modeled as a workflow in Kolena.</p> </li> </ul> <ul> <li> <p>  Test Cases &amp; Test Suites</p> <p>Test cases and test suites are used to organize test data in Kolena.</p> </li> </ul> <ul> <li> <p>  Models</p> <p>In Kolena, a model is a deterministic transformation from test samples to inferences.</p> </li> </ul>"},{"location":"core-concepts/model/","title":"Model","text":"<p>In Kolena, a model is a deterministic transformation from test samples to inferences.</p> <p>Kolena only stores metadata associated with your model in its   Models registry. Models themselves \u2014 their code or their weights \u2014 are never uploaded to Kolena, only the inferences from models.</p> <p>Models are considered black boxes, which makes Kolena agnostic to the underlying framework and architecture. It's possible to test any sort of model, from deep learning to rules-based, on Kolena.</p>","boost":2},{"location":"core-concepts/model/#creating-models","title":"Creating Models","text":"<p>The <code>Model</code> class is used to programmatically create models for testing. Rather than importing the class from <code>kolena.workflow</code> directly, use the <code>Model</code> definition returned from <code>define_workflow</code> bound to the test sample and inference types for your workflow:</p> <pre><code>from kolena.workflow import define_workflow\nfrom my_workflow import MyTestSample, MyGroundTruth, MyInference\n*_, Model = define_workflow(\"My Workflow\", MyTestSample, MyGroundTruth, MyInference)\n</code></pre> <p>With this class, models can be created, loaded, and updated:</p> <pre><code>my_model = Model(\"example-model\")\n</code></pre>","boost":2},{"location":"core-concepts/model/#implementing-infer","title":"Implementing <code>infer</code>","text":"<p>To test a model using the <code>test</code> method, a <code>Model.infer</code> implementation must be provided. <code>infer</code> is where the model itself \u2014 the deterministic transformation from test sample to inference \u2014 lives.</p> <pre><code># in practice, use TestSample and Inference types from your workflow\nfrom kolena.workflow import TestSample, Inference\ndef infer(test_sample: TestSample) -&gt; Inference:\n...\n</code></pre> <p>When running a model live, this function usually involves loading the image/document/etc. from the <code>TestSample</code>, passing it to your model, and constructing an <code>Inference</code> object from the model outputs. When loading results from e.g. a CSV, this function is often just a lookup.</p> Example: Loading inferences from CSV <p>This example considers a classification workflow using the <code>Image</code> test sample type and the following inference type:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Optional\nfrom kolena.workflow import Inference\nfrom kolena.workflow.annotation import ScoredClassificationLabel\n@dataclass(frozen=True)\nclass MyInference(Inference):\n# use Optional to accommodate missing inferences\nprediction: Optional[ScoredClassificationLabel] = None\n</code></pre> <p>With inferences stored in an <code>inferences.csv</code> with the <code>locator</code>, <code>label</code> and <code>score</code> columns, implementing <code>infer</code> as a lookup is straightforward:</p> <pre><code>import pandas as pd\nfrom kolena.workflow import Image\nfrom my_workflow import MyInference\ninference_by_locator = {\nrecord.locator: MyInference(prediction=ScoredClassificationLabel(\nlabel=record.label,\nscore=record.score,\n)) for record in pd.read_csv(\"inferences.csv\").itertuples()\n}\ndef infer(test_sample: Image) -&gt; MyInference:\nreturn inference_by_locator.get(test_sample.locator, MyInference())\n</code></pre> <p>Note: Ensure that models are deterministic</p> <p>To preserve reproducibility, ensure that models tested in Kolena are deterministic.</p> <p>This is particularly important for generative models. If your model has a random seed parameter, consider including the random seed value used for testing as a piece of metadata attached to the model.</p>","boost":2},{"location":"core-concepts/model/#metadata","title":"Metadata","text":"<p>When creating a model, you have the option to specify free-form <code>metadata</code> to associate with the model. This metadata can be useful to track relevant information about the model, such as:</p> <ul> <li>Framework (e.g. PyTorch, TensorFlow, custom, etc.) and version used</li> <li>Person who trained the model, e.g. <code>name@company.ai</code></li> <li>GitHub branch, file, or commit hash used to run the model</li> <li>Links to your experimentation tracking system</li> <li>Free-form notes about methodology or observations</li> <li>Location in e.g. S3 where the model's weights are stored</li> <li>Training dataset specifier or URL</li> <li>Hyperparameters applied during training</li> </ul> <p>Metadata can be specified on the command line or edited on the web on the   Models page.</p>","boost":2},{"location":"core-concepts/model/#faq-best-practices","title":"FAQ &amp; Best Practices","text":"How should models be named? <p>Two factors influence model naming:</p> <ol> <li>A model's name is unique, and</li> <li>A model is deterministic.</li> </ol> <p>This means that anything that may change your model's outputs, such as environment or packaging, should be tracked as a new model! We recommend storing a variety of information in the model name, for example:</p> <ul> <li>Model architecture, e.g. <code>YOLOR-D6</code></li> <li>Input size, e.g. <code>1280x1280</code></li> <li>Framework, e.g. <code>pytorch-1.7</code></li> <li>Additional tracking information, such as its name in Weights &amp; Biases, e.g. <code>helpful-meadow-5</code></li> </ul> <p>An example model name may therefore be:</p> <pre><code>helpful-meadow-5 (YOLOR-D6, 1280x1280, pytorch-1.7)\n</code></pre> <p>Model names can be edited on the web on the   Models page.</p>","boost":2},{"location":"core-concepts/test-suite/","title":"Test Case &amp; Test Suite","text":"<p>Test cases and test suites are used to organize test data in Kolena.</p> <p>A test case is a collection of test samples and their associated ground truths. Test cases can be thought of as benchmark datasets, or slices of a benchmark dataset.</p> <p>A test suite is a collection of test cases. Models are tested on test suites.</p> <p>Test cases and test suites are found on the   Test Suites page on Kolena.</p>","boost":2},{"location":"core-concepts/test-suite/#managing-test-cases-test-suites","title":"Managing Test Cases &amp; Test Suites","text":"<p>The <code>TestCase</code> and <code>TestSuite</code> classes are used to programmatically create test cases and test suites. Rather than importing these classes from <code>kolena.workflow</code> directly, Use the definitions returned from <code>define_workflow</code> bound to the test sample and ground truth types for your workflow:</p> <pre><code>from kolena.workflow import define_workflow\nfrom my_workflow import MyTestSample, MyGroundTruth, MyInference\n_, TestCase, TestSuite, _ = define_workflow(\n\"My Workflow\",\nMyTestSample,\nMyGroundTruth,\nMyInference,\n)\n</code></pre> <p>These classes can then be used to create, load, and edit test cases and test suites:</p> Test CaseTest Suite <p>Create using <code>TestCase.create</code>:</p> <pre><code># throws if a test case with name 'example-test-case' already exists\ntest_case = TestCase.create(\n\"example-test-case\",\n# optionally include list of test samples and ground truths to populate the new test case\n# test_samples=[(ts0, gt0), (ts1, gt1), (ts2, gt2)],\n)\n</code></pre> <p>Load using <code>TestCase.load</code>:</p> <pre><code># throws if a test case with name 'example-test-case' does not exist\ntest_case = TestCase.load(\"example-test-case\")\n</code></pre> <p>Use the <code>TestCase</code> constructor for idempotent create/load behavior:</p> <pre><code># loads 'example-test-case' or creates it if it does not already exist\ntest_case = TestCase(\"example-test-case\")\n</code></pre> <p>Use <code>TestCase.init_many</code> to initialize multiple test cases at once:</p> <pre><code># loads test cases or creates them if they do not already exist\ntest_cases = TestCase.init_many([\n(\"test case 1\", [(test_sample_0, ground_truth_0), (test_sample_1, ground_truth_1)]),\n(\"test case 2\", [(test_sample_2, ground_truth_2), (test_sample_3, ground_truth_3)])\n])\n# With 'reset=True', test cases that already exist would be updated with the new test_samples and ground_truths\ntest_cases = TestCase.init_many([\n(\"test case 1\", [(test_sample_0, ground_truth_0), (test_sample_1, ground_truth_1)]),\n(\"test case 2\", [(test_sample_2, ground_truth_2), (test_sample_3, ground_truth_3)])\n], reset=True)\n</code></pre> <p>Test cases can be edited using the context-managed <code>Editor</code> interface:</p> <pre><code>with TestCase(\"example-test-case\").edit(reset=True) as editor:\n# perform desired editing actions within context\neditor.add(ts0, gt0)\n</code></pre> <p>Create using <code>TestSuite.create</code>:</p> <pre><code># throws if a test suite with name 'example-test-suite' already exists\ntest_suite = TestSuite.create(\n\"example-test-suite\",\n# optionally include list of test cases to populate the new test suite\n# test_cases=[test_case0, test_case1, test_case2],\n)\n</code></pre> <p>Load using <code>TestSuite.load</code>:</p> <pre><code># throws if a test suite with name 'example-test-suite' does not exist\ntest_suite = TestSuite.load(\"example-test-suite\")\n</code></pre> <p>Use the <code>TestSuite</code> constructor for idempotent create/load behavior:</p> <pre><code># loads 'example-test-suite' or creates it if it does not already exist\ntest_suite = TestSuite(\"example-test-suite\")\n</code></pre> <p>Test suites be edited using the context-managed <code>Editor</code> interface:</p> <pre><code>with TestSuite(\"example-test-suite\").edit() as editor:\neditor.add(test_case_a)\neditor.remove(test_case_b)\n# perform desired editing actions within context\n</code></pre>","boost":2},{"location":"core-concepts/test-suite/#versioning","title":"Versioning","text":"<p>All test data on Kolena is versioned and immutable<sup>1</sup>. Previous versions of test cases and test suites are always available and can be visualized on the web and loaded programmatically by specifying a version.</p> <pre><code># load a specific version of a test suite\ntest_suite_v2 = TestSuite.load(\"example-name\", version=2)\n</code></pre>","boost":2},{"location":"core-concepts/test-suite/#faq-best-practices","title":"FAQ &amp; Best Practices","text":"How should I map my existing benchmark into test cases and test suites? <p>To start, create a test suite containing a single test case for the complete benchmark. This single-test-case test suite represents standard, aggregate evaluation on a benchmark dataset.</p> <p>Once this test suite has been created, you can start creating test cases! Use the Studio, the Stratifier, or the Python client to create test cases slicing through (stratifying) this benchmark.</p> How many test cases should a test suite include? <p>While test suites can hold anywhere from one to thousands of test cases, the sweet spot for the signal-to-noise ratio is in the dozens or low hundreds of test cases per test suite.</p> <p>Note that the relationship between benchmark dataset and test suite doesn't need to be 1:1. Often it can be useful to create different test suites for different stratification strategies applied to the same benchmark.</p> How many samples should be included in a test case? <p>While there's no one-size-fits-all answer, we usually recommend including at least 100 samples in each test case. Smaller test cases can be used to provide a very rough signal about the presence or absence of a model beahvior, but shouldn't be relied upon for much more than a directional indication of performance.</p> <p>The multi-model Results comparison view in Kolena takes the number of test samples within a test case into account when highlighting improvements and regressions. The larger the test case, the smaller the \u2206 required to consider a change from one model to another as \"significant.\"</p> How many negative samples should a test case include? <p>Many workflows, such as object detection or binary classification, have a concept of \"negative\" samples. In object detection, a \"negative sample\" is a sample (i.e. image) that does not include any objects to be detected.</p> <p>Negative samples can have a large impact on certain metrics. To continue with the object detection example, the precision metric depends on the number of false positive detections:</p> \\[ \\text{Precision} := \\dfrac{\\text{# True Positives}}{\\text{# True Positives} + \\text{# False Positives}} \\] <p>Therefore, since each negative sample has some likelihood of yielding false positive detections but no likelihood of yielding true positive detections, adding negative samples to a test case may decrease aggregate precision values computed across the test case.</p> <p>As a general rule of thumb, we recommend including an even balance of positive and negative samples in each test case. This composition minimizes the likelihood of different metrics being heavily skewed in one direction or another.</p> <ol> <li> <p>Immutability caveat: test suites, along with any test cases and test samples they hold, can be deleted on the   Test Suites page.\u00a0\u21a9</p> </li> </ol>","boost":2},{"location":"core-concepts/workflow/","title":"Workflow","text":"<p>Testing in Kolena is broken down by the type of ML problem you're solving, called a workflow. Any ML problem that can be tested can be modeled as a workflow in Kolena.</p> <p>Examples of workflows include:</p> <ul> <li>  Keypoint Detection using images</li> <li> array-string  Text Summarization using articles/documents</li> <li>  Age Estimation (regression) using images</li> <li>  Video Retrieval using text queries on a corpus of videos</li> </ul> <p>With the <code>kolena.workflow</code> client module, any arbitrary ML problem can be defined as a workflow and tested on Kolena.</p> <p>There are three main components of a workflow:</p> <p>Info</p> <p>These three types can be thought of as the data model, or the schema, of a workflow.</p> <ol> <li>Test Sample: the inputs to a model, e.g. image, video, document</li> <li>Ground Truth: the expected model outputs</li> <li>Inference: the actual model outputs</li> </ol>","boost":2},{"location":"core-concepts/workflow/#test-sample","title":"Test Sample","text":"<p>In Kolena, \"test sample\" is the general term for the input to a model.</p> <p>For standard computer vision (CV) models, the test sample is often a single image. Video-based computer vision models would have a video test sample type, and stereo vision models would use image pairs. For natural language processing models, the test sample may be a document or text snippet.</p> <p>When building a workflow, you can extend and compose these base test sample types as necessary, or use the base types directly if no customization is required.</p>","boost":2},{"location":"core-concepts/workflow/#metadata","title":"Metadata","text":"<p>Any additional information associated with a test sample, e.g. details about how it was collected, can be included as metadata. We recommend uploading any and all metadata that you have available, as metadata can be useful for searching through data in the Studio, interpreting model results, and creating new test cases.</p> <pre><code>from dataclasses import dataclass, field\nfrom kolena.workflow import Document, Metadata\n@dataclass(frozen=True)\nclass MyDocument(Document):\n# locator: str  # inherited from parent Document\ndoc_id: int  # example of a field that is explicitly required\nmetadata: Metadata = field(default_factory=dict)  # free-form, optional metadata\n</code></pre> <p>Use <code>pydantic</code> dataclasses</p> <p>When building a workflow, object definitions can us standard library <code>dataclasses</code> or Pydantic <code>dataclasses</code>. Pydantic brings helpful runtime type validation and coercion and can be used as a drop-in replacement for standard library <code>dataclasses</code>.</p>","boost":2},{"location":"core-concepts/workflow/#composite-test-samples","title":"Composite Test Samples","text":"<p>Kolena is not prescriptive about the shape of your ML problem. Test samples can be composed, using the <code>Composite</code> test sample type, to mirror the shape of your problem directly.</p> <p>Consider the example of an autonomous vehicle application that uses four cameras, one for each of the <code>front</code>, <code>right</code>, <code>rear</code>, and <code>left</code> views:</p> <pre><code>from dataclasses import dataclass\nfrom kolena.workflow import Composite, Image\n@dataclass(frozen=True)\nclass QuadImage(Composite):\nfront: Image\nright: Image\nrear: Image\nleft: Image\n</code></pre> How can I specify annotations on <code>Composite</code> test samples? <p>Image-level (or video-level, document-level, etc.) annotations can be specified when using composite test samples. To specify image-level objets in each of the four images, ground truth or inference definitions may look like this:</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\nfrom kolena.workflow import DataObject, GroundTruth\nfrom kolena.workflow.annotation import BoundingBox\n@dataclass(frozen=True)\nclass SingleImageGroundTruth(DataObject):\nobjects: List[BoundingBox]\n@dataclass(frozen=True)\nclass QuadImageGroundTruth(GroundTruth):\n# attribute names matches attribute names in test sample\nfront: SingleImageGroundTruth\nright: SingleImageGroundTruth\nrear: SingleImageGroundTruth\nleft: SingleImageGroundTruth\n</code></pre>","boost":2},{"location":"core-concepts/workflow/#ground-truth","title":"Ground Truth","text":"<p>The ground truth represents the expected output from a model when provided with a test sample. Ground truths are often manually annotated and are used to determine the correctness of model predictions.</p> <p>In the   Studio, ground truths are always displayed alongside their paired test samples. Any annotations, such as bounding boxes or polygons, are visualized on top of the test sample.</p> <p>The contents of a ground truth are driven by the requirements of the workflow. Take this example for a multiclass object detection workflow:</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\nfrom kolena.workflow import GroundTruth\nfrom kolena.workflow.annotation import LabeledBoundingBox\n@dataclass(frozen=True)\nclass MyGroundTruth(GroundTruth):\nobjects: List[LabeledBoundingBox]\n</code></pre> Where should additional information that isn't used for model evaluation live? <p>We recommend scoping the ground truth to only the data required for model evaluation. Any additional metadata, annotations, or assets associated with a test sample can be included as a part of the test sample itself or in its free-form metadata.</p> <p>However, it isn't a strict requirement that ground truths only contain information used for model evaluation. Sometimes it makes sense to include additional information as optional fields inside a ground truth definition.</p>","boost":2},{"location":"core-concepts/workflow/#inference","title":"Inference","text":"<p>A workflow's inference type contains the actual output produced by a model when given a test sample. Inferences are also referred to as \"raw inferences,\" as they represent the raw output from a model.</p> <p>The inference type and ground truth type for a workflow will often look very similar to one another.</p>","boost":2},{"location":"core-concepts/workflow/#extending-annotation-types","title":"Extending Annotation Types","text":"<p>Annotation types can be extended to include additional fields, when necessary.</p> <p>Consider the example of a <code>Keypoints</code> detection model that detects anywhere from 0 to N keypoints arrays when provided an image. Each keypoints array has an associated class label and confidence value. This model's inference type could be defined as follows:</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\nfrom kolena.workflow import Inference\nfrom kolena.workflow.annotation import Keypoints\n@dataclass(frozen=True)\nclass ScoredLabeledKeypoints(Keypoints):\n# points: List[Tuple[float, float]]  # inherited from Keypoints\nscore: float  # confidence score, between 0 and 1\nlabel: str  # predicted class\n@dataclass(frozen=True)\nclass MyInference(Inference):\npredictions: List[ScoredLabeledKeypoints]\n</code></pre>","boost":2},{"location":"core-concepts/workflow/#deduplication","title":"Deduplication","text":"<p>Models are considered deterministic inputs from test samples to inferences. This means that, when testing in Kolena, a given model only needs to process a given test sample once. Kolena uses this to speed up the process of running tests, ensuring that compute cycles are not wasted processing a given test sample multiple times when test samples exist in multiple test cases.</p> <p>When calling <code>test</code>, only samples that do not already have inferences uploaded from the given model will be processed. To change this behavior and re-process all test samples, regardless of any uploaded inferences, use the <code>reset</code> flag:</p> <pre><code># all test samples are processed and inferences [re]uploaded when reset=True\ntest(model, test_suite, evaluator, reset=True)\n</code></pre>","boost":2},{"location":"core-concepts/workflow/#defining-a-workflow","title":"Defining a Workflow","text":"<p>With test sample, ground truth, and inference types declared, defining a workflow provides the <code>TestCase</code>, <code>TestSuite</code>, and <code>Model</code> definitions to use when creating tests and testing models with this workflow:</p> <pre><code>from kolena.workflow import define_workflow\nfrom my_workflow import MyTestSample, MyGroundTruth, MyInference\n_, TestCase, TestSuite, Model = define_workflow(\n\"My Example Workflow\",\nMyTestSample,\nMyGroundTruth,\nMyInference,\n)\n</code></pre>","boost":2},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>This page answers common questions about Kolena and how to use it to test ML models.</p> <p>If you don't see your question here, please reach out to us on Slack or at contact@kolena.io!</p>"},{"location":"faq/#about-kolena","title":"About Kolena","text":"What data types does Kolena support? <p>Testing in Kolena is fully customizable and supports computer vision, natural language processing, and structured data (tabular, time series) machine learning models. This includes images, documents, videos, 3D models and point clouds, and more.</p> <p>See the available data types in <code>kolena.workflow.TestSample</code>, and the available annotation types in <code>kolena.workflow.annotation</code>.</p> <p>We're constantly adding new data types and annotation types \u2014 if you don't see what you're looking for, reach out to us and we'll happily extend our system to support your use case.</p> Do I have to upload my datasets to Kolena? <p>No. Kolena doesn't store your data (images, videos, documents, 3D assetes, etc.) directly, only URLs pointing to the right location in a cloud bucket or internal infrastructure that you own.</p> <p>While onboarding your team, we'll discuss what access restrictions are necessary for your data and select the right integration solution. As one example, as a part of the integration we might restrict access to files registered with Kolena to only users on your corporate VPN.</p> <p>We support a variety of integration patterns depending on your organization's requirements and security stance. Get in touch with us to discuss details!</p> Do I have to upload my models to Kolena? <p>No. Tests are always run in your environment using the <code>kolena</code> Python client, and you never have to package or upload models to Kolena.</p> Where does Kolena fit into the MLOps development life cycle? <p>Kolena is primarily a testing (or \"offline evaluation\") platform, coming after training and before deployment. We believe that increased emphasis on this offline evaluation segment of the model development life cycle can save effort upstream in the data collection and training process as well as prevent headaches downstream in deployment.</p>"},{"location":"faq/#using-kolena","title":"Using Kolena","text":"How do I generate an API token? <p>Generate an API token by visiting the   Developer page, located at the bottom of the lefthand sidebar, then copy/paste the shell snippet to set this token as <code>KOLENA_TOKEN</code> in your environment.</p> How many API tokens can I generate? <p>API tokens are scoped to your username. Each user is limited to one valid token at a time \u2014 generating a new token on the   Developer page invalidates any previous token generated for your user.</p> <p>To retrieve a service user API token that is not scoped to a specific username, please reach out to us on Slack or at contact@kolena.io.</p> How can I add new users to my organization? <p>Certain members of each organization have administrator privileges. These administrators can add new users, and grant users administrator privileges, by visiting the   Organization Settings page and adding entries to the Authorized Users table.</p> <p>Note that this page is only visible for organization administrators.</p> I'm new to Kolena \u2014 how can I learn more about the platform and how to use it? <p>On each page, there is a button with the   icon next to the page title. Click on this button to bring up a detailed tutorial explaining the contents of the current page and how it's used.</p> How can I report a bug? <p>If you encounter a bug when using the <code>kolena</code> Python client or when using app.kolena.io, message us on Slack, email your support representative or contact@kolena.io, or open an issue on the <code>kolena</code> repository for Python-client-related issues.</p> <p>Please include any relevant stacktrace or platform URL when reporting an issue.</p>"},{"location":"faq/#troubleshooting-errors","title":"Troubleshooting Errors","text":"I'm seeing the error: AttributeError: type object XXX has no attribute '__pydantic_model__' <p>If you see error message like this, please check that <code>pydantic &lt; 2.0</code> is installed. Kolena is not currently compatible with <code>pydantic</code> V2. For more information, refer to <code>kolena</code> Installation.</p>"},{"location":"metrics/","title":"Metrics Glossary","text":"<p>This section contains guides for different metrics used to measure model performance.</p> <p>Each ML use case requires different metrics. Using the right metrics is critical for understanding and meaningfully comparing model performance. In each metrics guide, you can learn about the metric with examples, its limitations and biases, and its intended uses.</p> <ul> <li> <p>Accuracy</p> <p>Accuracy measures how well a model predicts correctly. It's a good metric for assessing model performance in simple cases with balanced data.</p> </li> <li> <p>Averaging Methods: Macro, Micro, Weighted</p> <p>Different averaging methods for aggregating metrics for multiclass workflows, such as classification and object detection.</p> </li> <li> <p>F<sub>1</sub>-score</p> <p>F<sub>1</sub>-score is a metric that combines two competing metrics, precision and recall with an equal weight. It symmetrically represents both precision and recall as one metric.</p> </li> <li> <p>Geometry Matching</p> <p>Geometry matching is the process of matching inferences to ground truths for computer vision workflows with a localization component. It is a core building block for metrics such as TP, FP, and FN, and any metrics built on top of these, like precision, recall, and F<sub>1</sub>-score.</p> </li> <li> <p>Intersection over Union (IoU)</p> <p>IoU measures overlap between two geometries, segmentation masks, sets of labels, or time-series snippets. Also known as Jaccard index in classification workflow.</p> </li> <li> <p>Precision</p> <p>Precision measures the proportion of positive inferences from a model that are correct. It is useful when the objective is to measure and reduce false positive inferences.</p> </li> <li> <p>Recall (TPR, Sensitivity)</p> <p>Recall, also known as true positive rate (TPR) and sensitivity, measures the proportion of all positive ground truths that a model correctly predicts. It is useful when the objective is to measure and reduce false negative ground truths, i.e. model misses.</p> </li> <li> <p>TP / FP / FN / TN</p> <p>The counts of TP, FP, FN and TN ground truths and inferences are essential for summarizing model performance. They are the building blocks of many other metrics, including accuracy, precision, and recall.</p> </li> </ul>"},{"location":"metrics/accuracy/","title":"Accuracy","text":"<p>Accuracy is one of the most well-known metrics in machine learning model evaluation because it is simple to understand and straightforward to calculate.</p> <p>Accuracy measures how often a model correctly predicts something (ranging from 0 to 1, with 1 being perfect inferences). It reports the ratio of the number of correct inferences to the total number of inferences, making it a good metric for assessing model performance in simple cases with balanced data. However, accuracy is much less meaningful with imbalanced datasets (e.g. far more negative ground truths than positive ground truths) and should be used with caution.</p> <ul> <li>  API Reference: <code>accuracy</code> \u2197</li> </ul>"},{"location":"metrics/accuracy/#implementation-details","title":"Implementation Details","text":"<p>Accuracy is generally used to evaluate classification models. Aside from classification, accuracy is also often used to evaluate semantic segmentation models by measuring the percent of correctly classified pixels in an image.</p> <p>In a classification workflow, accuracy is the ratio of the number of correct inferences to the total number of inferences.</p> <p>With TP / FP / FN / TN counts computed, accuracy is defined:</p> \\[ \\text{Accuracy} =  \\frac {\\text{TP} + \\text{TN}} {\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}} \\]"},{"location":"metrics/accuracy/#examples","title":"Examples","text":"<p>Perfect inferences:</p> Metric Value TP 10 FP 0 FN 0 TN 10 \\[ \\begin{align} \\text{Accuracy} &amp;= \\frac{10 + 10}{10 + 0 + 0 + 10} \\\\[1em] &amp;= 1.0 \\end{align} \\] <p>Partially correct inferences:</p> Metric Value TP 8 FP 4 FN 2 TN 6 \\[ \\begin{align} \\text{Accuracy} &amp;= \\frac{8 + 6}{8 + 4 + 2 + 6} \\\\[1em] &amp;= 0.7 \\end{align} \\] <p>Highly imbalanced data, with 990 negative ground truths and 10 positive ground truths, with no positive inferences:</p> Metric Value TP 0 FP 0 FN 10 TN 990 \\[ \\begin{align} \\text{Accuracy} &amp;= \\frac{0 + 990}{0 + 0 + 10 + 990} \\\\[1em] &amp;= 0.99 \\end{align} \\] <p>Be careful with imbalanced datasets!</p> <p>This example describes a trivial model that only ever returns negative inferences, yet it has the high accuracy score of 99%.</p>"},{"location":"metrics/accuracy/#limitations-and-biases","title":"Limitations and Biases","text":"<p>While accuracy generally describes a classifier\u2019s performance, it is important to note that the metric can be deceptive, especially when the data is imbalanced.</p> <p>For example, let\u2019s say there are a total of 500 ground truths, with 450 belonging to the positive class and 50 to the negative. If the model correctly predicts all the positive ground truths but misses all the negative ones, its accuracy is <code>450 / 500 = 0.9</code>. An accuracy score of 90% indicates a pretty good model \u2014 but is a model that fails 100% of the time on negative ground truths useful? Using the accuracy metric alone can hide a model\u2019s true performance, so we recommend other metrics that are better suited for imbalanced data, such as:</p> <ul> <li>Balanced accuracy</li> <li>Precision</li> <li>Recall</li> <li>F<sub>1</sub>-score</li> </ul>"},{"location":"metrics/averaging-methods/","title":"Averaging Methods","text":"<p>For multiclass workflows like classification or object detection, metrics such as precision, recall, and F<sub>1</sub>-score are computed per class. To compute a single value that represents model performance across all classes, these per-class scores need to be aggregated. There are a few different averaging methods for doing this, most notably:</p> <ul> <li>Macro: unweighted mean of all per-class scores</li> <li>Micro: global average of per-sample TP, FP, FN scores</li> <li>Weighted: mean of all per-class scores, weighted by sample sizes for each class</li> </ul>"},{"location":"metrics/averaging-methods/#example-multiclass-classification","title":"Example: Multiclass Classification","text":"<p>Let\u2019s consider the following multiclass classification metrics, computed across a total of 10 samples:</p> Class # Samples # True Positives # False Positives # False Negatives Precision Recall F1-score <code>Airplane</code> 3 2 1 1 0.67 0.67 0.67 <code>Boat</code> 1 1 3 0 0.25 1.0 0.4 <code>Car</code> 6 3 0 3 1.0 0.5 0.67 Total 10 6 4 4 - - -"},{"location":"metrics/averaging-methods/#macro-average","title":"Macro Average","text":"<p>Macro average is perhaps the most straightforward among the numerous options and is computed by taking an unweighted mean of all the per-class scores:</p> \\[ \\begin{align} \\text{F}_{1 \\, \\text{macro}} &amp;= \\frac{\\text{F}_{1 \\, \\texttt{Airplane}} + \\text{F}_{1 \\, \\texttt{Boat}} + \\text{F}_{1 \\, \\texttt{Car}}}{3} \\\\[1em] &amp;= \\frac{0.67 + 0.4 + 0.67}{3} \\\\[1em] &amp;= 0.58 \\end{align} \\]"},{"location":"metrics/averaging-methods/#micro-average","title":"Micro Average","text":"<p>In contrast to macro, micro average computes a global average by counting the sums of true positive (TP), false negative (FN) and false positive (FP).</p> <p>Micro precision and micro recall are computed with the standard precision and recall formulas, using the total TP/FP/FN counts across all classes:</p> \\[ \\begin{align} \\text{Precision}_\\text{micro} &amp;= \\frac{\\text{TP}_\\text{Total}}{\\text{TP}_\\text{Total} + \\text{FP}_\\text{Total}} \\\\[1em] &amp;= \\frac{6}{6 + 4} \\\\[1em] &amp;= 0.6 \\end{align} \\] \\[ \\begin{align} \\text{Recall}_\\text{micro} &amp;= \\frac{\\text{TP}_\\text{Total}}{\\text{TP}_\\text{Total} + \\text{FN}_\\text{Total}} \\\\[1em] &amp;= \\frac{6}{6 + 4} \\\\[1em] &amp;= 0.6 \\end{align} \\] <p>What about micro F<sub>1</sub>? Plug the micro-averaged values for precision and recall into the standard formula for F<sub>1</sub>-score:</p> \\[ \\begin{align} \\text{F}_{1 \\, \\text{micro}} &amp;= 2 \\times \\frac{\\text{Precision}_\\text{micro} \\times \\text{Recall}_\\text{micro}}{\\text{Precision}_\\text{micro} + \\text{Recall}_\\text{micro}} \\\\[1em] &amp;= 2 \\times \\frac{0.6 \\times 0.6}{0.6 + 0.6} \\\\[1em] &amp;= 0.6 \\end{align} \\] <p>Note that precision, recall, and F<sub>1</sub>-score all have the same value: \\(0.6\\). This is because micro-averaging essentially computes the proportion of correctly classified instances out of all instances, which is the definition of overall accuracy.</p> <p>In the multiclass classification cases where each sample has a single label, we get the following:</p> \\[ \\text{F}_{1 \\, \\text{micro}} = \\text{Precision}_\\text{micro} = \\text{Recall}_\\text{micro} = \\text{Accuracy} \\]"},{"location":"metrics/averaging-methods/#weighted-average","title":"Weighted Average","text":"<p>Weighted average computes the mean of all per-class scores while considering each class\u2019s support. In this case, support is the number of actual instances of the class in the dataset.</p> <p>For example, if there are 3 samples of class <code>Airplane</code>, then the support value of class <code>Airplane</code> is 3. In other words, support is the sum of true positive (TP) and false negative (FN) counts. The weight is the proportion of each class\u2019s support relative to the sum of all support values:</p> \\[ \\begin{align} \\text{F}_{1 \\, \\text{weighted}} &amp;= \\left( \\text{F}_{1 \\, \\texttt{Airplane}} \\times \\tfrac{\\text{#}\\ \\texttt{Airplane}}{\\text{# Total}} \\right) + \\left( \\text{F}_{1 \\, \\texttt{Boat}} \\times \\tfrac{\\text{#}\\ \\texttt{Boat}}{\\text{# Total}} \\right) + \\left( \\text{F}_{1 \\, \\texttt{Car}} \\times \\tfrac{\\text{#}\\ \\texttt{Car}}{\\text{# Total}} \\right) \\\\[1em] &amp;= \\left( 0.67 \\times \\tfrac{3}{10} \\right) + \\left( 0.4 \\times \\tfrac{1}{10} \\right) + \\left( 0.67 \\times \\tfrac{6}{10} \\right) \\\\[1em] &amp;= 0.64 \\end{align} \\]"},{"location":"metrics/averaging-methods/#which-method-should-i-use","title":"Which Method Should I Use?","text":"<p>You would generally use these three methods to aggregate the metrics computed per class. Averaging is most commonly used in multiclass/multi-label classification and object detection tasks.</p> <p>So which average should you use?</p> <p>If you\u2019re looking for an easily understandable metric for overall model performance regardless of class, micro average is probably best.</p> <p>If you want to treat all classes equally, then using macro average would be a good choice.</p> <p>If you have an imbalanced dataset but want to assign more weight to classes with more samples, consider using weighted average instead of macro average.</p>"},{"location":"metrics/f1-score/","title":"F<sub>1</sub>-score","text":"<p>The F<sub>1</sub>-score, also known as balanced F-score or F-measure, is a metric that combines two competing metrics, precision and recall, with an equal weight. F<sub>1</sub>-score is the harmonic mean between precision and recall, and symmetrically represents both in one metric.</p> <p>Guides: Precision and Recall</p> <p>Read the precision and the recall guides if you're not familiar with those metrics.</p> <p>Precision and recall offer a trade-off: increasing precision often reduces recall, and vice versa. This is called the precision/recall trade-off.</p> <p>Ideally, we want to maximize both precision and recall to obtain the perfect model. This is where the F<sub>1</sub>-score comes in play. Because the F<sub>1</sub>-score is the harmonic mean of precision and recall, maximizing the F<sub>1</sub>-score implies simultaneously maximizing both precision and recall. Thus, the F<sub>1</sub>-score has become a popular metric for the evaluation of many workflows, such as classification, object detection, semantic segmentation, and information retrieval.</p> <ul> <li>  API Reference: <code>f1_score</code> \u2197</li> </ul>"},{"location":"metrics/f1-score/#implementation-details","title":"Implementation Details","text":"<p>Using TP / FP / FN / TN, we can define precision and recall. The F<sub>1</sub>-score is computed by taking the harmonic mean of precision and recall.</p> <p>The F<sub>1</sub>-score is defined:</p> \\[ \\begin{align} \\text{F}_1 &amp;= \\frac {2} {\\frac {1} {\\text{Precision}} + \\frac {1} {\\text{Recall}}} \\\\[1em] &amp;= \\frac {2 \\times \\text{Precision} \\times \\text{Recall}} {\\text{Precision} + \\text{Recall}} \\end{align} \\] <p>It can also be calculated directly from true positive (TP) / false positive (FP) / false negative (FN) counts:</p> \\[ \\text{F}_1 = \\frac {\\text{TP}} {\\text{TP} + \\frac 1 2 \\left( \\text{FP} + \\text{FN} \\right)} \\]"},{"location":"metrics/f1-score/#examples","title":"Examples","text":"<p>Perfect inferences:</p> Metric Value TP 20 FP 0 FN 0 \\[ \\begin{align} \\text{Precision} = \\frac{20}{20 + 0} &amp;= 1.0 \\\\[1em] \\text{Recall} = \\frac{20}{20 + 0} &amp;= 1.0 \\\\[1em] \\text{F}_1 = \\frac{20}{20 + \\frac 1 2 \\left( 0 + 0 \\right)} &amp;= 1.0 \\end{align} \\] <p>Partially correct inferences, where every ground truth is recalled by an inference:</p> Metric Value TP 25 FP 75 FN 0 \\[ \\begin{align} \\text{Precision} = \\frac{25}{25 + 75} &amp;= 0.25 \\\\[1em] \\text{Recall} = \\frac{25}{25 + 0} &amp;= 1.0 \\\\[1em] \\text{F}_1 = \\frac{25}{25 + \\frac 1 2 \\left( 75 + 0 \\right)} &amp;= 0.4 \\end{align} \\] <p>Perfect inferences but some ground truths are missed:</p> Metric Value TP 25 FP 0 FN 75 \\[ \\begin{align} \\text{Precision} = \\frac{25}{25 + 0} &amp;= 1.0 \\\\[1em] \\text{Recall} = \\frac{25}{25 + 75} &amp;= 0.25 \\\\[1em] \\text{F}_1 = \\frac{25}{25 + \\frac 1 2 \\left( 0 + 75 \\right)} &amp;= 0.4 \\end{align} \\] <p>Zero correct inferences with non-zero false positive and false negative:</p> Metric Value TP 0 FP 15 FN 10 \\[ \\begin{align} \\text{Precision} = \\frac{0}{0 + 15} &amp;= 0.0 \\\\[1em] \\text{Recall} = \\frac{0}{0 + 10} &amp;= 0.0 \\\\[1em] \\text{F}_1 = \\frac{0}{0 + \\frac 1 2 \\left( 15 + 10\\right)} &amp;= 0.0 \\end{align} \\] <p>Zero correct inferences with zero false positive and false negative:</p> Metric Value TP 0 FP 0 FN 0 <p>Undefined F<sub>1</sub></p> <p>This example shows an edge case where both precision and recall are <code>undefined</code>. When either metric is <code>undefined</code>, F<sub>1</sub> is also <code>undefind</code>. In such cases, it's often interpreted as <code>0.0</code> instead.</p> \\[ \\begin{align} \\text{Precision} &amp;= \\frac{0}{0 + 0} \\\\[1em] &amp;= \\text{undefined} \\\\[1em] \\text{Recall} &amp;= \\frac{0}{0 + 0} \\\\[1em] &amp;= \\text{undefined} \\\\[1em] \\text{F}_1 &amp;= \\frac{0}{0 + \\frac 1 2 \\left( 0 + 0\\right)} \\\\[1em] &amp;= \\text{undefined} \\\\[1em] \\end{align} \\]"},{"location":"metrics/f1-score/#multiple-classes","title":"Multiple Classes","text":"<p>In workflows with multiple classes, the F<sub>1</sub>-score can be computed per class. In the TP / FP / FN / TN guide, we learned how to compute per-class metrics when there are multiple classes, using the one-vs-rest (OvR) strategy. Once you have TP, FP, and FN counts computed for each class, you can compute precision, recall, and F<sub>1</sub>-score for each class by treating each as a single-class problem.</p>"},{"location":"metrics/f1-score/#aggregating-per-class-metrics","title":"Aggregating Per-class Metrics","text":"<p>If you are looking for a single F<sub>1</sub>-score that summarizes model performance across all classes, there are different ways to aggregate per-class F<sub>1</sub>-scores: macro, micro, and weighted. Read more about these methods in the Averaging Methods guide.</p>"},{"location":"metrics/f1-score/#f_beta-score","title":"F\\(_\\beta\\)-score","text":"<p>The F\\(_\\beta\\)-score is a generic form of the F<sub>1</sub>-score with a weight parameter, \\(\\beta\\), where recall is considered \\(\\beta\\) times more important than precision:</p> \\[ \\text{F}_{\\beta} = \\frac {(1 + \\beta^2) \\times \\text{precision} \\times \\text{recall}} {(\\beta^2 \\times \\text{precision}) + \\text{recall}} \\] <p>The three most common values for the beta parameter are as follows:</p> <ul> <li>F<sub>0.5</sub>-score \\(\\left(\\beta = 0.5\\right)\\), where precision is more important than recall, it focuses more on minimizing FPs than minimizing FNs</li> <li>F<sub>1</sub>-score \\(\\left(\\beta = 1\\right)\\), the true harmonic mean of precision and recall</li> <li>F<sub>2</sub>-score \\(\\left(\\beta = 2\\right)\\), where recall is more important than precision, it focuses more on minimizing FNs than minimizing FPs</li> </ul>"},{"location":"metrics/f1-score/#limitations-and-biases","title":"Limitations and Biases","text":"<p>While the F<sub>1</sub>-score can be used to evaluate classification/object detection models with a single metric, this metric is not adequate to use for all applications. In some applications, such as identifying pedestrians from an autonomous vehicle, any false negatives can be life-threatening. In these scenarios, having a few more false positives as a trade-off for reducing the chance of any life-threatening events happening is preferred. Here, recall should be weighted much more than the precision as it minimizes false negatives. To address the significance of recall, \\(\\text{F}_\\beta\\) score can be used as an alternative.</p>"},{"location":"metrics/f1-score/#threshold-dependence","title":"Threshold-Dependence","text":"<p>Precision, recall, and F<sub>1</sub>-score are all threshold-dependent metrics. Threshold-dependent means that, before computing these metrics, a confidence score threshold must be applied to inferences to decide which should be used for metrics computation and which should be ignored.</p> <p>A small change to this confidence score threshold can have a large impact on threshold-dependent metrics. To evaluate a model across all thresholds, rather than at a single-threshold, use threshold-independent metrics, like average precision.</p>"},{"location":"metrics/geometry-matching/","title":"Geometry Matching","text":"<p>Geometry matching is the process of matching inferences to ground truths for computer vision workflows with a localization component, such as 2D and 3D object detection and instance segmentation. It is a building block for metrics like TP / FP / FN counts and any metrics derived from these, such as precision and recall.</p> <p>While it may sound simple, geometry matching is surprisingly challenging and full of edge cases! In this guide, we'll focus on 2D object detection\u2014specifically 2D bounding box matching\u2014to learn about geometry matching algorithms.</p> <ul> <li>  API Reference: <code>match_inferences</code>,   <code>match_inferences_multiclass</code> \u2197</li> </ul>"},{"location":"metrics/geometry-matching/#algorithm-overview","title":"Algorithm Overview","text":"<p>In a geometry matching algorithm, the following criteria must be met for a valid match:</p> <ol> <li>The IoU between the inference and ground truth must be greater than or equal to a threshold</li> <li>For multiclass workflows, inference label must match the ground truth label</li> </ol> Pseudocode: Geometry Matching <ol> <li>Loop through all images in your dataset;</li> <li>Loop through all labels;</li> <li>Get inferences and ground truths with the current label;</li> <li>Sort inferences by descending confidence score;</li> <li>Check against all ground truths and find a ground truth that results in maximum IoU;</li> <li>Check for the following criteria for a valid match:<ol> <li>This ground truth is not matched yet AND</li> <li>The IoU is greater than or equal to the IoU threshold;</li> </ol> </li> <li>Repeat 5-6 on the next inference;</li> </ol>"},{"location":"metrics/geometry-matching/#examples-matching-2d-bounding-boxes","title":"Examples: Matching 2D Bounding Boxes","text":"<p>Let's apply the algorithm above to the following examples of 2D object detection. Bounding boxes (see: <code>BoundingBox</code>) in the diagrams below use the following colors based on their type and the matching result:</p> <p> </p> <p>This example contains two ground truth and two inference bounding boxes, each with the same label. The pair \\((\\text{A}, \\text{a})\\) has high overlap (IoU of 0.9) and the pair \\((\\text{B}, \\text{b})\\) has low overlap (IoU of 0.13). Let's find out what the matched results look like in this example with a IoU threshold of 0.5:</p> <p> </p> <p> Bounding Box Score IoU(\\(\\text{A}\\)) IoU(\\(\\text{B}\\)) \\(\\text{a}\\) 0.98 0.9 0.0 \\(\\text{b}\\) 0.6 0.0 0.13 <p></p> <p>Because inference \\(\\text{a}\\) has a higher confidence score than inference \\(\\text{b}\\), it gets matched first. It is pretty clear that ground truth \\(\\text{A}\\) scores the highest IoU with inference \\(\\text{a}\\), and IoU is greater than IoU threshold, so \\(\\text{a}\\) and \\(\\text{A}\\) are matched.</p> <p>Next, inference \\(\\text{b}\\) gets compared against all ground truth bounding boxes. Once again, it is clear that ground truth \\(\\text{B}\\) scores the maximum IoU with inference \\(\\text{b}\\), but this time IoU is less than the IoU threshold, so \\(\\text{b}\\) becomes an unmatched inference.</p> <p>Now that we have checked all inferences, any ground truth bounding boxes that are not matched yet are marked as unmatched. In this case, ground truth \\(\\text{B}\\) is the only unmatched ground truth.</p> <p> Bounding Box(es) Match Type \\((\\text{A}, \\text{a})\\) Matched Pair \\(\\text{B}\\) Unmatched Ground Truth \\(\\text{b}\\) Unmatched Inference <p></p> <p>Let's take a look at another example with multiple classes, <code>Apple</code> and <code>Banana</code>:</p> <p> </p> <p> Bounding Box Class Score IoU(\\(\\text{A}\\)) \\(\\text{A}\\) <code>Apple</code> \u2014 \u2014 \\(\\text{a}\\) <code>Apple</code> 0.3 0.0 \\(\\text{b}\\) <code>Banana</code> 0.5 0.8 <p></p> <p>Each class is evaluated independently. Starting with <code>Apple</code>, there is one ground truth \\(\\text{A}\\) and one inference \\(\\text{a}\\), but these two do not overlap at all (IoU of 0.0). Because IoU is less than the IoU threshold, there is no match for class <code>Apple</code>.</p> <p>For class <code>Banana</code>, there is only one inference and no ground truths. Therefore, there is also no match for class <code>Banana</code>.</p> <p> Bounding Box(es) Match Type \\(\\text{A}\\) Unmatched Ground Truth \\(\\text{a}\\) Unmatched Inference \\(\\text{b}\\) Unmatched Inference <p></p> <p>Here is another example with multiple inferences overlapping with the same ground truth:</p> <p> </p> <p> Bounding Box Score IoU(\\(\\text{A}\\)) \\(\\text{a}\\) 0.5 0.8 \\(\\text{b}\\) 0.8 0.5 <p></p> <p>Among the two inferences \\(\\text{a}\\) and \\(\\text{b}\\), \\(\\text{b}\\) has a higher confidence score, so \\(\\text{b}\\) gets matched first. IoU between ground truth \\(\\text{A}\\) and \\(\\text{b}\\) is greater than the IoU threshold, so they become a match.</p> <p>Inference \\(\\text{a}\\) is compared with ground truth \\(\\text{A}\\), but even though IoU is greater than the IoU threshold, they cannot become a match because \\(\\text{A}\\) is already matched with \\(\\text{b}\\), so inference \\(\\text{a}\\) remains unmatched.</p> <p> Bounding Box(es) Match Type \\((\\text{A}, \\text{b})\\) Matched Pair \\(\\text{a}\\) Unmatched Inference <p></p> <p>Finally, let's consider another scenario where there are multiple ground truths overlapping with the same inference:</p> <p> </p> <p> Bounding Box Score IoU(\\(\\text{A}\\)) IoU(\\(\\text{B}\\)) \\(\\text{a}\\) 0.8 0.6 0.9 <p></p> <p>Inference \\(\\text{a}\\) has a higher IoU with ground truth \\(\\text{B}\\), so \\(\\text{a}\\) and \\(\\text{B}\\) become matched.</p> <p> Bounding Box(es) Match Type \\((\\text{B}, \\text{a})\\) Matched Pair \\(\\text{A}\\) Unmatched Ground Truth <p></p>"},{"location":"metrics/geometry-matching/#comparison-of-matching-algorithms-from-popular-benchmarks","title":"Comparison of Matching Algorithms from Popular Benchmarks","text":"<p>Geometry matching is a fundamental part of evaluation for workflows with localization. Metrics such as precision, recall, and average precision are built on top of these matches. The matching algorithm we've covered above is standard across various popular object detection benchmarks.</p> <p>In this section, we'll examine the differences in matching algorithm from a few popular benchmarks:</p> <ul> <li>PASCAL VOC 2012</li> <li>COCO</li> <li>Open Images V7</li> </ul>"},{"location":"metrics/geometry-matching/#pascal-voc-2012","title":"PASCAL VOC 2012","text":"<p>The PASCAL VOC 2012 benchmark includes a <code>difficult</code> boolean annotation for each ground truth, used to differentiate objects that are difficult to recognize from an image. Any ground truth with the <code>difficult</code> flag and any inferences that are matched with a <code>difficult</code> ground truth will be ignored in the matching process. In other words, these ground truths and the inferences that are matched with them are excluded in the matched results. Hence, models will not be penalized for failing to detect these <code>difficult</code> objects, nor rewarded for detecting them.</p> <p>Another difference that is noteworthy is how PASCAL VOC outlines the IoU criteria for a valid match. According to the evaluation section (4.4) in development kit doc, IoU must exceed the IoU threshold to be considered as a valid match.</p> Pseudocode: PASCAL VOC Matching <ol> <li>Loop through all images in your dataset;</li> <li>Loop through all labels;</li> <li>Get inferences and ground truths with the evaluating label;</li> <li>Sort inferences by descending confidence score;</li> <li>Check against all ground truths and find a ground truth that results in maximum IoU;</li> <li>Check for the following criteria for a valid match:<ol> <li>This ground truth is not matched yet AND</li> <li>The IoU is greater than the IoU threshold;</li> </ol> </li> <li>If matched with a <code>difficult</code> ground truth, ignore;</li> <li>Repeat 5-7 on the next inference;</li> </ol>"},{"location":"metrics/geometry-matching/#coco","title":"COCO","text":"<p>COCO (Common Objects in Context) labels its ground truth annotations with an <code>iscrowd</code> field to specify when a ground truth includes multiple objects. Similarly to how <code>difficult</code> ground truths are treated in PASCAL VOC, any inferences matched with these <code>iscrowd</code> ground truths, are excluded from the matched results. This <code>iscrowd</code> flag is intended to avoid penalizing models for failing to detect objects in a crowded scene.</p> Pseudocode: COCO Matching <ol> <li>Loop through all images in your dataset;</li> <li>Loop through all labels;</li> <li>Get inferences and ground truths with the evaluating label;</li> <li>Sort inferences by descending confidence score;</li> <li>Check against all ground truths and find a ground truth that results in maximum IoU;</li> <li>Check for the following criteria for a valid match:<ol> <li>This ground truth is not matched yet AND</li> <li>The IoU is greater than or equal to the IoU threshold;</li> </ol> </li> <li>If matched with a <code>iscrowd</code> ground truth, ignore;</li> <li>Repeat 5-7 on the next inference;</li> </ol>"},{"location":"metrics/geometry-matching/#open-images-v7","title":"Open Images V7","text":"<p>The Open Images V7 Challenge evaluation introduces two key differences in its matching algorithm.</p> <p>The first is with the way that the images are annotated in this dataset. Images are annotated with positive image-level labels, indicating certain object classes are present, and with negative image-level labels, indicating certain classes are absent. Therefore, for fair evaluation, all unannotated classes are excluded from evaluation in that image, so if an inference has a class label that is unannotated on that image, this inference is excluded in the matching results.</p> <p> </p> <p>An example of non-exhaustive image-level labeling from Open Images V7</p> <p>The second difference is with handling <code>group-of</code> boxes, which is similar to <code>iscrowd</code> annotation from COCO but is not just simply ignored. If at least one inference is inside the <code>group-of</code> box, then it is considered to be a match. Otherwise, the <code>group-of</code> box is considered as an unmatched ground truth. Also, multiple correct inferences inside the same <code>group-of</code> box still count as a single match:</p> <p> </p> <p>An example of <code>group-of</code> boxes from Open Images V7</p> Pseudocode: Open Images V7 Matching <ol> <li>Loop through all images in your dataset;</li> <li>Loop through all positive image-level labels;</li> <li>Get inferences and ground truths with the evaluating label;</li> <li>Sort inferences by descending confidence score;</li> <li>Check against all non-<code>ground-of</code> ground truths and find a ground truth that results in maximum IoU;</li> <li>Check for the following criteria for a valid match:<ol> <li>This ground truth is not matched yet AND</li> <li>The IoU is greater than or equal to the IoU threshold;</li> </ol> </li> <li>If matched with a <code>difficult</code> ground truth, ignore;</li> <li>Repeat 5-7 on the next inference;</li> <li>Loop through all unmatched inferences;</li> <li>Check against all <code>group-of</code> ground truths and find a ground truth that results in maximum IoU;</li> <li>Check for the matching criteria (6);</li> <li>Repeat 10-11 on the next unmatched inference;</li> </ol>"},{"location":"metrics/geometry-matching/#limitations-and-biases","title":"Limitations and Biases","text":"<p>The standard matching algorithm appears to have an undesirable behavior when there are many overlapping ground truths and inferences with high confidence scores due to its greedy matching. Because it optimizes for higher confidence score and maximum IoU, it can potentially miss valid matches by matching a non-optimal pair, resulting in a poorer matching performance.</p> Example: Greedy Matching <p> </p> <p> Bounding Box Score IoU(\\(\\text{A}\\)) IoU(\\(\\text{B}\\)) \\(\\text{a}\\) 0.7 0.0 0.6 \\(\\text{b}\\) 0.8 0.5 0.7 <p></p> <p>When there are two ground truths and two inferences, one inference \\(\\text{b}\\) with a higher score overlaps well with both ground truths \\(\\text{A}\\) and \\(\\text{B}\\), and the other one, \\(\\text{a}\\), with a lower score overlaps well with just one ground truth \\(\\text{B}\\). Because the IoU between \\(\\text{B}\\) and \\(\\text{b}\\) is greater than IoU between \\(\\text{A}\\) and \\(\\text{b}\\), inference \\(\\text{b}\\) is matched with ground truth \\(\\text{B}\\), causing inference \\(\\text{a}\\) to fail to match.</p> <p>This greedy matching behavior results in a higher false positive count in this type of scenario. Ideally, inference \\(\\text{a}\\) matches with ground truth \\(\\text{B}\\), and inference \\(\\text{b}\\) matches with ground truth \\(\\text{A}\\), resulting in no FPs.</p> <p>Another behavior to note here is that it is possible to get different matching results depending on the ground truth order when there are multiple ground truths overlapping with an inference with the equal IoU or depending on the inference order when there are multiple inferences overlapping with a ground truth with the equal confidence score.</p> Example: Different Matching Results When Ground Truth Order Changes <p> </p> <p> Bounding Box Score IoU(\\(\\text{A}\\)) IoU(\\(\\text{B}\\)) \\(\\text{a}\\) 0.7 0.0 0.5 \\(\\text{b}\\) 0.7 0.5 0.5 <p></p> <p>The three pairs of ground truth and inference have same IoU and both inferences have same confidence score.</p> <p>If the ground truths are ordered as \\([\\text{A}, \\text{B}]\\) and the inferences as \\([\\text{a}, \\text{b}]\\), inference \\(\\text{a}\\) is matched with \\(\\text{B}\\) first, so inference \\(\\text{b}\\) gets matched with \\(\\text{A}\\).</p> <p>If the inference order changes to \\([\\text{b}, \\text{a}]\\), now inference \\(\\text{a}\\) may or may not be matched with any ground truth. The matched result can change depending on the ground truth order. If \\(\\text{A}\\) is evaluated before \\(\\text{B}\\), inference \\(\\text{b}\\) is matched with \\(\\text{A}\\), and \\(\\text{a}\\) can be matched with \\(\\text{B}\\). However, if \\(\\text{B}\\) comes before \\(\\text{A}\\), inference \\(\\text{b}\\) is matched with \\(\\text{B}\\) instead, leaving inference \\(\\text{a}\\) with no match.</p>"},{"location":"metrics/iou/","title":"Intersection over Union (IoU)","text":"<p>Intersection over Union (IoU) measures the ratio of the intersection and the union between ground truth and inference, ranging from 0 to 1 where 1 indicates a perfect match.  The objective of this metric is to compare inferences to ground truths by measuring similarity between them.</p> <p>As the name suggests, the IoU of two instances (\\(\\text{A}\\) and \\(\\text{B}\\)) is defined as:</p> \\[\\text{IoU} \\left( \\text{A}, \\text{B} \\right) = \\frac {\\text{A} \\cap \\text{B}} {\\text{A} \\cup \\text{B}}\\] <ul> <li>  API Reference: <code>iou</code> \u2197</li> </ul>"},{"location":"metrics/iou/#when-do-i-use-iou","title":"When Do I Use IoU?","text":"<p>It is often used to compare two geometries (e.g., <code>BoundingBox</code>, <code>Polygon</code> or <code>SegmentationMask</code>) in object detection, instance segmentation, or semantic segmentation workflows. In multi-label classification, IoU, more likely known as the Jaccard index, is used to compare set of inference labels for a sample to the corresponding set of ground truth labels. Moreover, there are workflows such as action detection and video moment retrieval where IoU measures the temporal overlap between two time-series snippets.</p> <p>Because IoU can be used on various types of data, let's look at how the metric is defined for some of these data types:</p> <ul> <li>2D Axis-Aligned Bounding Box</li> <li>Segmentation Mask</li> <li>Set of Labels</li> </ul>"},{"location":"metrics/iou/#2d-axis-aligned-bounding-box","title":"2D Axis-Aligned Bounding Box","text":"<p>Let's consider two 2D axis-aligned bounding boxes, \\(\\text{A}\\) and \\(\\text{B}\\), with the origin of the coordinates being the top-left corner of the image, and to the right and down are the positive directions of the \\(x\\) and \\(y\\) axes, respectively. This is the most common coordinate system in computer vision.</p> <p> </p> <p>Guides: Commonly Used Bounding Box Representations</p> <p>A bounding box is often defined by the \\(x\\) and \\(y\\) coordinates of the top-left and bottom-right corners. This is the format used in this guide and in the <code>kolena</code> package.</p> <p>Another commonly used representation is the \\(x\\) and \\(y\\) coordinates of bounding box center, along with the width and height of the box.</p> <p>In order to compute IoU for two 2D bounding boxes, the first step is identifying the area of the intersection box, \\((\\text{A} \\cap \\text{B})\\).  This is the highlighted overlap region in the image above. The two coordinates of the intersection box, top-left and bottom-right corners, can be defined as:</p> \\[ \\text{A} \\cap \\text{B}\\,_{\\text{top-left}} = (\\max \\left( x_{a1}, \\, x_{b1} \\right), \\, \\max \\left( y_{a1}, \\, y_{b1} \\right)) \\] \\[ \\text{A} \\cap \\text{B}\\,_{\\text{bottom-right}} = (\\min \\left( x_{a2}, \\, x_{b2} \\right), \\, \\min \\left(y_{a2}, \\, y_{b2} \\right)) \\] <p>Once the intersection box \\((\\text{A} \\cap \\text{B})\\) is identified, the area of the union, \\((\\text{A} \\cup \\text{B})\\), is simply a sum of the area of \\(\\text{A}\\) and \\({\\text{B}}\\) minus the area of the intersection box.</p> \\[ \\text{area} \\left( \\text{A} \\cup \\text{B} \\right) = \\text{area} \\left( \\text{A} \\right) + \\text{area} \\left( \\text{B} \\right) - \\text{area} \\left( \\text{A} \\cap \\text{B} \\right) \\] <p>Finally, IoU is calculated by taking the ratio of the area of intersection box and the area of the union region.</p> \\[ \\begin{align} \\text{IoU} \\left( \\text{A}, \\, \\text{B} \\right) &amp;= \\frac {\\text{area} \\left( \\text{A} \\cap \\text{B} \\right)} {\\text{area} \\left( \\text{A} \\cup \\text{B} \\right)} \\\\[1em] &amp;= \\frac {\\text{area} \\left( \\text{A} \\cap \\text{B} \\right)} {\\text{area} \\left( \\text{A} \\right) + \\text{area} \\left( \\text{B} \\right) - \\text{area} \\left( \\text{A} \\cap \\text{B} \\right)} \\end{align} \\]"},{"location":"metrics/iou/#examples-iou-of-2d-bounding-boxes","title":"Examples: IoU of 2D Bounding Boxes","text":"<p>The following examples show what IoU values look like in different scenarios with 2D bounding boxes:</p> <p>Example 1: overlapping bounding boxes</p> <p> </p> \\[ \\begin{align} \\text{IoU} \\left( \\text{A}, \\, \\text{B} \\right) &amp;= \\frac {(10 - 5) \\times (10 - 2)} {10 \\times 10 + (15 - 5) \\times (12 - 2) - (10 - 5) \\times (10 - 2)} \\\\[1em] &amp;= \\frac {40} {100 + 100 - 40} \\\\[1em] &amp;= 0.25 \\end{align} \\] <p>Example 2: non-overlapping bounding boxes</p> <p> </p> \\[ \\begin{align} \\text{IoU} \\left( \\text{A}, \\, \\text{B} \\right) &amp;= \\frac {0} {10 \\times 10 + (15 - 10) \\times (15 - 10) - 0} \\\\[1em] &amp;= \\frac {0} {100 + 25 - 0} \\\\[1em] &amp;= 0.0 \\end{align} \\] <p>Example 3: completely matching bounding boxes</p> <p> </p> \\[ \\begin{align} \\text{IoU} \\left( \\text{A}, \\, \\text{B} \\right) &amp;= \\frac {10 \\times 10} {10 \\times 10 + 10 \\times 10 - 10 \\times 10} \\\\[1em] &amp;= \\frac {100} {100 + 100 - 100} \\\\[1em] &amp;= 1.0 \\end{align} \\]"},{"location":"metrics/iou/#segmentation-mask","title":"Segmentation Mask","text":"<p>A segmentation mask is a 2D image where each pixel is a class label commonly used in semantic segmentation workflow. The inference shape matches the ground truth shape (width and height), with a channel depth equivalent to the number of class labels to be predicted. Each channel is a binary mask that labels areas where a specific class is present:</p> <p> </p> From left to right: the original RGB image, the ground truth segmentation mask, and the inference segmentation mask <p>The IoU metric measures the intersection (the number of pixels common between the ground truth and inference masks, true positive (TP)) divided by the union (the total number of pixels present across both masks, TP + false negative (FN) + false positive (FP)). And here is the formula for the IoU metric for a segmentation mask:</p> \\[ \\text{IoU} \\left( \\text{A}, \\, \\text{B} \\right) = \\frac {\\text{TP}} {\\text{TP} + \\text{FN} + \\text{FP}} \\] <p>Let\u2019s look at what TP, FP, and FN look like on a segmentation mask:</p> <p> </p> From left to right: the ground truth segmentation mask, the inference segmentation mask, and the overlay with TP, FP, and FN labeled <p>From the cat image shown above, when you overlay the ground truth and inference masks, the pixels that belong to both masks are TP. The pixels that only exist in the ground truth mask are FNs, and the pixels that only exist in the inference mask are FPs. Let's consider the following pixel counts for each category:</p> <p> # True Positives # False Positives # False Negatives 100 25 75 <p></p> <p>Then the IoU becomes:</p> \\[ \\begin{align} \\text{IoU} &amp;= \\frac {100} {100 + 25 + 75} \\\\[1em] &amp;= 0.5 \\end{align} \\]"},{"location":"metrics/iou/#set-of-labels","title":"Set of Labels","text":"<p>The set of labels used in multi-label classification workflow is often a binarized list with a number of label elements, for example, let\u2019s say there are three classes, <code>Airplane</code>, <code>Boat</code>, and <code>Car</code>, and a sample is labeled as <code>Boat</code> and <code>Car</code>. The binary set of labels would then be \\([0, 1, 1]\\), where each element represents each class, respectively.</p> <p>Similar to the segmentation mask, the IoU or Jaccard index metric for the ground truth/inference labels would be the size of the intersection of the two sets (the number of labels common between two sets, or TP) divided by the size of the union of the two sets (the total number of labels present in both sets, TP + FN + FP):</p> \\[ \\text{IoU} \\left( \\text{A}, \\, \\text{B} \\right) = \\frac {\\text{TP}} {\\text{TP} + \\text{FN} + \\text{FP}} \\] <p>The IoU for multi-label classification is defined per class. This technique, also known as one-vs-the-rest (OvR), evaluates each class as a binary classification problem. Per-class IoU values can then be aggregated using different averaging methods. The popular choice for this workflow is macro, so let\u2019s take a look at examples of different averaged IoU/Jaccard index metrics for multiclass multi-label classification:</p>"},{"location":"metrics/iou/#example-macro-iou-of-multi-label-classification","title":"Example: Macro IoU of Multi-label Classification","text":"<p>Consider the case of multi-label classification with classes <code>Airplane</code>, <code>Boat</code>, <code>Car</code>:</p> <p> Set Sample #1 Sample #2 Sample #3 Sample #4 ground truth <code>Airplane</code>, <code>Boat</code>, <code>Car</code> <code>Airplane</code>, <code>Car</code> <code>Boat</code>, <code>Car</code> <code>Airplane</code>, <code>Boat</code>, <code>Car</code> inference <code>Boat</code> <code>Airplane</code>, <code>Boat</code>, <code>Car</code> <code>Airplane</code>, <code>Boat</code>, <code>Car</code> <code>Airplane</code>, <code>Boat</code>, <code>Car</code> <p></p> \\[ \\text{ground truth} = [[1, 1, 1], \\, [1, 0, 1], \\, [0, 1, 1], \\, [1, 1, 1]] \\] \\[ \\text{inference} = [[0, 1, 0], \\, [1, 1, 1], \\, [1, 1, 1], \\, [1, 1, 1]] \\] \\[ \\begin{align} \\text{IoU}_\\text{macro} &amp;= \\frac {\\text{IoU}_\\texttt{Airplane} + \\text{IoU}_\\texttt{Boat} + \\text{IoU}_\\texttt{Car}} {3} \\\\[1em] &amp;= \\frac {\\frac 2 4 + \\frac 3 4 + \\frac 3 4} {3} \\\\[1em] &amp;= \\frac 2 3 \\end{align} \\]"},{"location":"metrics/iou/#limitations-and-biases","title":"Limitations and Biases","text":"<p>IoU works well to measure the overlap between two sets, whether they are types of geometry or a list of labels. However, this metric cannot be directly used to measure the overlap of an inference and <code>iscrowd</code> ground truth, which is an annotation from COCO Detection Challenge Evaluation used to label a\u00a0large groups of objects (e.g., a crowd of people). Therefore, the inference is expected to take up a small portion of the ground truth region, resulting in a low IoU score and a pair not being a valid match. In this scenario, a variation of IoU, called Intersection over Foreground (IoF), is preferred. This variation is used when there are ground truth regions you want to ignore in evaluation, such as <code>iscrowd</code>.</p> <p>The second limitation of IoU is measuring the localization performance of non-overlaps. IoU ranges from 0 (no overlap) to 1 (complete overlap), so when two bounding boxes have zero overlap, it\u2019s hard to tell how bad the localization performance is solely based on IoU. There are variations of IoU, such as signed IoU (SIoU) and generalized IoU (GIoU), that aim to measure the localization error even when there is no overlap. These metrics can replace IoU metric if the objective is to measure the localization performance of non-overlaps.</p>"},{"location":"metrics/precision/","title":"Precision","text":"<p>Precision measures the proportion of positive inferences from a model that are correct, ranging from 0 to 1 (where 1 is best).</p> <p>As shown in this diagram, precision is the fraction of all inferences that are correct:</p> \\[\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\\] <p>In the above formula, \\(\\text{TP}\\) is the number of true positive inferences and \\(\\text{FP}\\) is the number of false positive inferences.</p> <p>Guide: True Positive / False Positive</p> <p>Read the TP / FP / FN / TN guide if you're not familiar with \"TP\" and \"FP\" terminology.</p> <p> </p> <ul> <li>  API Reference: <code>precision</code> \u2197</li> </ul>"},{"location":"metrics/precision/#implementation-details","title":"Implementation Details","text":"<p>Precision is used across a wide range of workflows, including classification, object detection, instance segmentation, semantic segmentation, and information retrieval. It is especially useful when the objective is to measure and reduce false positive inferences.</p> <p>For most workflows, precision is the ratio of the number of correct positive inferences to the total number of positive inferences:</p> \\[\\text{Precision} = \\frac{\\text{# True Positives}}{\\text{# True Positives} + \\text{# False Positives}}\\] <p>For workflows with a localization component, such as object detection and instance segmentation, see the Geometry Matching guide to learn how to compute true positive and false positive counts.</p>"},{"location":"metrics/precision/#examples","title":"Examples","text":"<p>Perfect inferences:</p> Metric Value TP 20 FP 0 \\[ \\begin{align} \\text{Precision} &amp;= \\frac{20}{20 + 0} \\\\[1em] &amp;= 1.0 \\end{align} \\] <p>Partially correct inferences, where some inferences are correct (TP) and others are incorrect (FP):</p> Metric Value TP 90 FP 10 \\[ \\begin{align} \\text{Precision} &amp;= \\frac{90}{90 + 10} \\\\[1em] &amp;= 0.9 \\end{align} \\] <p>Zero correct inferences \u2014 all positive predictions are incorrect:</p> Metric Value TP 0 FP 20 \\[ \\begin{align} \\text{Precision} &amp;= \\frac{0}{0 + 20} \\\\[1em] &amp;= 0.0 \\end{align} \\]"},{"location":"metrics/precision/#multiple-classes","title":"Multiple Classes","text":"<p>So far, we have only looked at binary classification/object detection cases, but in multiclass or multi-label cases, precision is computed per class. In the TP / FP / FN / TN guide, we went over multiple-class cases and how these metrics are computed. Once you have these four metrics computed per class, you can compute precision for each class by treating each as a single-class problem.</p>"},{"location":"metrics/precision/#aggregating-per-class-metrics","title":"Aggregating Per-class Metrics","text":"<p>If you are looking for a single precision score that summarizes model performance across all classes, there are different ways to aggregate per-class precision scores: macro, micro, and weighted. Read more about these methods in the Averaging Methods guide.</p>"},{"location":"metrics/precision/#limitations-and-biases","title":"Limitations and Biases","text":"<p>As seen in its formula, precision only takes positive inferences (TP and FP) into account; negative inferences (TN and FN) are not considered. Thus, precision only provides one half of the picture, and should always be used in tandem with recall: recall penalizes false negatives (FN), whereas precision does not.</p> <p>For a single metric that takes both precision and recall into account, use F<sub>1</sub>-score, which is the harmonic mean between precision and recall.</p>"},{"location":"metrics/recall/","title":"Recall (TPR, Sensitivity)","text":"<p>Recall, also known as true positive rate (TPR) and sensitivity, measures the proportion of all positive ground truths that a model correctly predicts, ranging from 0 to 1 (where 1 is best).</p> <p>As shown in this diagram, recall is the fraction of all positive ground truths that are correctly predicted:</p> \\[\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\] <p>In the above formula, \\(\\text{TP}\\) is the number of true positive inferences and \\(\\text{FN}\\) is the number of false negative ground truths.</p> <p>Guide: True Positive / False Negative</p> <p>Read the TP / FP / FN / TN guide if you're not familiar with \"TP\" and \"FN\" terminology.</p> <p> </p> <ul> <li>  API Reference: <code>recall</code> \u2197</li> </ul>"},{"location":"metrics/recall/#implementation-details","title":"Implementation Details","text":"<p>Recall is used across a wide range of workflows, including classification, object detection, instance segmentation, semantic segmentation, and information retrieval. It is especially useful when the objective is to measure and reduce false negative ground truths, i.e. model misses.</p> <p>For most tasks, recall is the ratio of the number of correct positive inferences to the total number of positive ground truths.</p> \\[ \\text{Recall} = \\frac {\\text{# True Positives}} {\\text{# True Positives} + \\text{# False Negatives}} \\] <p>For workflows with a localization component, such as object detection and instance segmentation, see the Geometry Matching guide to learn how to compute true positive and false negative counts.</p>"},{"location":"metrics/recall/#examples","title":"Examples","text":"<p>Perfect model inferences, where every ground truth is recalled by an inference:</p> Metric Value TP 20 FN 0 \\[ \\begin{align} \\text{Recall} &amp;= \\frac{20}{20 + 0} \\\\[1em] &amp;= 1.0 \\end{align} \\] <p>Partially correct inferences, where some ground truths are correctly recalled (TP) and others are missed (FN):</p> Metric Value TP 85 FN 15 \\[ \\begin{align} \\text{Recall} &amp;= \\frac{85}{85 + 15} \\\\[1em] &amp;= 0.85 \\end{align} \\] <p>Zero correct inferences \u2014 no positive ground truths are recalled:</p> Metric Value TP 0 FN 20 \\[ \\begin{align} \\text{Recall} &amp;= \\frac{0}{0 + 20} \\\\[1em] &amp;= 0.0 \\end{align} \\]"},{"location":"metrics/recall/#multiple-classes","title":"Multiple Classes","text":"<p>So far, we have only looked at binary classification/object detection cases, but in multiclass or multi-label cases, recall is computed per class. In the TP / FP / FN / TN guide, we went over multiple-class cases and how these metrics are computed. Once you have these four metrics computed per class, you can compute recall for each class by treating each as a single-class problem.</p>"},{"location":"metrics/recall/#aggregating-per-class-metrics","title":"Aggregating Per-class Metrics","text":"<p>If you are looking for a single recall score that summarizes model performance across all classes, there are different ways to aggregate per-class recall scores: macro, micro, and weighted. Read more about these methods in the Averaging Methods guide.</p>"},{"location":"metrics/recall/#limitations-and-biases","title":"Limitations and Biases","text":"<p>As seen in its formula, recall only takes positive ground truths (TP and FN) into account; negative ground truths (TN and FP) are not considered. Thus, recall only provides one half of the picture, and should always be used in tandem with precision: precision penalizes false positives (FP), whereas recall does not.</p> <p>For a single metric that takes both precision and recall into account, use F<sub>1</sub>-score, which is the harmonic mean between precision and recall.</p>"},{"location":"metrics/tp-fp-fn-tn/","title":"TP / FP / FN / TN","text":"<p>The counts of true positive (TP), false positive (FP), false negative (FN), and true negative (TN) ground truths and inferences are essential for summarizing model performance. These metrics are the building blocks of many other metrics, including accuracy, precision, and recall.</p> Metric Description True Positive TP An instance for which both predicted and actual values are positive False Positive FP An instance for which predicted value is positive but actual value is negative False Negative FN An instance for which predicted value is negative but actual value is positive True Negative TN An instance for which both predicted and actual values are negative <p>To compute these metrics, each inference is compared to a ground truth and categorized into one of the four groups. Let\u2019s say we\u2019re building a dog classifier that predicts whether an image has a dog or not:</p> <p> Positive Inference (Dog) Negative Inference (No Dog) Positive Ground Truth (Dog) True Positive (TP) False Negative (FN) Negative Ground Truth (No Dog) False Positive (FP) True Negative (TN) <p></p> <p>Images of a dog are positive samples, and images without a dog are negative samples.</p> <p>If a classifier predicts that there is a dog on a positive sample, that inference is a true positive (TP). If that classifier predicts that there isn\u2019t a dog on a positive sample, that inference is a false negative (FN).</p> <p>Similarly, if that classifier predicts that there is a dog on a negative sample, that inference is a false positive (FP). A negative inference on a negative sample is a true negative (TN).</p>"},{"location":"metrics/tp-fp-fn-tn/#implementation-details","title":"Implementation Details","text":"<p>The TP / FP / FN / TN metrics have been around for a long time and are mainly used to evaluate classification, detection, and segmentation models.</p> <p>The implementation of these metrics is simple and straightforward. That said, there are different guidelines and edge cases to be aware of for binary and multiclass problems as well as object detection and other workflows.</p>"},{"location":"metrics/tp-fp-fn-tn/#classification","title":"Classification","text":"<p>There are three types of classification workflows: binary, multiclass, and multi-label.</p>"},{"location":"metrics/tp-fp-fn-tn/#binary","title":"Binary","text":"<p>In binary classification workflow, TP, FN, FP, and TN are implemented as follows:</p> Variable Type Description <code>ground_truths</code> <code>List[bool]</code> Ground truth labels, where <code>True</code> indicates a positive sample <code>inferences</code> <code>List[float]</code> Predicted confidence scores, where a higher score indicates a higher confidence of the sample being positive <code>T</code> <code>float</code> Threshold value to compare against the inference\u2019s confidence score, where <code>score &gt;= T</code> is positive <p>Should Threshold Be Inclusive or Exclusive?</p> <p>A confidence threshold is defined as \"the minimum score that the model will consider the inference to be positive (i.e. true)\". Therefore, it is a standard practice to consider inferences with confidence score greater than or equal to the confidence threshold as positive.</p> <p>With these inputs, TP / FP/ FN / TN metrics are defined:</p> <pre><code>TP = sum(    gt and inf &gt;= T for gt, inf in zip(ground_truths, inferences))\nFP = sum(not gt and inf &gt;= T for gt, inf in zip(ground_truths, inferences))\nFN = sum(    gt and inf &lt;  T for gt, inf in zip(ground_truths, inferences))\nTN = sum(not gt and inf &lt;  T for gt, inf in zip(ground_truths, inferences))\n</code></pre> Example: Binary Classification <p>This example considers five samples with the following ground truths, inferences, and threshold:</p> <pre><code>ground_truths = [False, True, False, False, True]\ninferences = [0.3, 0.2, 0.9, 0.4, 0.5]\nT = 0.5\n</code></pre> <p>Using the above formula for TP, FP, FN, and TN yields the following metrics:</p> <pre><code>print(f\"TP={TP}, FP={FP}, FN={FN}, TN={TN}\")\n# TP=1, FN=1, FP=1, TN=2\n</code></pre>"},{"location":"metrics/tp-fp-fn-tn/#multiclass","title":"Multiclass","text":"<p>TP / FP / FN / TN metrics are computed a little differently in a multiclass classification workflow.</p> <p>For a multiclass classification workflow, these four metrics are defined per class. This technique, also known as one-vs-rest (OvR), essentially evaluates each class as a binary classification problem.</p> <p>Consider a classification problem where a given image belongs to either the <code>Airplane</code>, <code>Boat</code>, or <code>Car</code> class. Each of these TP / FP / FN / TN metrics is computed for each class. For class <code>Airplane</code>, the metrics are defined as follows:</p> Metric Example True Positive Any image predicted as an <code>Airplane</code> that is labeled as an <code>Airplane</code> False Positive Any image predicted as an <code>Airplane</code> that is not labeled as an <code>Airplane</code> (e.g. labeled as <code>Boat</code> but predicted as <code>Airplane</code>) False Negative Any image not predicted as an <code>Airplane</code> that is labeled as an <code>Airplane</code> (e.g. labeled as <code>Airplane</code> but predicted as <code>Car</code>) True Negative Any image not predicted as an <code>Airplane</code> that is not labeled as an <code>Airplane</code> (e.g. labeled as <code>Boat</code> but predicted as <code>Boat</code> or <code>Car</code>)"},{"location":"metrics/tp-fp-fn-tn/#multi-label","title":"Multi-label","text":"<p>In a multi-label classification workflow, TP / FP / FN / TN are computed per class, like in multiclass classification.</p> <p>A sample is considered to be a positive one if the ground truth includes the evaluating class; otherwise, it\u2019s a negative sample. The same logic can be applied to the inferences, so, for example, if a classifier predicts that this sample belongs to class <code>Airplane</code> and <code>Boat</code>, and the ground truth for the same sample is only class <code>Airplane</code>, then this sample is considered to be a TP for class <code>Airplane</code>, and FP for class <code>Boat</code>.</p> <p>Multi-label classification workflow can alternately be thought of as a collection of binary classification workflows.</p>"},{"location":"metrics/tp-fp-fn-tn/#object-detection","title":"Object Detection","text":"<p>There are some differences in how these four metrics work for a detection workflow compared to a classification workflow. Rather than being computed at the sample level (e.g. per image), they're computed at the instance level (i.e. per object) for instances that the model is detecting. When given an image with multiple objects, each inference and each ground truth is assigned to one group, and the definitions of the terms are slightly altered:</p> Metric Description True Positive TP Positive inference (<code>score &gt;= T</code>) that is matched with a ground truth False Positive FP Positive inference (<code>score &gt;= T</code>) that is not matched with a ground truth False Negative FN Ground truth that is not matched with an inference or that is matched with a negative inference (<code>score &lt; T</code>) True Negative TN <p> Poorly defined for object detection! </p><p>In object detection workflow, a true negative is any non-object that isn't detected as an object. This isn't well defined and as such true negative isn't a commonly used metric in object detection.</p>Occasionally, for object detection workflow \"true negative\" is used to refer to any image that does not have any true positive or false positive inferences. <p>In object detection workflow, checking for detection correctness requires a couple of other metrics (e.g., Intersection over Union (IoU) and Geometry Matching).</p>"},{"location":"metrics/tp-fp-fn-tn/#single-class","title":"Single-class","text":"<p>Let\u2019s assume that a matching algorithm has already been run on all inferences and that the matched pairs and unmatched ground truths and inferences are given. Consider the following variables, adapted from <code>match_inferences</code>:</p> Variable Type Description <code>matched</code> <code>List[Tuple[GT, Inf]]</code> List of matched ground truth and inference bounding box pairs <code>unmatched_gt</code> <code>List[GT]</code> List of unmatched ground truth bounding boxes <code>unmatched_inf</code> <code>List[Inf]</code> List of unmatched inference bounding boxes <code>T</code> <code>float</code> Threshold used to filter valid inference bounding boxes based on their confidence scores <p>Then these metrics are defined:</p> <pre><code>TP = len([inf.score &gt;= T for _, inf in matched])\nFN = len([inf.score &lt;  T for _, inf in matched]) + len(unmatched_gt)\nFP = len([inf.score &gt;= T for inf in unmatched_inf])\n</code></pre> Example: Single-class Object Detection <p> </p> <p> Bounding Box Score IoU(\\(\\text{A}\\)) IoU(\\(\\text{B}\\)) \\(\\text{a}\\) 0.98 0.9 0.0 \\(\\text{b}\\) 0.6 0.0 0.13 <p></p> <p>This example includes two ground truths and two inferences, and when computed with an IoU threshold of 0.5 and confidence score threshold of 0.5 yields:</p> <p> TP FP FN 1 1 1 <p></p>"},{"location":"metrics/tp-fp-fn-tn/#multiclass_1","title":"Multiclass","text":"<p>Like classification, multiclass object detection workflow compute TP / FP / FN per class.</p> Example: Multiclass Object Detection <p> </p> <p> Bounding Box Class Score IoU(\\(\\text{A}\\)) \\(\\text{A}\\) <code>Apple</code> \u2014 \u2014 \\(\\text{a}\\) <code>Apple</code> 0.3 0.0 \\(\\text{b}\\) <code>Banana</code> 0.5 0.8 <p></p> <p>Similar to multiclass classification, TP / FP / FN are computed for class <code>Apple</code> and class <code>Banana</code> separately.</p> <p>Using an IoU threshold of 0.5 and a confidence score threshold of 0.5, this example yields:</p> <p> Class TP FP FN <code>Apple</code> 0 0 1 <code>Banana</code> 0 1 0 <p></p>"},{"location":"metrics/tp-fp-fn-tn/#averaging-per-class-metrics","title":"Averaging Per-class Metrics","text":"<p>For problems with multiple classes, these TP / FP / FN / TN metrics are computed for each class. If you are looking for a single score that summarizes model performance across all classes, there are a few different ways to aggregate per-class metrics: macro, micro, and weighted.</p> <p>Read more about these different averaging methods in the Averaging Methods guide.</p>"},{"location":"metrics/tp-fp-fn-tn/#limitations-and-biases","title":"Limitations and Biases","text":"<p>TP, FP, FN, and TN are four metrics based on the assumption that each sample/instance can be classified as a positive or a negative, thus they can only be applied to single-class applications. The workaround for multiple-class applications is to compute these metrics for each label using the one-vs-rest (OvR) strategy and then treat it as a single-class problem.</p> <p>Additionally, these four metrics don't take model confidence score into account. All inferences above the confidence score threshold are treated the same! For example, when using a confidence score threshold of 0.5, an inference with a confidence score barely above the threshold (e.g. 0.55) is treated the same as an inference with a very high confidence score (e.g. 0.99). In other words, any inference above the confidence threshold is considered as a positive inference. To examine performance taking confidence score into account, consider plotting a histogram of the distribution of confidence scores.</p>"},{"location":"reference/","title":"API Reference","text":"<p>This section contains detailed API reference documentation for <code>kolena</code>.</p> <ul> <li> <p><code>kolena.initialize</code></p> <p>Initialize client sessions</p> </li> <li> <p><code>kolena.errors</code></p> <p>Custom error definitions</p> </li> </ul> <ul> <li> <p> <code>kolena.workflow</code></p> <p>Building blocks to test any ML problem in Kolena</p> </li> </ul> <ul> <li> <p>  Pre-built Workflows</p> <p>Ready-to-use workflows built with <code>kolena.workflow</code>.</p> </li> </ul> <ul> <li> <p>  Legacy Definitions</p> <p>Built-in <code>kolena.classification</code>, <code>kolena.detection</code>, and <code>kolena.fr</code> workflows</p> </li> </ul>"},{"location":"reference/errors/","title":"<code>kolena.errors</code>","text":"<p>Reference for various exceptions raised from <code>kolena</code>. All custom exceptions extend the base <code>KolenaError</code>.</p>"},{"location":"reference/errors/#kolena.errors.KolenaError","title":"<code>KolenaError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Base error for all Kolena errors to extend. Allows consumers to catch Kolena specific errors.</p>"},{"location":"reference/errors/#kolena.errors.InputValidationError","title":"<code>InputValidationError</code>","text":"<p>             Bases: <code>ValueError</code>, <code>KolenaError</code></p> <p>Exception indicating that provided input data failed validation.</p>"},{"location":"reference/errors/#kolena.errors.IncorrectUsageError","title":"<code>IncorrectUsageError</code>","text":"<p>             Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that the user performed a disallowed action with the client.</p>"},{"location":"reference/errors/#kolena.errors.InvalidTokenError","title":"<code>InvalidTokenError</code>","text":"<p>             Bases: <code>ValueError</code>, <code>KolenaError</code></p> <p>Exception indicating that provided token value was invalid.</p>"},{"location":"reference/errors/#kolena.errors.InvalidClientStateError","title":"<code>InvalidClientStateError</code>","text":"<p>             Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that client state was invalid.</p>"},{"location":"reference/errors/#kolena.errors.UninitializedError","title":"<code>UninitializedError</code>","text":"<p>             Bases: <code>InvalidClientStateError</code></p> <p>Exception indicating that the client has not been properly initialized before usage.</p>"},{"location":"reference/errors/#kolena.errors.DirectInstantiationError","title":"<code>DirectInstantiationError</code>","text":"<p>             Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that the default constructor was used for a class that does not support direct instantiation. Available static constructors should be used when this exception is encountered.</p>"},{"location":"reference/errors/#kolena.errors.FrozenObjectError","title":"<code>FrozenObjectError</code>","text":"<p>             Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that the user attempted to modify a frozen object.</p>"},{"location":"reference/errors/#kolena.errors.UnauthenticatedError","title":"<code>UnauthenticatedError</code>","text":"<p>             Bases: <code>HTTPError</code>, <code>KolenaError</code></p> <p>Exception indicating unauthenticated usage of the client.</p>"},{"location":"reference/errors/#kolena.errors.RemoteError","title":"<code>RemoteError</code>","text":"<p>             Bases: <code>HTTPError</code>, <code>KolenaError</code></p> <p>Exception indicating that a remote error occurred in communications between the Kolena client and server.</p>"},{"location":"reference/errors/#kolena.errors.CustomMetricsException","title":"<code>CustomMetricsException</code>","text":"<p>             Bases: <code>KolenaError</code></p> <p>Exception indicating that there's an error when computing custom metrics.</p>"},{"location":"reference/errors/#kolena.errors.WorkflowMismatchError","title":"<code>WorkflowMismatchError</code>","text":"<p>             Bases: <code>KolenaError</code></p> <p>Exception indicating a workflow mismatch.</p>"},{"location":"reference/errors/#kolena.errors.NotFoundError","title":"<code>NotFoundError</code>","text":"<p>             Bases: <code>RemoteError</code></p> <p>Exception indicating an entity is not found</p>"},{"location":"reference/errors/#kolena.errors.NameConflictError","title":"<code>NameConflictError</code>","text":"<p>             Bases: <code>RemoteError</code></p> <p>Exception indicating the name of an entity is conflict</p>"},{"location":"reference/initialize/","title":"<code>kolena.initialize</code>","text":""},{"location":"reference/initialize/#kolena.initialize.initialize","title":"<code>initialize(api_token, *args, verbose=False, proxies=None, **kwargs)</code>","text":"<p>Initialize a client session.</p> <p>Retrieve an API token from the   Developer page and populate the <code>KOLENA_TOKEN</code> environment variable before initializing:</p> <pre><code>import os\nimport kolena\nkolena.initialize(os.environ[\"KOLENA_TOKEN\"], verbose=True)\n</code></pre> <p>A session has a global scope and remains active until interpreter shutdown.</p> <p>Note</p> <p>As of version 0.29.0: the <code>entity</code> argument is no longer needed; the signature <code>initialize(entity, api_token)</code> has been deprecated and replaced by <code>initialize(api_token)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>api_token</code> <code>str</code> <p>Provided API token. This token is a secret and should be treated with caution.</p> required <code>verbose</code> <code>bool</code> <p>Optionally configure client to run in verbose mode, providing more information about execution. All logging events are emitted as Python standard library <code>logging</code> events from the <code>\"kolena\"</code> logger as well as to stdout/stderr directly.</p> <code>False</code> <code>proxies</code> <code>Optional[Dict[str, str]]</code> <p>Optionally configure client to run with <code>http</code> or <code>https</code> proxies. The <code>proxies</code> parameter is passed through to the <code>requests</code> package and can be configured accordingly.</p> <code>None</code> <p>Raises:</p> Type Description <code>InvalidTokenError</code> <p>The provided <code>api_token</code> is not valid.</p> <code>InputValidationError</code> <p>The provided combination or number of args is not valid.</p>"},{"location":"reference/pre-built/","title":"Pre-built Workflows","text":"<p>Ready-to-use workflows built with <code>kolena.workflow</code>.</p> <ul> <li> <p>  Object Detection (2D)</p> <p></p> <p>Object Detection on 2D images using axis-aligned bounding boxes.</p> </li> </ul>"},{"location":"reference/pre-built/classification/","title":"Classification","text":"<p>Experimental Feature</p> <p>This pre-built workflow is an experimental feature. Experimental features are under active development and may occasionally undergo API-breaking changes.</p> <p>Classification is a machine learning task aiming to group objects and ideas into preset categories. Classification models used in machine learning predict the likelihood or probability that the data will fall into one of the predetermined categories.</p> <p>There are different types of classification models:</p> Classification Type Description Binary Classification model predicts a single class, using a threshold on prediction confidence to bisect the test set Multiclass Classification model predicts a single class from more than two classes, with highest prediction confidence Multi-label Classification model predicts multiple classes, with each prediction over a threshold considered positive (i.e. ensemble of binary classifiers) <p>This pre-built workflow is work in progress; however, you can refer to the workflow implementation for binary and multiclass types from the examples below:</p> <ul> <li> <p>  Example: Binary Classification</p> <p></p> <p>Binary Classification of class \"Dog\" using the Dogs vs. Cats dataset</p> </li> <li> <p>  Example: Multiclass Classification</p> <p></p> <p>Multiclass Classification using the CIFAR-10 dataset</p> </li> </ul>"},{"location":"reference/pre-built/classification/#utility-methods","title":"Utility Methods","text":""},{"location":"reference/pre-built/classification/#kolena._experimental.classification.utils.compute_confusion_matrix","title":"<code>compute_confusion_matrix(ground_truths, inferences, title='Confusion Matrix', labels=None)</code>","text":"<p>Computes confusion matrix given a list of ground truth and inference labels.</p> <p>For a binary classification case, a 2x2 confusion matrix with the count of TP, FP, FN, and TP is computed.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[str]</code> <p>The ground truth labels.</p> required <code>inferences</code> <code>List[str]</code> <p>The inference labels.</p> required <code>title</code> <code>Optional[str]</code> <p>The title of confusion matrix.</p> <code>'Confusion Matrix'</code> <code>labels</code> <code>Optional[List[str]]</code> <p>The list of labels to index the matrix. This may be used to reorder or select a subset of labels. By default, labels that appear at least once in <code>ground_truths</code> or <code>inferences</code> are used in sorted order.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ConfusionMatrix]</code> <p>The <code>ConfusionMatrix</code>.</p>"},{"location":"reference/pre-built/classification/#kolena._experimental.classification.utils.compute_roc_curves","title":"<code>compute_roc_curves(ground_truths, inferences, labels=None, title=None)</code>","text":"<p>Computes OvR (one-vs-rest) ROC (receiver operating characteristic) curves for each class appears in <code>ground_truths</code> if not specified.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[Optional[ClassificationLabel]]</code> <p>The list of ground truth <code>ClassificationLabel</code>. For binary classification, the negative class can be <code>None</code>.</p> required <code>inferences</code> <code>List[List[ScoredClassificationLabel]]</code> <p>The list of inference <code>ScoredClassificationLabel</code>. For <code>N</code>-class problems, each inference is expected to contain <code>N</code> entries, one for each class and its associated confidence score.</p> required <code>labels</code> <code>Optional[List[str]]</code> <p>The labels to plot. If not specified, classes appear in <code>ground_truths</code> are used. Use <code>labels</code> to specify the evaluating classes especially if <code>ground_truths</code> only have negative classes.</p> <code>None</code> <code>title</code> <code>Optional[str]</code> <p>The title of the plot.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[CurvePlot]</code> <p>A <code>CurvePlot</code> if there is any valid <code>Curve</code> computed; otherwise, <code>None</code>.</p>"},{"location":"reference/pre-built/classification/#kolena._experimental.classification.utils.compute_threshold_curves","title":"<code>compute_threshold_curves(ground_truths, inferences, thresholds=None)</code>","text":"<p>Computes scores (i.e. Precision, Recall and F1-score) vs. threshold curves for a single class presented in <code>inferences</code>.</p> <p>Expects <code>ground_truths</code> and <code>inferences</code> correspond to the same sample for the same given index.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[Optional[ClassificationLabel]]</code> <p>The list of ground truth <code>ClassificationLabel</code>s. For binary classification, the negative class can be <code>None</code>.</p> required <code>inferences</code> <code>List[ScoredClassificationLabel]</code> <p>The list of inference <code>ScoredClassificationLabel</code>s. The length of <code>inferences</code> must match the length of <code>ground_truths</code>. The list should only include inferences of a specific class to plot the threshold curves for.</p> required <code>thresholds</code> <code>Optional[List[float]]</code> <p>The list of thresholds to plot with. If not specified, all the unique confidence scores are used as thresholds, including evenly spaced thresholds from 0 to 1 with 0.1 step.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[List[Curve]]</code> <p>A list of <code>Curve</code>s if there is any valid <code>Curve</code> computed; otherwise, <code>None</code>.</p>"},{"location":"reference/pre-built/classification/#kolena._experimental.classification.utils.create_histogram","title":"<code>create_histogram(values, range, title='', x_label='', y_label='')</code>","text":"<p>Creates a <code>Histogram</code> for the specified range and the number of bins.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>List[float]</code> <p>The list of confidence scores to plot.</p> required <code>range</code> <code>Tuple[float, float, int]</code> <p>The min, max and # of bins of the histogram.</p> required <code>title</code> <code>str</code> <p>The title of the plot.</p> <code>''</code> <code>x_label</code> <code>str</code> <p>The label on the x-axis.</p> <code>''</code> <code>y_label</code> <code>str</code> <p>The label on the y-axis.</p> <code>''</code> <p>Returns:</p> Type Description <code>Histogram</code> <p>The <code>Histogram</code>.</p>"},{"location":"reference/pre-built/classification/#kolena._experimental.classification.utils.get_histogram_range","title":"<code>get_histogram_range(values)</code>","text":"<p>Computes an ideal range for a confidence score histograms given a list of confidence scores.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>List[float]</code> <p>The list of confidence scores, [0, 1].</p> required <p>Returns:</p> Type Description <code>Optional[Tuple[float, float, int]]</code> <p>A tuple of min, max and # of bins for a confidence score histograms. The range is rounded up/down to the nearest 0.02. The bin size is 0.02.</p>"},{"location":"reference/pre-built/classification/#kolena._experimental.classification.utils.get_label_confidence","title":"<code>get_label_confidence(label, inference_labels)</code>","text":"<p>Returns the confidence score of the specified <code>label</code> from a list of confidence scores for each label.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>The label whose confidence score to return.</p> required <code>inference_labels</code> <code>List[ScoredClassificationLabel]</code> <p>The list of confidence scores for each label. For <code>N</code>-class problem, expected to have <code>N</code> entries, one for each class.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The confidence score of the specified <code>label</code>. If the <code>label</code> doesn't exist in <code>inference_labels</code> then returns 0.</p>"},{"location":"reference/pre-built/object-detection-2d/","title":"Object Detection (2D)","text":"<p>Experimental Feature</p> <p>This pre-built workflow is an experimental feature. Experimental features are under active development and may occasionally undergo API-breaking changes.</p> <p>Object Detection (OD) is a computer vision task that aims to classify and locate objects of interest presented in an image. So, it can be viewed as a combination of localization and classification tasks.</p> <p>This pre-built workflow is prepared for a 2D Object Detection problem and here is an example of using this workflow on the COCO dataset.</p> <ul> <li> <p>  Example: Object Detection (2D) \u2197</p> <p></p> <p>2D Object Detection using the COCO dataset</p> </li> </ul>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.TestSample","title":"<code>TestSample</code>","text":"<p>             Bases: <code>Image</code></p> <p>The <code>Image</code> sample type for the pre-built 2D Object Detection workflow.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.workflow.TestSample.metadata","title":"<code>metadata: Metadata = dataclasses.field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The optional <code>Metadata</code> dictionary.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.ObjectDetectionEvaluator","title":"<code>ObjectDetectionEvaluator</code>","text":"<p>             Bases: <code>Evaluator</code></p> <p>This <code>ObjectDetectionEvaluator</code> transforms inferences into metrics for the object detection workflow for a single class or multiple classes.</p> <p>When a <code>ThresholdConfiguration</code> is configured to use an F1-Optimal threshold strategy, the evaluator requires that the first test case retrieved for a test suite contains the complete sample set.</p> <p>For additional functionality, see the associated base class documentation.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.GroundTruth","title":"<code>GroundTruth</code>","text":"<p>             Bases: <code>BaseGroundTruth</code></p> <p>Ground truth type for the pre-built 2D Object Detection workflow.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.workflow.GroundTruth.bboxes","title":"<code>bboxes: List[LabeledBoundingBox]</code>  <code>instance-attribute</code>","text":"<p>The ground truth <code>LabeledBoundingBox</code>es associated with an image.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.workflow.GroundTruth.ignored_bboxes","title":"<code>ignored_bboxes: List[LabeledBoundingBox] = dataclasses.field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The ground truth <code>LabeledBoundingBox</code>es to be ignored in evaluation associated with an image.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.Inference","title":"<code>Inference</code>","text":"<p>             Bases: <code>BaseInference</code></p> <p>Inference type for the pre-built 2D Object Detection workflow.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.workflow.Inference.bboxes","title":"<code>bboxes: List[ScoredLabeledBoundingBox]</code>  <code>instance-attribute</code>","text":"<p>The inference <code>ScoredLabeledBoundingBox</code>es associated with an image.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.workflow.Inference.ignored","title":"<code>ignored: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the image (and its associated inference <code>bboxes</code>) should be ignored in evaluating the results of the model.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.ThresholdStrategy","title":"<code>ThresholdStrategy</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Threshold strategy enumerations used in <code>ThresholdConfiguration</code>.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.workflow.ThresholdStrategy.F1_OPTIMAL","title":"<code>F1_OPTIMAL = 'F1_OPTIMAL'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Confidence threshold that yields the most optimal F1-score.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.workflow.ThresholdStrategy.FIXED_03","title":"<code>FIXED_03 = 'FIXED_03'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Confidence threshold fixed at 0.3.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.workflow.ThresholdStrategy.FIXED_05","title":"<code>FIXED_05 = 'FIXED_05'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Confidence threshold fixed at 0.5.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.workflow.ThresholdStrategy.FIXED_075","title":"<code>FIXED_075 = 'FIXED_075'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Confidence threshold fixed at 0.75.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.ThresholdConfiguration","title":"<code>ThresholdConfiguration</code>","text":"<p>             Bases: <code>EvaluatorConfiguration</code></p> <p>Confidence and IoU \u2197 threshold configuration for the pre-built 2D Object Detection workflow. Specify a confidence and IoU threshold to apply to all classes.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.workflow.ThresholdConfiguration.threshold_strategy","title":"<code>threshold_strategy: ThresholdStrategy</code>  <code>instance-attribute</code>","text":"<p>The confidence threshold strategy.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.workflow.ThresholdConfiguration.iou_threshold","title":"<code>iou_threshold: float</code>  <code>instance-attribute</code>","text":"<p>The IoU \u2197 threshold.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.workflow.ThresholdConfiguration.with_class_level_metrics","title":"<code>with_class_level_metrics: bool</code>  <code>instance-attribute</code>","text":"<p>The flag that enables multiclass evaluation. If it's set to <code>False</code> on a multiclass <code>TestSuite</code>, it will only perform localization evaluation by treating it as a binary class problem.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.workflow.ThresholdConfiguration.min_confidence_score","title":"<code>min_confidence_score: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The minimum confidence score to consider for the evaluation. This is usually set to reduce noise by excluding inferences with low confidence score.</p>"},{"location":"reference/workflow/","title":"<code>kolena.workflow</code>","text":"<ul> <li>  Developer Guide: Building a Workflow \u2197</li> <li>  Examples: <code>kolena/examples</code> \u2197</li> </ul> <p><code>kolena.workflow</code> contains the definitions to build a workflow:</p> <ol> <li> <p>Design data types, including any <code>annotations</code> or <code>assets</code>:</p> <p>Defining a workflow</p> <p><code>TestSample</code>, <code>GroundTruth</code>, and <code>Inference</code> can be thought of as the data model, or schema, for a workflow.</p> <p>  Learn more \u2197</p> <ul> <li><code>TestSample</code>: model inputs, e.g. images, videos, documents</li> <li><code>GroundTruth</code>: expected model outputs</li> <li><code>Inference</code>: real model outputs</li> </ul> </li> <li> <p>Define metrics and how they are computed:</p> <ul> <li><code>Evaluator</code>: metrics computation engine</li> </ul> </li> <li> <p>Create tests:</p> <p>Managing tests</p> <p>See the test case and test suite developer guide for an introduction to the test case and test suite concept.</p> <ul> <li><code>TestCase</code>: a test dataset, or a slice thereof</li> <li><code>TestSuite</code>: a collection of test cases</li> </ul> </li> <li> <p>Test models:</p> <ul> <li><code>Model</code>: descriptor for a model</li> <li><code>test</code>: interface to run tests</li> </ul> </li> </ol>"},{"location":"reference/workflow/annotation/","title":"Annotations: <code>kolena.workflow.annotation</code>","text":"<p>Annotations are visualized in Kolena as overlays on top of <code>TestSample</code> objects.</p> <p>The following annotation types are available:</p> <ul> <li><code>BoundingBox</code></li> <li><code>Polygon</code></li> <li><code>Polyline</code></li> <li><code>Keypoints</code></li> <li><code>BoundingBox3D</code></li> <li><code>SegmentationMask</code></li> <li><code>BitmapMask</code></li> <li><code>ClassificationLabel</code></li> </ul> <p>For example, when viewing images in the Studio, any annotations (such as lists of <code>BoundingBox</code> objects) present in the <code>TestSample</code>, <code>GroundTruth</code>, <code>Inference</code>, or <code>MetricsTestSample</code> objects are rendered on top of the image.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Annotation","title":"<code>Annotation</code>","text":"<p>             Bases: <code>TypedDataObject[_AnnotationType]</code></p> <p>The base class for all annotation types.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox","title":"<code>BoundingBox</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices.</p> <p>The reserved fields <code>width</code>, <code>height</code>, <code>area</code>, and <code>aspect_ratio</code> are automatically populated with values derived from the provided coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox.top_left","title":"<code>top_left: Tuple[float, float]</code>  <code>instance-attribute</code>","text":"<p>The top left vertex (in <code>(x, y)</code> pixel coordinates) of this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox.bottom_right","title":"<code>bottom_right: Tuple[float, float]</code>  <code>instance-attribute</code>","text":"<p>The bottom right vertex (in <code>(x, y)</code> pixel coordinates) of this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox","title":"<code>LabeledBoundingBox</code>","text":"<p>             Bases: <code>BoundingBox</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices and a string label.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox","title":"<code>ScoredBoundingBox</code>","text":"<p>             Bases: <code>BoundingBox</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox","title":"<code>ScoredLabeledBoundingBox</code>","text":"<p>             Bases: <code>BoundingBox</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices, a string label, and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polygon","title":"<code>Polygon</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polygon.points","title":"<code>points: List[Tuple[float, float]]</code>  <code>instance-attribute</code>","text":"<p>The sequence of <code>(x, y)</code> pixel coordinates comprising the boundary of this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledPolygon","title":"<code>LabeledPolygon</code>","text":"<p>             Bases: <code>Polygon</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates and a string label.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledPolygon.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredPolygon","title":"<code>ScoredPolygon</code>","text":"<p>             Bases: <code>Polygon</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredPolygon.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledPolygon","title":"<code>ScoredLabeledPolygon</code>","text":"<p>             Bases: <code>Polygon</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates with a string label and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledPolygon.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledPolygon.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Keypoints","title":"<code>Keypoints</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Array of any number of keypoints specified in pixel coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Keypoints.points","title":"<code>points: List[Tuple[float, float]]</code>  <code>instance-attribute</code>","text":"<p>The sequence of discrete <code>(x, y)</code> pixel coordinates comprising this keypoints annotation.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polyline","title":"<code>Polyline</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Polyline with any number of vertices specified in pixel coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polyline.points","title":"<code>points: List[Tuple[float, float]]</code>  <code>instance-attribute</code>","text":"<p>The sequence of connected <code>(x, y)</code> pixel coordinates comprising this polyline.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D","title":"<code>BoundingBox3D</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Three-dimensional cuboid bounding box in a right-handed coordinate system.</p> <p>Specified by <code>(x, y, z)</code> coordinates for the <code>center</code> of the cuboid, <code>(x, y, z)</code> <code>dimensions</code>, and a <code>rotation</code> parameter specifying the degrees of rotation about each axis <code>(x, y, z)</code> ranging <code>[-\u03c0, \u03c0]</code>.</p> <p>The reserved field <code>volume</code> is automatically derived from the provided <code>dimensions</code>.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D.center","title":"<code>center: Tuple[float, float, float]</code>  <code>instance-attribute</code>","text":"<p><code>(x, y, z)</code> coordinates specifying the center of the bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D.dimensions","title":"<code>dimensions: Tuple[float, float, float]</code>  <code>instance-attribute</code>","text":"<p><code>(x, y, z)</code> measurements specifying the dimensions of the bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D.rotations","title":"<code>rotations: Tuple[float, float, float]</code>  <code>instance-attribute</code>","text":"<p>Rotations in degrees about each <code>(x, y, z)</code> axis.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox3D","title":"<code>LabeledBoundingBox3D</code>","text":"<p>             Bases: <code>BoundingBox3D</code></p> <p><code>BoundingBox3D</code> with an additional string label.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox3D.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox3D","title":"<code>ScoredBoundingBox3D</code>","text":"<p>             Bases: <code>BoundingBox3D</code></p> <p><code>BoundingBox3D</code> with an additional float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox3D.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox3D","title":"<code>ScoredLabeledBoundingBox3D</code>","text":"<p>             Bases: <code>BoundingBox3D</code></p> <p><code>BoundingBox3D</code> with an additional string label and float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox3D.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox3D.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.SegmentationMask","title":"<code>SegmentationMask</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Raster segmentation mask. The <code>locator</code> is the URL to the image file representing the segmentation mask.</p> <p>The segmentation mask must be rendered as a single-channel, 8-bit-depth (grayscale) image. For the best results, use a lossless file format such as PNG. Each pixel's value is the numerical ID of its class label, as specified in the <code>labels</code> map. Any pixel value not present in the <code>labels</code> map is rendered as part of the background.</p> <p>For example, <code>labels = {255: \"object\"}</code> will highlight all pixels with the value of 255 as <code>\"object\"</code>. Every other pixel value will be transparent.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.SegmentationMask.labels","title":"<code>labels: Dict[int, str]</code>  <code>instance-attribute</code>","text":"<p>Mapping of unique label IDs (pixel values) to unique label values.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.SegmentationMask.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL of the segmentation mask image.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BitmapMask","title":"<code>BitmapMask</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Arbitrary bitmap mask. The <code>locator</code> is the URL to the image file representing the mask.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BitmapMask.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL of the bitmap data.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ClassificationLabel","title":"<code>ClassificationLabel</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Label of classification.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ClassificationLabel.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>String label for this classification.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredClassificationLabel","title":"<code>ScoredClassificationLabel</code>","text":"<p>             Bases: <code>ClassificationLabel</code></p> <p>Classification label with accompanying score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredClassificationLabel.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>Score associated with this label.</p>"},{"location":"reference/workflow/asset/","title":"Assets: <code>kolena.workflow.asset</code>","text":"<p>Assets are additional files linked to the <code>TestSample</code>, <code>GroundTruth</code>, or <code>Inference</code> objects for your workflow. Assets can be visualized in the Kolena Studio when exploring your test cases or model results.</p> <p>The following asset types are available:</p> <ul> <li><code>ImageAsset</code></li> <li><code>PlainTextAsset</code></li> <li><code>BinaryAsset</code></li> <li><code>PointCloudAsset</code></li> <li><code>VideoAsset</code></li> </ul>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.Asset","title":"<code>Asset</code>","text":"<p>             Bases: <code>TypedDataObject[_AssetType]</code></p> <p>Base class for all asset types.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.ImageAsset","title":"<code>ImageAsset</code>","text":"<p>             Bases: <code>Asset</code></p> <p>An image in a cloud bucket.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.ImageAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this image in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-image-asset.png</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PlainTextAsset","title":"<code>PlainTextAsset</code>","text":"<p>             Bases: <code>Asset</code></p> <p>A plain text file in a cloud bucket.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PlainTextAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this text file in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-text-asset.txt</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BinaryAsset","title":"<code>BinaryAsset</code>","text":"<p>             Bases: <code>Asset</code></p> <p>A binary file in a cloud bucket.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BinaryAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this text file in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-binary-asset.bin</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PointCloudAsset","title":"<code>PointCloudAsset</code>","text":"<p>             Bases: <code>Asset</code></p> <p>A three-dimensional point cloud located in a cloud bucket. Points are assumed to be specified in a right-handed, Z-up coordinate system with the origin around the sensor that captured the point cloud.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PointCloudAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this point cloud in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-point-cloud.pcd</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BaseVideoAsset","title":"<code>BaseVideoAsset</code>","text":"<p>             Bases: <code>Asset</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BaseVideoAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset","title":"<code>VideoAsset</code>","text":"<p>             Bases: <code>BaseVideoAsset</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.thumbnail","title":"<code>thumbnail: Optional[ImageAsset] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally provide asset locator for custom video thumbnail image.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.start","title":"<code>start: Optional[float] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally specify start time of video snippet, in seconds.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.end","title":"<code>end: Optional[float] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally specify end time of video snippet, in seconds.</p>"},{"location":"reference/workflow/define-workflow/","title":"<code>kolena.workflow.define_workflow</code>","text":""},{"location":"reference/workflow/define-workflow/#kolena.workflow.define_workflow.define_workflow","title":"<code>define_workflow(name, test_sample_type, ground_truth_type, inference_type)</code>","text":"<p>Define a new workflow, specifying its test sample, ground truth, and inference types.</p> <pre><code>from kolena.workflow import define_workflow\nfrom my_code import MyTestSample, MyGroundTruth, MyInference\n_, TestCase, TestSuite, Model = define_workflow(\n\"My Workflow\",\nMyTestSample,   # extends e.g. kolena.workflow.Image (or uses directly)\nMyGroundTruth,  # extends kolena.workflow.GroundTruth\nMyInference,    # extends kolena.workflow.Inference\n)\n</code></pre> <p><code>define_workflow</code> is provided as a convenience method to create the <code>TestCase</code>, <code>TestSuite</code>, and <code>Model</code> objects for a new workflow. These objects can also be defined manually by subclassing them and binding the <code>workflow</code> class variable:</p> <pre><code>from kolena.workflow import TestCase\nfrom my_code import my_workflow\nclass MyTestCase(TestCase):\nworkflow = my_workflow\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the workflow.</p> required <code>test_sample_type</code> <code>Type[TestSample]</code> <p>The type of the <code>TestSample</code> for this workflow.</p> required <code>ground_truth_type</code> <code>Type[GroundTruth]</code> <p>The type of the <code>GroundTruth</code> for this workflow.</p> required <code>inference_type</code> <code>Type[Inference]</code> <p>The type of the <code>Inference</code> for this workflow.</p> required <p>Returns:</p> Type Description <code>Tuple[Workflow, Type[TestCase], Type[TestSuite], Type[Model]]</code> <p>The <code>Workflow</code> object for this workflow along with the <code>TestCase</code>, <code>TestSuite</code>, and <code>Model</code> objects to use when creating and running tests for this workflow.</p>"},{"location":"reference/workflow/evaluator/","title":"<code>kolena.workflow.Evaluator</code>","text":"<p>Simplified interface for <code>Evaluator</code> implementations.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestSample","title":"<code>MetricsTestSample</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Test-sample-level metrics produced by an <code>Evaluator</code>.</p> <p>This class should be subclassed with the relevant fields for a given workflow.</p> <p>Examples here may include the number of true positive detections on an image, the mean IOU of inferred polygon(s) with ground truth polygon(s), etc.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestCase","title":"<code>MetricsTestCase</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Test-case-level metrics produced by an <code>Evaluator</code>.</p> <p>This class should be subclassed with the relevant fields for a given workflow.</p> <p>Test-case-level metrics are aggregate metrics like <code>precision</code>, <code>recall</code>, and <code>f1_score</code>. Any and all aggregate metrics that fit a workflow should be defined here.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestCase--nesting-aggregate-metrics","title":"Nesting Aggregate Metrics","text":"<p><code>MetricsTestCase</code> supports nesting metrics objects, for e.g. reporting class-level metrics within a test case that contains multiple classes. Example usage:</p> <pre><code>@dataclass(frozen=True)\nclass PerClassMetrics(MetricsTestCase):\nClass: str\nPrecision: float\nRecall: float\nF1: float\nAP: float\n@dataclass(frozen=True)\nclass TestCaseMetrics(MetricsTestCase):\nmacro_Precision: float\nmacro_Recall: float\nmacro_F1: float\nmAP: float\nPerClass: List[PerClassMetrics]\n</code></pre> <p>Any <code>str</code>-type fields (e.g. <code>Class</code> in the above example) will be used as identifiers when displaying nested metrics on Kolena. For best results, include at least one <code>str</code>-type field in nested metrics definitions.</p> <p>When comparing nested metrics from multiple models, an <code>int</code>-type column with any of the following names will be used for sample size in statistical significance calculations: <code>N</code>, <code>n</code>, <code>nTestSamples</code>, <code>n_test_samples</code>, <code>sampleSize</code>, <code>sample_size</code>, <code>SampleSize</code>.</p> <p>For a detailed overview of this feature, see the   Nesting Test Case Metrics advanced usage guide.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestSuite","title":"<code>MetricsTestSuite</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Test-suite-level metrics produced by an <code>Evaluator</code>.</p> <p>This class should be subclassed with the relevant fields for a given workflow.</p> <p>Test-suite-level metrics typically measure performance across test cases, e.g. penalizing variance across different subsets of a benchmark.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.EvaluatorConfiguration","title":"<code>EvaluatorConfiguration</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Configuration for an <code>Evaluator</code>.</p> <p>Example evaluator configurations may specify:</p> <ul> <li>Fixed confidence thresholds at which detections are discarded.</li> <li>Different algorithms/strategies used to compute confidence thresholds     (e.g. \"accuracy optimal\" for a classification-type workflow).</li> </ul>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.EvaluatorConfiguration.display_name","title":"<code>display_name()</code>  <code>abstractmethod</code>","text":"<p>The name to display for this configuration in Kolena. Must be implemented when extending <code>EvaluatorConfiguration</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator","title":"<code>Evaluator(configurations=None)</code>","text":"<p>An <code>Evaluator</code> transforms inferences into metrics.</p> <p>Metrics are computed at the individual test sample level (<code>MetricsTestSample</code>), in aggregate at the test case level (<code>MetricsTestCase</code>), and across populations at the test suite level (<code>MetricsTestSuite</code>).</p> <p>Test-case-level plots (<code>Plot</code>) may also be computed.</p> <p>Parameters:</p> Name Type Description Default <code>configurations</code> <code>Optional[List[EvaluatorConfiguration]]</code> <p>The configurations at which to perform evaluation. Instance methods such as <code>compute_test_sample_metrics</code> are called once per test case per configuration.</p> <code>None</code>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.configurations","title":"<code>configurations: List[EvaluatorConfiguration] = configurations or []</code>  <code>instance-attribute</code>","text":"<p>The configurations with which to perform evaluation, provided on instantiation.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.display_name","title":"<code>display_name()</code>","text":"<p>The name to display for this evaluator in Kolena. Defaults to the name of this class.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_sample_metrics","title":"<code>compute_test_sample_metrics(test_case, inferences, configuration=None)</code>  <code>abstractmethod</code>","text":"<p>Compute metrics for every test sample in a test case, i.e. one <code>MetricsTestSample</code> object for each of the provided test samples.</p> <p>Must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The <code>TestCase</code> to which the provided test samples and ground truths belong.</p> required <code>inferences</code> <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>The test samples, ground truths, and inferences for all entries in a test case.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>The evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[TestSample, MetricsTestSample]]</code> <p><code>TestSample</code>-level metrics for each provided test sample.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_case_metrics","title":"<code>compute_test_case_metrics(test_case, inferences, metrics, configuration=None)</code>  <code>abstractmethod</code>","text":"<p>Compute aggregate metrics (<code>MetricsTestCase</code>) across a test case.</p> <p>Must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case in question.</p> required <code>inferences</code> <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>The test samples, ground truths, and inferences for all entries in a test case.</p> required <code>metrics</code> <code>List[MetricsTestSample]</code> <p>The <code>TestSample</code>-level metrics computed by <code>compute_test_sample_metrics</code>, provided in the same order as <code>inferences</code>.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>The evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>MetricsTestCase</code> <p><code>TestCase</code>-level metrics for the provided test case.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_case_plots","title":"<code>compute_test_case_plots(test_case, inferences, metrics, configuration=None)</code>","text":"<p>Optionally compute any number of plots to visualize the results for a test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case in question</p> required <code>inferences</code> <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>The test samples, ground truths, and inferences for all entries in a test case.</p> required <code>metrics</code> <code>List[MetricsTestSample]</code> <p>The <code>TestSample</code>-level metrics computed by <code>compute_test_sample_metrics</code>, provided in the same order as <code>inferences</code>.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>the evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[List[Plot]]</code> <p>Zero or more plots for this test case at this configuration.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_suite_metrics","title":"<code>compute_test_suite_metrics(test_suite, metrics, configuration=None)</code>","text":"<p>Optionally compute <code>TestSuite</code>-level metrics (<code>MetricsTestSuite</code>) across the provided <code>test_suite</code>.</p> <p>Parameters:</p> Name Type Description Default <code>test_suite</code> <code>TestSuite</code> <p>The test suite in question.</p> required <code>metrics</code> <code>List[Tuple[TestCase, MetricsTestCase]]</code> <p>The <code>TestCase</code>-level metrics computed by <code>compute_test_case_metrics</code>.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>The evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[MetricsTestSuite]</code> <p>The <code>TestSuite</code>-level metrics for this test suite.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.BasicEvaluatorFunction","title":"<code>BasicEvaluatorFunction = Union[ConfiguredEvaluatorFunction, UnconfiguredEvaluatorFunction]</code>  <code>module-attribute</code>","text":"<p><code>BasicEvaluatorFunction</code> provides a function-based evaluator interface that takes the inferences for all test samples in a test suite and a <code>TestCases</code> as input and computes the corresponding test-sample-level, test-case-level, and test-suite-level metrics (and optionally plots) as output.</p> <p>Example implementation, relying on <code>compute_per_sample</code> and <code>compute_aggregate</code> functions implemented elsewhere:</p> <pre><code>def evaluate(\ntest_samples: List[TestSample],\nground_truths: List[GroundTruth],\ninferences: List[Inference],\ntest_cases: TestCases,\n# configuration: EvaluatorConfiguration,  # uncomment when configuration is used\n) -&gt; EvaluationResults:\n# compute per-sample metrics for each test sample\nper_sample_metrics = [compute_per_sample(gt, inf) for gt, inf in zip(ground_truths, inferences)]\n# compute aggregate metrics across all test cases using `test_cases.iter(...)`\naggregate_metrics: List[Tuple[TestCase, MetricsTestCase]] = []\nfor test_case, *s in test_cases.iter(test_samples, ground_truths, inferences, per_sample_metrics):\n# subset of `test_samples`/`ground_truths`/`inferences`/`test_sample_metrics` in given test case\ntc_test_samples, tc_ground_truths, tc_inferences, tc_per_sample_metrics = s\naggregate_metrics.append((test_case, compute_aggregate(tc_per_sample_metrics)))\n# if desired, compute and add `plots_test_case` and `metrics_test_suite`\nreturn EvaluationResults(\nmetrics_test_sample=list(zip(test_samples, per_sample_metrics)),\nmetrics_test_case=aggregate_metrics,\n)\n</code></pre> <p>The control flow is in general more streamlined than with <code>Evaluator</code>, but requires a couple of assumptions to hold:</p> <ul> <li>Test-sample-level metrics do not vary by test case</li> <li>Ground truths corresponding to a given test sample do not vary by test case</li> </ul> <p>This <code>BasicEvaluatorFunction</code> is provided to the test run at runtime, and is expected to have the following signature:</p> <p>Parameters:</p> Name Type Description Default <code>test_samples</code> <code>List[TestSample]</code> <p>A list of distinct <code>TestSample</code> values that correspond to all test samples in the test run.</p> required <code>ground_truths</code> <code>List[GroundTruth]</code> <p>A list of <code>GroundTruth</code> values corresponding to and sequenced in the same order as <code>test_samples</code>.</p> required <code>inferences</code> <code>List[Inference]</code> <p>A list of <code>Inference</code> values corresponding to and sequenced in the same order as <code>test_samples</code>.</p> required <code>test_cases</code> <code>TestCases</code> <p>An instance of <code>TestCases</code>, used to provide iteration groupings for evaluating test-case-level metrics.</p> required <code>evaluator_configuration</code> <code>EvaluatorConfiguration</code> <p>The <code>EvaluatorConfiguration</code> to use when performing the evaluation. This parameter may be omitted in the function definition for implementations that do not use any configuration object.</p> required <p>Returns:</p> Type Description <code>EvaluationResults</code> <p>An <code>EvaluationResults</code> object tracking the test-sample-level, test-case-level and test-suite-level metrics and plots for the input collection of test samples.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.TestCases","title":"<code>TestCases</code>","text":"<p>Provides an iterator method for grouping test-sample-level metric results with the test cases that they belong to.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.TestCases.iter","title":"<code>iter(test_samples, ground_truths, inferences, metrics_test_sample)</code>  <code>abstractmethod</code>","text":"<p>Matches test sample metrics to the corresponding test cases that they belong to.</p> <p>Parameters:</p> Name Type Description Default <code>test_samples</code> <code>List[TestSample]</code> <p>All unique test samples within the test run, sequenced in the same order as the other parameters.</p> required <code>ground_truths</code> <code>List[GroundTruth]</code> <p>Ground truths corresponding to <code>test_samples</code>, sequenced in the same order.</p> required <code>inferences</code> <code>List[Inference]</code> <p>Inferences corresponding to <code>test_samples</code>, sequenced in the same order.</p> required <code>metrics_test_sample</code> <code>List[MetricsTestSample]</code> <p>Test-sample-level metrics corresponding to <code>test_samples</code>, sequenced in the same order.</p> required <p>Returns:</p> Type Description <code>Iterator[Tuple[TestCase, List[TestSample], List[GroundTruth], List[Inference], List[MetricsTestSample]]]</code> <p>Iterator that groups each test case in the test run to the lists of member test samples, inferences, and test-sample-level metrics.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults","title":"<code>EvaluationResults</code>","text":"<p>A bundle of metrics computed for a test run grouped at the test-sample-level, test-case-level, and test-suite-level. Optionally includes <code>Plot</code>s at the test-case-level.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.metrics_test_sample","title":"<code>metrics_test_sample: List[Tuple[BaseTestSample, BaseMetricsTestSample]]</code>  <code>instance-attribute</code>","text":"<p>Sample-level metrics, extending <code>MetricsTestSample</code>, for every provided test sample.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.metrics_test_case","title":"<code>metrics_test_case: List[Tuple[TestCase, MetricsTestCase]]</code>  <code>instance-attribute</code>","text":"<p>Aggregate metrics, extending <code>MetricsTestCase</code>, computed across each test case yielded from <code>TestCases.iter</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.plots_test_case","title":"<code>plots_test_case: List[Tuple[TestCase, List[Plot]]] = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional test-case-level plots.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.metrics_test_suite","title":"<code>metrics_test_suite: Optional[MetricsTestSuite] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional test-suite-level metrics, extending <code>MetricsTestSuite</code>.</p>"},{"location":"reference/workflow/ground-truth/","title":"<code>kolena.workflow.GroundTruth</code>","text":"<p>The ground truth associated with a <code>TestSample</code>. Typically, a ground truth will represent the expected output of a model when given a test sample and will be manually annotated by a human.</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\nfrom kolena.workflow import GroundTruth\nfrom kolena.workflow.annotation import Polyline, SegmentationMask\n@dataclass(frozen=True)\nclass AvGroundTruth(GroundTruth):\nroad_area: SegmentationMask\nlane_boundaries: List[Polyline]\nvisibility_score: int\n</code></pre> <p>A <code>TestCase</code> holds a list of test samples (model inputs) paired with ground truths (expected outputs).</p>"},{"location":"reference/workflow/ground-truth/#kolena.workflow.ground_truth.GroundTruth","title":"<code>GroundTruth</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>The ground truth against which a model is evaluated.</p> <p>A test case contains one or more <code>TestSample</code> objects each paired with a ground truth object. During evaluation, these test samples, ground truths, and your model's inferences are provided to the <code>Evaluator</code> implementation.</p> <p>This object may contain any combination of scalars (e.g. <code>str</code>, <code>float</code>), <code>Annotation</code> objects, or lists of these objects.</p> <p>For <code>Composite</code>, each object can contain multiple basic test sample elements. To associate a set of attributes and/or annotations as the ground truth to a target test sample element, declare annotations by extending <code>DataObject</code> and use the same attribute name as used in the <code>Composite</code> test sample.</p> <p>Continue with the example given in <code>Composite</code>, where the <code>FacePairSample</code> test sample type is defined using a pair of images under the <code>source</code> and <code>target</code> members, we can design a corresponding ground truth type with image-level annotations defined in the <code>FaceRegion</code> object:</p> <pre><code>from dataclasses import dataclass\nfrom kolena.workflow import DataObject, GroundTruth\nfrom kolena.workflow.annotation import BoundingBox, Keypoints\n@dataclass(frozen=True)\nclass FaceRegion(DataObject):\nbounding_box: BoundingBox\nkeypoints: Keypoints\n@dataclass(frozen=True)\nclass FacePair(GroundTruth):\nsource: FaceRegion\ntarget: FaceRegion\nis_same_person: bool\n</code></pre> <p>This way, it is clear which bounding boxes and keypoints are associated to which image in the test sample.</p>"},{"location":"reference/workflow/inference/","title":"<code>kolena.workflow.Inference</code>","text":"<p>The output from a <code>Model</code>. In other words, a model is a deterministic transformation from a <code>TestSample</code> to an <code>Inference</code>.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Optional\nfrom kolena.workflow import Inference\nfrom kolena.workflow.annotation import Keypoints\n@dataclass(frozen=True)\nclass PoseEstimate(Inference):\nskeleton: Optional[Keypoints] = None  # leave empty if nothing is detected\nconfidence: Optional[float] = None\n</code></pre>"},{"location":"reference/workflow/inference/#kolena.workflow.inference.Inference","title":"<code>Inference</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>The inference produced by a model.</p> <p>Typically the structure of this object closely mirrors the structure of the <code>GroundTruth</code> for a workflow, but this is not a requirement.</p> <p>During evaluation, the <code>TestSample</code> objects, ground truth objects, and these inference objects are provided to the <code>Evaluator</code> implementation to compute metrics.</p> <p>This object may contain any combination of scalars (e.g. <code>str</code>, <code>float</code>), <code>Annotation</code> objects, or lists of these objects.</p> <p>A model processing a <code>Composite</code> test sample can produce an inference result for each of its elements. To associate an inference result to each test sample element, put the attributes and/or annotations inside a <code>DataObject</code> and use the same attribute name as that used in the <code>Composite</code> test sample.</p> <p>Continue with the example given in <code>Composite</code>, where the <code>FacePairSample</code> test sample type is defined using a pair of images under the <code>source</code> and <code>target</code> members, we can design a corresponding inference type with image-level annotations defined in the <code>FaceRegion</code> object:</p> <pre><code>from dataclasses import dataclass\nfrom kolena.workflow import DataObject, Inference\nfrom kolena.workflow.annotation import BoundingBox, Keypoints\n@dataclass(frozen=True)\nclass FaceRegion(DataObject):\nbounding_box: BoundingBox\nkeypoints: Keypoints\n@dataclass(frozen=True)\nclass FacePair(Inference):\nsource: FaceRegion\ntarget: FaceRegion\nsimilarity: float\n</code></pre> <p>This way, it is clear which bounding boxes and keypoints are associated to which image in the test sample.</p>"},{"location":"reference/workflow/metrics/","title":"<code>kolena.workflow.metrics</code>","text":""},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.InferenceMatches","title":"<code>InferenceMatches</code>","text":"<p>             Bases: <code>Generic[GT, Inf]</code></p> <p>The result of <code>match_inferences</code>, providing lists of matches between ground truth and inference objects, unmatched ground truths, and unmatched inferences. After applying some confidence threshold on returned inference objects, <code>InferenceMatches</code> can be used to calculate metrics such as precision and recall.</p> <p>Objects are of type <code>BoundingBox</code> or <code>Polygon</code>, depending on the type of inputs provided to <code>match_inferences</code>.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.InferenceMatches.matched","title":"<code>matched: List[Tuple[GT, Inf]]</code>  <code>instance-attribute</code>","text":"<p>Pairs of matched ground truth and inference objects above the IoU threshold. Considered as true positive detections after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.InferenceMatches.unmatched_gt","title":"<code>unmatched_gt: List[GT]</code>  <code>instance-attribute</code>","text":"<p>Unmatched ground truth objects. Considered as false negatives.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.InferenceMatches.unmatched_inf","title":"<code>unmatched_inf: List[Inf]</code>  <code>instance-attribute</code>","text":"<p>Unmatched inference objects. Considered as false positives after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.MulticlassInferenceMatches","title":"<code>MulticlassInferenceMatches</code>","text":"<p>             Bases: <code>Generic[GT_Multiclass, Inf_Multiclass]</code></p> <p>The result of <code>match_inferences_multiclass</code>, providing lists of matches between ground truth and inference objects, unmatched ground truths, and unmatched inferences.</p> <p>Unmatched ground truths may be matched with an inference of a different class when no inference of its own class is suitable, i.e. a \"confused\" match. <code>MultiClassInferenceMatches</code> can be used to calculate metrics such as precision and recall per class, after applying some confidence threshold on the returned inference objects.</p> <p>Objects are of type <code>LabeledBoundingBox</code> or <code>LabeledPolygon</code>, depending on the type of inputs provided to <code>match_inferences_multiclass</code>.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.MulticlassInferenceMatches.matched","title":"<code>matched: List[Tuple[GT_Multiclass, Inf_Multiclass]]</code>  <code>instance-attribute</code>","text":"<p>Pairs of matched ground truth and inference objects above the IoU threshold. Considered as true positive detections after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.MulticlassInferenceMatches.unmatched_gt","title":"<code>unmatched_gt: List[Tuple[GT_Multiclass, Optional[Inf_Multiclass]]]</code>  <code>instance-attribute</code>","text":"<p>Pairs of unmatched ground truth objects with its confused inference object (i.e. IoU above threshold with mismatching <code>label</code>), if such an inference exists. Considered as false negatives and \"confused\" detections.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.MulticlassInferenceMatches.unmatched_inf","title":"<code>unmatched_inf: List[Inf_Multiclass]</code>  <code>instance-attribute</code>","text":"<p>Unmatched inference objects. Considered as false positives after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.accuracy","title":"<code>accuracy(true_positives, false_positives, false_negatives, true_negatives)</code>","text":"<p>Accuracy represents the proportion of inferences that are correct (including both positives and negatives).</p> \\[ \\text{Accuracy} = \\frac{\\text{# TP} + \\text{# TN}} {\\text{# TP} + \\text{# FP} + \\text{# FN} + \\text{# TN}} \\] <ul> <li>  Metrics Glossary: Accuracy \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>int</code> <p>Number of true positive inferences.</p> required <code>false_positives</code> <code>int</code> <p>Number of false positive inferences.</p> required <code>false_negatives</code> <code>int</code> <p>Number of false negatives.</p> required <code>true_negatives</code> <code>int</code> <p>Number of true negatives.</p> required"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.precision","title":"<code>precision(true_positives, false_positives)</code>","text":"<p>Precision represents the proportion of inferences that are correct.</p> \\[ \\text{Precision} = \\frac{\\text{# True Positives}}{\\text{# True Positives} + \\text{# False Positives}} \\] <ul> <li>  Metrics Glossary: Precision \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>int</code> <p>Number of true positive inferences.</p> required <code>false_positives</code> <code>int</code> <p>Number of false positive inferences.</p> required"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.recall","title":"<code>recall(true_positives, false_negatives)</code>","text":"<p>Recall represents the proportion of ground truths that were successfully predicted.</p> \\[ \\text{Recall} = \\frac{\\text{# True Positives}}{\\text{# True Positives} + \\text{# False Negatives}} \\] <ul> <li>  Metrics Glossary: Recall \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>int</code> <p>Number of true positive inferences.</p> required <code>false_negatives</code> <code>int</code> <p>Number of false negatives.</p> required"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.iou","title":"<code>iou(a, b)</code>","text":"<p>Compute the Intersection Over Union (IoU) of two geometries.</p> <ul> <li>  Metrics Glossary: Intersection over Union (IoU) \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[BoundingBox, Polygon]</code> <p>The first geometry in computation.</p> required <code>b</code> <code>Union[BoundingBox, Polygon]</code> <p>The second geometry in computation.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The value of the IoU between geometries <code>a</code> and <code>b</code>.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.f1_score","title":"<code>f1_score(true_positives, false_positives, false_negatives)</code>","text":"<p>F<sub>1</sub>-score is the harmonic mean between <code>precision</code> and <code>recall</code>.</p> \\[ \\begin{align} \\text{F1} &amp;= \\frac{2}{\\frac{1}{\\text{Precision}} + \\frac{1}{\\text{Recall}}} \\\\[1em] &amp;= 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\end{align} \\] <ul> <li>  Metrics Glossary: F<sub>1</sub>-score \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>int</code> <p>Number of true positive inferences.</p> required <code>false_positives</code> <code>int</code> <p>Number of false positive inferences.</p> required <code>false_negatives</code> <code>int</code> <p>Number of false negatives.</p> required"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.match_inferences","title":"<code>match_inferences(ground_truths, inferences, *, ignored_ground_truths=None, mode='pascal', iou_threshold=0.5)</code>","text":"<p>Matches model inferences with annotated ground truths using the provided configuration.</p> <p>This matcher does not consider labels, which is appropriate for single class object matching. To match with multiple classes (i.e. heeding <code>label</code> classifications), use the multiclass matcher <code>match_inferences_multiclass</code>.</p> <p>Available modes:</p> <ul> <li><code>pascal</code> (PASCAL VOC): For every inference by order of highest confidence, the ground truth of highest IoU is   its match. Multiple inferences are able to match with the same ignored ground truth. See the   PASCAL VOC paper for more information.</li> </ul> <ul> <li>  Metrics Glossary: Geometry Matching \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[Geometry]</code> <p>A list of <code>BoundingBox</code> or <code>Polygon</code> ground truths.</p> required <code>inferences</code> <code>List[ScoredGeometry]</code> <p>A list of <code>ScoredBoundingBox</code> or <code>ScoredPolygon</code> inferences.</p> required <code>ignored_ground_truths</code> <code>Optional[List[Geometry]]</code> <p>Optionally specify a list of <code>BoundingBox</code> or <code>Polygon</code> ground truths to ignore. These ignored ground truths and any inferences matched with them are omitted from the returned <code>InferenceMatches</code>.</p> <code>None</code> <code>mode</code> <code>Literal['pascal']</code> <p>The matching methodology to use. See available modes above.</p> <code>'pascal'</code> <code>iou_threshold</code> <code>float</code> <p>The IoU (intersection over union, see <code>iou</code>) threshold for valid matches.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>InferenceMatches[GT, Inf]</code> <p><code>InferenceMatches</code> containing the matches (true positives), unmatched ground truths (false negatives) and unmatched inferences (false positives).</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.match_inferences_multiclass","title":"<code>match_inferences_multiclass(ground_truths, inferences, *, ignored_ground_truths=None, mode='pascal', iou_threshold=0.5)</code>","text":"<p>Matches model inferences with annotated ground truths using the provided configuration.</p> <p>This matcher considers <code>label</code> values matching per class. After matching inferences and ground truths with equivalent <code>label</code> values, unmatched inferences and unmatched ground truths are matched once more to identify confused matches, where localization succeeded (i.e. IoU above <code>iou_threshold</code>) but classification failed (i.e. mismatching <code>label</code> values).</p> <p>Available modes:</p> <ul> <li><code>pascal</code> (PASCAL VOC): For every inference by order of highest confidence, the ground truth of highest IoU is   its match. Multiple inferences are able to match with the same ignored ground truth. See the   PASCAL VOC paper for more information.</li> </ul> <ul> <li>  Metrics Glossary: Geometry Matching \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[LabeledGeometry]</code> <p>A list of <code>LabeledBoundingBox</code> or <code>LabeledPolygon</code> ground truths.</p> required <code>inferences</code> <code>List[ScoredLabeledGeometry]</code> <p>A list of <code>ScoredLabeledBoundingBox</code> or <code>ScoredLabeledPolygon</code> inferences.</p> required <code>ignored_ground_truths</code> <code>Optional[List[LabeledGeometry]]</code> <p>Optionally specify a list of <code>LabeledBoundingBox</code> or <code>LabeledPolygon</code> ground truths to ignore. These ignored ground truths and any inferences matched with them are omitted from the returned <code>MulticlassInferenceMatches</code>.</p> <code>None</code> <code>mode</code> <code>Literal['pascal']</code> <p>The matching methodology to use. See available modes above.</p> <code>'pascal'</code> <code>iou_threshold</code> <code>float</code> <p>The IoU threshold cutoff for valid matches.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>MulticlassInferenceMatches[GT_Multiclass, Inf_Multiclass]</code> <p><code>MulticlassInferenceMatches</code> containing the matches (true positives), unmatched ground truths (false negatives), and unmatched inferences (false positives).</p>"},{"location":"reference/workflow/model/","title":"<code>kolena.worfklow.Model</code>","text":""},{"location":"reference/workflow/model/#kolena.workflow.model.Model","title":"<code>Model(name, infer=None, metadata=None)</code>","text":"<p>             Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>The descriptor of a model tested on Kolena. A model is a deterministic transformation from <code>TestSample</code> inputs to <code>Inference</code> outputs.</p> <p>Rather than importing this class directly, use the <code>Model</code> type definition returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.workflow","title":"<code>workflow: Workflow</code>  <code>instance-attribute</code>","text":"<p>The workflow of this model. Automatically populated when constructing via the model type returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Unique name of the model.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.metadata","title":"<code>metadata: Dict[str, Any]</code>  <code>instance-attribute</code>","text":"<p>Unstructured metadata associated with the model.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.infer","title":"<code>infer: Optional[Callable[[TestSample], Inference]]</code>  <code>instance-attribute</code>","text":"<p>Function transforming a <code>TestSample</code> for a workflow into an <code>Inference</code> object. Required when using <code>test</code> or <code>TestRun.run</code>.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.create","title":"<code>create(name, infer=None, metadata=None)</code>  <code>classmethod</code>","text":"<p>Create a new model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the new model to create.</p> required <code>infer</code> <code>Optional[Callable[[TestSample], Inference]]</code> <p>Optional inference function for this model.</p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Optional unstructured metadata to store with this model.</p> <code>None</code> <p>Returns:</p> Type Description <code>Model</code> <p>The newly created model.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.load","title":"<code>load(name, infer=None)</code>  <code>classmethod</code>","text":"<p>Load an existing model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the model to load.</p> required <code>infer</code> <code>Optional[Callable[[TestSample], Inference]]</code> <p>Optional inference function for this model.</p> <code>None</code>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.load_inferences","title":"<code>load_inferences(test_case)</code>","text":"<p>Load all inferences stored for this model on the provided test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case for which to load inferences.</p> required <p>Returns:</p> Type Description <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>The ground truths and inferences for all test samples in the test case.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.iter_inferences","title":"<code>iter_inferences(test_case)</code>","text":"<p>Iterate over all inferences stored for this model on the provided test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case over which to iterate inferences.</p> required <p>Returns:</p> Type Description <code>Iterator[Tuple[TestSample, GroundTruth, Inference]]</code> <p>Iterator exposing the ground truths and inferences for all test samples in the test case.</p>"},{"location":"reference/workflow/plot/","title":"Plots: <code>kolena.workflow.plot</code>","text":"<p>This module surfaces plot definitions to visualize test-case-level data. Evaluator implementations can optionally compute plots using these definitions for visualization on the   Results page.</p> <p>The following plot types are available:</p> <ul> <li><code>CurvePlot</code></li> <li><code>Histogram</code></li> <li><code>BarPlot</code></li> <li><code>ConfusionMatrix</code></li> </ul>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.NumberSeries","title":"<code>NumberSeries = Sequence[Union[float, int]]</code>  <code>module-attribute</code>","text":"<p>A sequence of numeric values.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.NullableNumberSeries","title":"<code>NullableNumberSeries = Sequence[Union[float, int, None]]</code>  <code>module-attribute</code>","text":"<p>A sequence of numeric values or <code>None</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.AxisConfig","title":"<code>AxisConfig</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Configuration for the format of a given axis on a plot.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.AxisConfig.type","title":"<code>type: Literal[linear, log]</code>  <code>instance-attribute</code>","text":"<p>Type of axis to display. Supported options are <code>linear</code> and <code>log</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Plot","title":"<code>Plot</code>","text":"<p>             Bases: <code>TypedDataObject[_PlotType]</code></p> <p>A data visualization shown when exploring model results in the web platform.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Curve","title":"<code>Curve</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>A single series on a <code>CurvePlot</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Curve.x","title":"<code>x: NumberSeries</code>  <code>instance-attribute</code>","text":"<p>The <code>x</code> coordinates of this curve. Length must match the provided <code>y</code> coordinates.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Curve.y","title":"<code>y: NumberSeries</code>  <code>instance-attribute</code>","text":"<p>The <code>y</code> coordinates of this curve. Length must match the provided <code>x</code> coordinates.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Curve.label","title":"<code>label: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally specify an additional label (in addition to the associated test case) to apply to this curve, for use when e.g. there are multiple curves generated per test case.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot","title":"<code>CurvePlot</code>","text":"<p>             Bases: <code>Plot</code></p> <p>A plot visualizing one or more curves per test case.</p> <p>Examples include Receiver Operating Characteristic (ROC) curves, Precision versus Recall (PR) curves, Detection-Error Tradeoff (DET) curves, etc.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The title for the plot.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.x_label","title":"<code>x_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>x</code> axis.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.y_label","title":"<code>y_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>y</code> axis.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.curves","title":"<code>curves: List[Curve]</code>  <code>instance-attribute</code>","text":"<p>A test case may generate zero or more curves on a given plot. However, under most circumstances, a single curve per test case is desirable.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.x_config","title":"<code>x_config: Optional[AxisConfig] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>x</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.y_config","title":"<code>y_config: Optional[AxisConfig] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>y</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram","title":"<code>Histogram</code>","text":"<p>             Bases: <code>Plot</code></p> <p>A plot visualizing distribution of one or more continuous values, e.g. distribution of an error metric across all samples within a test case.</p> <p>For visualization of discrete values, see <code>BarPlot</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The title for the plot.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.x_label","title":"<code>x_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>x</code> axis.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.y_label","title":"<code>y_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>y</code> axis.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.buckets","title":"<code>buckets: NumberSeries</code>  <code>instance-attribute</code>","text":"<p>A Histogram requires intervals to bucket the data. For <code>n</code> buckets, <code>n+1</code> consecutive bounds must be specified in increasing order.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.frequency","title":"<code>frequency: Union[NumberSeries, Sequence[NumberSeries]]</code>  <code>instance-attribute</code>","text":"<p>For <code>n</code> buckets, there are <code>n</code> frequencies corresponding to the height of each bucket. The frequency at index <code>i</code> corresponds to the bucket with bounds (<code>i</code>, <code>i+1</code>) in <code>buckets</code>.</p> <p>To specify multiple distributions for a given test case, multiple frequency series can be provided, corresponding e.g. to the distribution for a given class within a test case, with name specified in <code>labels</code>.</p> <p>Specify a list of labels corresponding to the different <code>frequency</code> series when multiple series are provided. Can be omitted when a single <code>frequency</code> series is provided.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.labels","title":"<code>labels: Optional[List[str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Specify the label corresponding to a given distribution when multiple are specified in <code>frequency</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.x_config","title":"<code>x_config: Optional[AxisConfig] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>x</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.y_config","title":"<code>y_config: Optional[AxisConfig] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>y</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot","title":"<code>BarPlot</code>","text":"<p>             Bases: <code>Plot</code></p> <p>A plot visualizing a set of bars per test case.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The plot title.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.x_label","title":"<code>x_label: str</code>  <code>instance-attribute</code>","text":"<p>Axis label for the axis along which the bars are laid out (<code>labels</code>).</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.y_label","title":"<code>y_label: str</code>  <code>instance-attribute</code>","text":"<p>Axis label for the axis corresponding to bar height (<code>values</code>).</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.labels","title":"<code>labels: Sequence[Union[str, int, float]]</code>  <code>instance-attribute</code>","text":"<p>Labels for each bar with corresponding height specified in <code>values</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.values","title":"<code>values: NullableNumberSeries</code>  <code>instance-attribute</code>","text":"<p>Values for each bar with corresponding label specified in <code>labels</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.config","title":"<code>config: Optional[AxisConfig] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom format options to allow for control over the display of the numerical plot axis (<code>values</code>).</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix","title":"<code>ConfusionMatrix</code>","text":"<p>             Bases: <code>Plot</code></p> <p>A confusion matrix. Example:</p> <pre><code>ConfusionMatrix(\ntitle=\"Cat and Dog Confusion\",\nlabels=[\"Cat\", \"Dog\"],\nmatrix=[[90, 10], [5, 95]],\n)\n</code></pre> <p>Yields a confusion matrix of the form:</p> <pre><code>            Predicted\n\n            Cat   Dog\n           +----+----+\n       Cat | 90 | 10 |\nActual     +----+----+\n       Dog |  5 | 95 |\n           +----+----+\n</code></pre>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The plot title.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.labels","title":"<code>labels: List[str]</code>  <code>instance-attribute</code>","text":"<p>The labels corresponding to each entry in the square <code>matrix</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.matrix","title":"<code>matrix: Sequence[NullableNumberSeries]</code>  <code>instance-attribute</code>","text":"<p>A square matrix, typically representing the number of matches between class <code>i</code> and class <code>j</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.x_label","title":"<code>x_label: str = 'Predicted'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The label for the <code>x</code> axis of the confusion matrix.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.y_label","title":"<code>y_label: str = 'Actual'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The label for the <code>y</code> axis of the confusion matrix.</p>"},{"location":"reference/workflow/test-case/","title":"<code>kolena.workflow.TestCase</code>","text":""},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase","title":"<code>TestCase(name, version=None, description=None, test_samples=None, reset=False)</code>","text":"<p>             Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A test case holds a list of test samples paired with ground truths representing a testing dataset or a slice of a testing dataset.</p> <p>Rather than importing this class directly, use the <code>TestCase</code> type definition returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.workflow","title":"<code>workflow: Workflow</code>  <code>instance-attribute</code>","text":"<p>The workflow of this test case. Automatically populated when constructing via test case type returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>The unique name of this test case. Cannot be changed after creation.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.version","title":"<code>version: int</code>  <code>instance-attribute</code>","text":"<p>The version of this test case. A test case's version is automatically incremented whenever it is edited via <code>TestCase.edit</code>.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Free-form, human-readable description of this test case. Can be edited at any time via <code>TestCase.edit</code>.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.Editor","title":"<code>Editor(description, reset)</code>","text":""},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.Editor.description","title":"<code>description(description)</code>","text":"<p>Update the description of the test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.Editor.add","title":"<code>add(test_sample, ground_truth)</code>","text":"<p>Add a test sample to the test case. When the test sample already exists in the test case, its ground truth is overwritten with the ground truth provided here.</p> <p>Parameters:</p> Name Type Description Default <code>test_sample</code> <code>TestSample</code> <p>The test sample to add.</p> required <code>ground_truth</code> <code>GroundTruth</code> <p>The ground truth for the test sample.</p> required"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.Editor.remove","title":"<code>remove(test_sample)</code>","text":"<p>Remove a test sample from the test case. Does nothing if the test sample is not in the test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_sample</code> <code>TestSample</code> <p>The test sample to remove.</p> required"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.create","title":"<code>create(name, description=None, test_samples=None)</code>  <code>classmethod</code>","text":"<p>Create a new test case with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new test case to create.</p> required <code>description</code> <code>Optional[str]</code> <p>Optional free-form description of the test case to create.</p> <code>None</code> <code>test_samples</code> <code>Optional[List[Tuple[TestSample, GroundTruth]]]</code> <p>Optionally specify a set of test samples and ground truths to populate the test case.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestCase</code> <p>The newly created test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.load","title":"<code>load(name, version=None)</code>  <code>classmethod</code>","text":"<p>Load an existing test case with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test case to load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify a particular version of the test case to load. Defaults to the latest version when unset.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestCase</code> <p>The loaded test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.load_test_samples","title":"<code>load_test_samples()</code>","text":"<p>Load all <code>TestSample</code>s and <code>GroundTruth</code>s contained in this test case.</p> <p>Returns:</p> Type Description <code>List[Tuple[TestSample, GroundTruth]]</code> <p>A list of each test sample, paired with its ground truth, in this test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.iter_test_samples","title":"<code>iter_test_samples()</code>","text":"<p>Iterate through all <code>TestSample</code>s and <code>GroundTruth</code>s contained in this test case.</p> <p>Returns:</p> Type Description <code>Iterator[Tuple[TestSample, GroundTruth]]</code> <p>An iterator yielding each test sample, paired with its ground truth, in this test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.edit","title":"<code>edit(reset=False)</code>","text":"<p>Edit this test case in a context:</p> <pre><code>with test_case.edit() as editor:\n# perform as many editing actions as desired\neditor.add(...)\neditor.remove(...)\n</code></pre> <p>Changes are committed to the Kolena platform when the context is exited.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>Clear any and all test samples currently in the test case.</p> <code>False</code>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.init_many","title":"<code>init_many(data, reset=False)</code>  <code>classmethod</code>","text":"<p>Experimental</p> <p>This function is considered experimental, so beware that it is subject to changes even in patch releases.</p> <p>Create, load or edit multiple test cases.</p> <pre><code>test_cases = TestCase.init_many([\n(\"test-case 1\", [(test_sample_1, ground_truth_1), ...]),\n(\"test-case 2\", [(test_sample_2, ground_truth_2), ...])\n])\ntest_suite = TestSuite(\"my test suite\", test_cases=test_cases)\n</code></pre> <p>Changes are committed to the Kolena platform together. If there is an error, none of the edits would take effect.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Tuple[str, List[Tuple[TestSample, GroundTruth]]]]</code> <p>A list of tuples where each tuple is a test case name and a set of test samples and ground truths tuples for the test case.</p> required <code>reset</code> <code>bool</code> <p>If a test case of the same name already exists, overwrite with the provided test_samples.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[TestCase]</code> <p>The test cases.</p>"},{"location":"reference/workflow/test-run/","title":"<code>kolena.workflow.test</code>","text":""},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun","title":"<code>TestRun(model, test_suite, evaluator=None, configurations=None, reset=False)</code>","text":"<p>             Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A <code>Model</code> tested on a <code>TestSuite</code> using a specific <code>Evaluator</code> implementation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model being tested.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>evaluator</code> <code>Union[Evaluator, BasicEvaluatorFunction, None]</code> <p>An optional evaluator implementation. Requires a previously configured server-side evaluator to default to if omitted. (Please see <code>BasicEvaluatorFunction</code> for type definition.)</p> <code>None</code> <code>configurations</code> <code>Optional[List[EvaluatorConfiguration]]</code> <p>a list of configurations to use when running the evaluator.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>overwrites existing inferences if set.</p> <code>False</code>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.run","title":"<code>run()</code>","text":"<p>Run the testing process, first extracting inferences for all test samples in the test suite then performing evaluation.</p>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.load_test_samples","title":"<code>load_test_samples()</code>","text":"<p>Load the test samples in the test suite that do not yet have inferences uploaded.</p> <p>Returns:</p> Type Description <code>List[TestSample]</code> <p>a list of all test samples in the test suite still requiring inferences.</p>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.iter_test_samples","title":"<code>iter_test_samples()</code>","text":"<p>Iterate through the test samples in the test suite that do not yet have inferences uploaded.</p> <p>Returns:</p> Type Description <code>Iterator[TestSample]</code> <p>an iterator over each test sample still requiring inferences.</p>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.upload_inferences","title":"<code>upload_inferences(inferences)</code>","text":"<p>Upload inferences from a model.</p> <p>Parameters:</p> Name Type Description Default <code>inferences</code> <code>List[Tuple[TestSample, Inference]]</code> <p>the inferences, paired with their corresponding test samples, to upload.</p> required"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.evaluate","title":"<code>evaluate()</code>","text":"<p>Perform evaluation by computing metrics for individual test samples, in aggregate across test cases, and across the complete test suite at each <code>EvaluatorConfiguration</code>.</p>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.test","title":"<code>test(model, test_suite, evaluator=None, configurations=None, reset=False)</code>","text":"<p>Test a <code>Model</code> on a <code>TestSuite</code> using a specific <code>Evaluator</code> implementation.</p> <pre><code>from kolena.workflow import test\ntest(model, test_suite, evaluator, reset=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model being tested, implementing the <code>infer</code> method.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>evaluator</code> <code>Union[Evaluator, BasicEvaluatorFunction, None]</code> <p>An optional evaluator implementation. Requires a previously configured server-side evaluator to default to if omitted. (Please see <code>BasicEvaluatorFunction</code> for type definition.)</p> <code>None</code> <code>configurations</code> <code>Optional[List[EvaluatorConfiguration]]</code> <p>A list of configurations to use when running the evaluator.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>Overwrites existing inferences if set.</p> <code>False</code>"},{"location":"reference/workflow/test-sample/","title":"<code>kolena.workflow.TestSample</code>","text":"<p>Test samples are the inputs to your models when testing.</p> <p>For example, for a model that processes specific regions within a larger image, its test sample may be defined:</p> <pre><code>from dataclasses import dataclass\nfrom kolena.workflow import Image\nfrom kolena.workflow.annotation import BoundingBox\n@dataclass(frozen=True)\nclass ImageWithRegion(Image):\nregion: BoundingBox\nexample = ImageWithRegion(\nlocator=\"s3://my-bucket/example-image.png\",  # field from Image base class\nregion=BoundingBox(top_left=(0, 0), bottom_right=(100, 100)),\n)\n</code></pre>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Metadata","title":"<code>Metadata = Dict[str, Union[None, StrictStr, StrictFloat, StrictInt, StrictBool, str, float, int, bool, List[Union[None, StrictStr, StrictFloat, StrictInt, StrictBool, str, float, int, bool]]]]</code>  <code>module-attribute</code>","text":"<p>Type of the <code>metadata</code> field that can be included on <code>TestSample</code> definitions. String (<code>str</code>) keys and scalar values (<code>int</code>, <code>float</code>, <code>str</code>, <code>bool</code>, <code>None</code>) as well as scalar list values are permitted.</p> <pre><code>from dataclasses import dataclass, field\nfrom kolena.workflow import Image, Metadata\n@dataclass(frozen=True)\nclass ImageWithMetadata(Image):\nmetadata: Metadata = field(default_factory=dict)\n</code></pre>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.TestSample","title":"<code>TestSample</code>","text":"<p>             Bases: <code>TypedDataObject[_TestSampleType]</code></p> <p>The inputs to a model.</p> <p>Test samples can be customized as necessary for a workflow by extending this class or one of the built-in test sample types.</p> <p>Extensions to the <code>TestSample</code> class may define a <code>metadata</code> field of type <code>Metadata</code> containing a dictionary of scalar properties associated with the test sample, intended for use when sorting or filtering test samples.</p> <p>Kolena handles the <code>metadata</code> field differently from other test sample fields. Updates to the <code>metadata</code> object for a given test sample are merged with previously uploaded metadata. As such, <code>metadata</code> for a given test sample within a test case is not immutable, and should not be relied on when an implementation of <code>Model</code> computes inferences, or when an implementation of <code>Evaluator</code> evaluates metrics.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Composite","title":"<code>Composite</code>","text":"<p>             Bases: <code>TestSample</code></p> <p>A test sample composed of multiple basic <code>TestSample</code> elements.</p> <p>An example application would be each test sample is a pair of face images, and the goal is to predict whether the two images are of the same person. For this use-case the test sample can be defined as:</p> <pre><code>class FacePairSample(Composite):\nsource: Image\ntarget: Image\n</code></pre> <p>To facilitate visualization for this kind of use cases, see usage of <code>GroundTruth</code> and <code>Inference</code>.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Image","title":"<code>Image</code>","text":"<p>             Bases: <code>TestSample</code></p> <p>An image located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Image.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The URL of this image, using e.g. <code>s3</code>, <code>gs</code>, or <code>https</code> scheme (<code>s3://my-bucket/path/to/image.png</code>).</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImagePair","title":"<code>ImagePair</code>","text":"<p>             Bases: <code>Composite</code></p> <p>Two <code>Image</code>s paired together.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImagePair.a","title":"<code>a: Image</code>  <code>instance-attribute</code>","text":"<p>The left <code>Image</code> in the image pair.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImagePair.b","title":"<code>b: Image</code>  <code>instance-attribute</code>","text":"<p>The right <code>Image</code> in the image pair.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Text","title":"<code>Text</code>","text":"<p>             Bases: <code>TestSample</code></p> <p>An inline text snippet.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Text.text","title":"<code>text: str</code>  <code>instance-attribute</code>","text":"<p>The text snippet.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImageText","title":"<code>ImageText</code>","text":"<p>             Bases: <code>Composite</code></p> <p>An image paired with a text snippet.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImageText.image","title":"<code>image: Image</code>  <code>instance-attribute</code>","text":"<p>The <code>Image</code> in this image-text pair.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImageText.text","title":"<code>text: Text</code>  <code>instance-attribute</code>","text":"<p>The text snippet in this image-text pair.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.BaseVideo","title":"<code>BaseVideo</code>","text":"<p>             Bases: <code>TestSample</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.BaseVideo.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video","title":"<code>Video</code>","text":"<p>             Bases: <code>BaseVideo</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video.thumbnail","title":"<code>thumbnail: Optional[ImageAsset] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally provide asset locator for custom video thumbnail.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video.start","title":"<code>start: Optional[float] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally specify start time of video snippet, in seconds.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video.end","title":"<code>end: Optional[float] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally specify end time of video snippet, in seconds.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Document","title":"<code>Document</code>","text":"<p>             Bases: <code>TestSample</code></p> <p>A remotely linked document, e.g. PDF or TXT file.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Document.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the document.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.PointCloud","title":"<code>PointCloud</code>","text":"<p>             Bases: <code>TestSample</code></p> <p>A pointcloud file located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.PointCloud.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The URL of the pointcloud file, using e.g. <code>s3</code>, <code>gs</code>, or <code>https</code> scheme (<code>s3://my-bucket/path/to/image.pcd</code>).</p>"},{"location":"reference/workflow/test-suite/","title":"<code>kolena.workflow.TestSuite</code>","text":""},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite","title":"<code>TestSuite(name, version=None, description=None, test_cases=None, reset=False, tags=None)</code>","text":"<p>             Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A test suite groups together one or more test cases. Typically a test suite represents a benchmark test dataset, with test cases representing different meaningful subsets, or slices, or this benchmark.</p> <p>Rather than importing this class directly, use the <code>TestSuite</code> type definition returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.workflow","title":"<code>workflow: Workflow</code>  <code>instance-attribute</code>","text":"<p>The workflow of this test suite. Automatically populated when constructing via test suite type returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>The unique name of this test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.version","title":"<code>version: int</code>  <code>instance-attribute</code>","text":"<p>The version of this test suite. A test suite's version is automatically incremented whenever it is edited via <code>TestSuite.edit</code>.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Free-form, human-readable description of this test suite. Can be edited at any time via <code>TestSuite.edit</code>.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.test_cases","title":"<code>test_cases: List[TestCase]</code>  <code>instance-attribute</code>","text":"<p>The <code>TestCase</code> objects belonging to this test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.tags","title":"<code>tags: Set[str]</code>  <code>instance-attribute</code>","text":"<p>The tags associated with this test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.Editor","title":"<code>Editor(test_cases, description, tags, reset)</code>","text":""},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.Editor.description","title":"<code>description(description)</code>","text":"<p>Update the description of the test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.Editor.add","title":"<code>add(test_case)</code>","text":"<p>Add a test case to this test suite. If a different version of the test case already exists in this test suite, it is replaced.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case to add to the test suite.</p> required"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.Editor.remove","title":"<code>remove(test_case)</code>","text":"<p>Remove a test case from this test suite. Does nothing if the test case is not in the test suite.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case to remove.</p> required"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.create","title":"<code>create(name, description=None, test_cases=None, tags=None)</code>  <code>classmethod</code>","text":"<p>Create a new test suite with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new test suite to create.</p> required <code>description</code> <code>Optional[str]</code> <p>Optional free-form description of the test suite to create.</p> <code>None</code> <code>test_cases</code> <code>Optional[List[TestCase]]</code> <p>Optionally specify a list of test cases to populate the test suite.</p> <code>None</code> <code>tags</code> <code>Optional[Set[str]]</code> <p>Optionally specify a set of tags to attach to the test suite.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestSuite</code> <p>The newly created test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.load","title":"<code>load(name, version=None)</code>  <code>classmethod</code>","text":"<p>Load an existing test suite with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test suite to load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify a particular version of the test suite to load. Defaults to the latest version when unset.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestSuite</code> <p>The loaded test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.load_all","title":"<code>load_all(*, tags=None)</code>  <code>classmethod</code>","text":"<p>Load the latest version of all non-archived test suites with this workflow.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Optional[Set[str]]</code> <p>Optionally specify a set of tags to apply as a filter. The loaded test suites will include only test suites with tags matching each of these specified tags, i.e. <code>test_suite.tags.intersection(tags) == tags</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[TestSuite]</code> <p>The latest version of all non-archived test suites, with matching tags when specified.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.edit","title":"<code>edit(reset=False)</code>","text":"<p>Edit this test suite in a context:</p> <pre><code>with test_suite.edit() as editor:\n# perform as many editing actions as desired\neditor.add(...)\neditor.remove(...)\n</code></pre> <p>Changes are committed to the Kolena platform when the context is exited.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>Clear any and all test cases currently in the test suite.</p> <code>False</code>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.load_test_samples","title":"<code>load_test_samples()</code>","text":"<p>Load test samples for all test cases within this test suite.</p> <p>Returns:</p> Type Description <code>List[Tuple[TestCase, List[TestSample]]]</code> <p>A list of <code>TestCase</code>s, each paired with the list of <code>TestSample</code>s it contains.</p>"},{"location":"reference/workflow/visualization/","title":"<code>kolena.workflow.visualization</code>","text":""},{"location":"reference/workflow/visualization/#kolena.workflow.visualization.Colormap","title":"<code>Colormap(fade_low_activation=True)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>A <code>Colormap</code> maps a pixel intensity to RGBA.</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization._activation_map.Colormap.fade_low_activation","title":"<code>fade_low_activation: bool = fade_low_activation</code>  <code>instance-attribute</code>","text":"<p>Fades out the regions with low activation by applying zero alpha value if set <code>True</code>; otherwise, activation map is shown as is without any fading applied. By default, it's set to <code>True</code>. This option makes the overlay visualization better by highlighting only the important regions.</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization._activation_map.Colormap.red","title":"<code>red(intensity)</code>  <code>abstractmethod</code>","text":"<p>Maps a grayscale pixel intensity to color red: [0, 255]</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization._activation_map.Colormap.green","title":"<code>green(intensity)</code>  <code>abstractmethod</code>","text":"<p>Maps a grayscale pixel intensity to color green: [0, 255]</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization._activation_map.Colormap.blue","title":"<code>blue(intensity)</code>  <code>abstractmethod</code>","text":"<p>Maps a grayscale pixel intensity to color blue: [0, 255]</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization._activation_map.Colormap.alpha","title":"<code>alpha(intensity)</code>","text":"<p>Maps the grayscale pixel intensity to alpha: [0, 255]. If <code>fade_low_activation</code> is False, then it returns the maximum alpha value.</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization.ColormapJet","title":"<code>ColormapJet</code>","text":"<p>             Bases: <code>Colormap</code></p> <p>The MATLAB \"Jet\" color palette is a standard palette used for scientific and mathematical data.</p> <p>It is defined as a linear ramp between the following colours: \"#00007F\", \"blue\", \"#007FFF\", \"cyan\", \"#7FFF7F\", \"yellow\", \"#FF7F00\", \"red\", \"#7F0000\"</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization.encode_png","title":"<code>encode_png(image, mode)</code>","text":"<p>Encodes an image into an in-memory PNG file that is represented as binary data. It is used when you want to upload a 2 or 3-dimensional image in a NumPy array format to cloud.</p> <p>It can be used in conjunction with <code>colorized_activation_map</code> when uploading an activation map.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p>A 2D or 3D NumPy array, shaped either <code>(h, w)</code>, <code>(h, w, 1)</code>, <code>(h, w, 3)</code>, or <code>(h, w, 4)</code></p> required <code>mode</code> <code>str</code> <p>A PIL mode</p> required <p>Returns:</p> Type Description <code>io.BytesIO</code> <p>The in-memory PNG file represented as binary data.</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization.colorize_activation_map","title":"<code>colorize_activation_map(activation_map, colormap=ColormapJet())</code>","text":"<p>Applies the specified colormap to the activation map.</p> <p>Parameters:</p> Name Type Description Default <code>activation_map</code> <code>np.ndarray</code> <p>A 2D numpy array, shaped (h, w) or (h, w, 1), of the activation map in <code>np.uint8</code> or <code>float</code> ranging [0, 1].</p> required <code>colormap</code> <code>Optional[Colormap]</code> <p>The colormap used to colorize the input activation map. Defaults to the MATLAB \"Jet\" colormap.</p> <code>ColormapJet()</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The colorized activation map in RGBA format, in (h, w, 4) shape.</p>"}]}